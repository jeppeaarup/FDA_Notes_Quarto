[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Analysis for Food Science",
    "section": "",
    "text": "Preface\nDuring the past decades the production of data in relation to research, production, consumer behavior, social network etc. has increased dramatically. Today we are faced with data structures which were unimaginable just 50 years ago. Traditionally, a system under investigation were characterized by a few samples associated with say one to five descriptors and, carefully selected, responses. Today all aspects of the classical system interrogation has blown up, such that we have many more samples (e.g. production monitoring every minute), more descriptors (e.g. consumer characteristics), and by far more response variables (e.g. high throughput omics technologies). Tools developed for handling traditional scenarios still pertain the corner of how to approach today’s data analytical challenges, however, by the development of computers, it is possible to carry out challenging mathematical procedures in no time and further produce visual graphics as resources for translating information into knowledge. Due to this fact, the traditional tools has gotten a makeover and new tools has been developed.\nFood is, as such, an extremely inherent part of the human life, although one could argue that so is e.g. cardiovascular biology and governmental policy making, these subjects either work autonomously or does not demand everyday mental capacity. Everyday all humans need to eat- and drink in some social context, pay attention to the perception of the meal, and further deal with the possible health- and emotional implications of this process. When studying food science all these aspects are relevant.\nFood science constitute a broad range of disciplines spanning controlled artificial model systems, over functional modification of real food matrices, production technology, to the relation between food- and meal composition, taste, perception and health. All by means of data.\nThese notes are thought to cover data analysis within food science. That is to; provide a general understanding of the purpose of data analysis, found a theoretical- and practical basis for understanding various numerical and graphical tools and couple generic tools to concrete issues within related disciplines. To this end by theory, examples and exercises.\nThe Book material used in these notes are mostly from the notes for the course; Introduction to Statistics at DTU by P.B. Brockhoff and co workers. Additionally there are relevant chapters from other sources. All exercises are custom made and deal with real problems within food science.\nWelcome to the course in Fødevaredataanalyse for second year bachelor students in Food Science and Technology - Hope you will enjoy learning about how to use data for getting insight on food systems.\nMorten Arendt Vils Rasmussen\nAugust 2024",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "chapters/week1/r_basics.html",
    "href": "chapters/week1/r_basics.html",
    "title": "1  R Basics",
    "section": "",
    "text": "1.1 Installing packages\nA package in R is a set of commands which are not a part of the base-set in R. Many of the R-commands which are used throughout this course requires a certain package to be installed on the computer/Mac. It is a good idea to get familiar with Installing packages and loading them onto your R-script mainly so you won’t be missing them at the exercises, casework or examination.\nIn R there are two important commands concerning installation of packages.\nFor example: install.packages(’readxl’) Installs the package readxl on the computer and remove.packages(’readxl’) uninstalls the package readxl from the computer.",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R Basics</span>"
    ]
  },
  {
    "objectID": "chapters/week1/r_basics.html#installing-packages",
    "href": "chapters/week1/r_basics.html#installing-packages",
    "title": "1  R Basics",
    "section": "",
    "text": "install.packages() installs the target package on your computer.\nremove.packages() uninstalls the package from your computer.\n\n\n\n1.1.1 Loading a package\nWhen the packages are installed on the computer, you can load them onto your workspace/script at every occasion you initiate your analysis in R. To do this, you use the library() command. library() points at a package-library stored on your computer. Everytime you open a new session of R, you need to load the needed packages again.\nFor example, library(readxl) Loads the package readxl onto the workspace.\nWhen you load a package, you might get warning messages like the following:\n\nlibrary(readxl)\n\nAdvarselsbesked:\npakke ‘readxl’ blev bygget under R version 3.1.3",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R Basics</span>"
    ]
  },
  {
    "objectID": "chapters/week1/r_basics.html#sec-r-basics-working-directory",
    "href": "chapters/week1/r_basics.html#sec-r-basics-working-directory",
    "title": "1  R Basics",
    "section": "1.2 Working directory",
    "text": "1.2 Working directory\nIn R you are using something called a working directory or wd for short. This is the folder on your computer in which R saves and finds the projects that you are working on. This also makes it easier to load datasets. The working directory can be changed in R either manually or through code. getwd() and setwd() are the two important commands for changing the working directory.\n\ngetwd() # Show the current working directory\n\n“/Users/madsbjorlie/Documents/Statistik/Exercises/Week 1”\n\nsetwd(\"~/Documents/R-træning\") # Change the working directory \ngetwd() # Show the current working directory\n\n“/Users/madsbjorlie/Documents/R-træning”",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R Basics</span>"
    ]
  },
  {
    "objectID": "chapters/week1/r_basics.html#importing-a-dataset",
    "href": "chapters/week1/r_basics.html#importing-a-dataset",
    "title": "1  R Basics",
    "section": "1.3 Importing a dataset",
    "text": "1.3 Importing a dataset\nThroughout this course you will need to import a lot of data into R. Getting familiar with the following packages and commands will help minimize your R-related frustration. Datasets can be imported into R in numerous ways. Like changing the working directory, it can be done both manually and through coding. We recommend doing it through coding since this makes it easier to maintain an overview.\nAlmost all of the datasets that will be handed out in this course will be in both the excel file-type .xlsx, as well as the .RData format. R is also capable of importing text-files such as .txt or .csv.\n.xlsx-files are Microsoft Excel’s standard project file type, whereas .csv-files are short for comma separated values and is a term for text-files where the values are separated by a comma (or in the Danish Excel, a semicolon).\nYou can either load .RData files, import datasets through R’s inherent commands or use some data-import packages to import file-types such as .xlsx or .xls. Both methods works fine and which one you will use depends on your personal preference.\n\n1.3.1 Importing an .RData file\nIf someone imported and stored the data as an .RData file, you can simply import it using the load() function. For this you do not need any libraries.\n\nload(file.choose())\nload(\"~/Documents/..../Beer GCMS.RData”)\n\nThe only difference in comparison with the import-methods below, is that you do not “pipe” (the &lt;- function) the object into something you name yourself. The data object will retain the name as it was saved with. However, if you like your objects to be named something special (like X), then simply just add a line below the load() where you define it: e.g., X &lt;- beer.\n\n\n1.3.2 Importing a dataset through R’s own commands\nAs a default, R can not import Excel-files such as .xls and .xlsx. To use R’s read.csv() function, you need to save the Excel dataset as a .csv file. This is done by choosing (in Excel) and then selecting the .csv file-type. This might seem a bit tedious, but it eliminates the demand for other packages.\nread.csv() imports the dataset specified in the parenthesis. This can be done in two ways: by typing the path to the file on your computer or by using the command file.choose() which corresponds to opening a new file. If the dataset is in the working directory, you do not have to type the full path, but just the file-name.\nFor example:\n\nBeer &lt;- read.csv(file.choose(), header=TRUE, sep=\";\", dec=\",\")  \nBeer &lt;- read.csv(”Beerdata.csv”, header=TRUE, sep=\";\", dec=\",\")  \nBeer &lt;- read.csv(\"~/Documents/R-traening/Øldata.csv”, header=TRUE, sep=\";\", dec=\",\")\n\nThe different arguments: header =, sep =, and dec = tells R how to import the data. header=TRUE tells R that the first row in the dataset is not a part of the data itself but carries the variable names. sep=”;” defines which separator the document uses. By using Danish Excel, this will always be semicolon. This can be checked by opening the dataset in NotePad on windows or TextEditor on Mac. dec=”,” defines which symbol is used for decimals. It is necessary to make sure that the dataset in R is separated by a full stop rather than a comma. This can be checked by using summary commands after the data has been imported.\n\n\n1.3.3 Importing a dataset using packages\nBy using various packages, it is possible to import Excel-documents directly into R. This can be quite handy, but some of the packages will not run on Mac or on Windows due to other programs missing. The most commonly used data-import packages are: gdata, readxl, xlsx and rio. gdata requires Perl which is default on Mac and Linux but not on Windows and therefor it will not run on Windows unless it is installed. The following is a couple of examples using the various packages.\n\nlibrary(readxl)  \nBeer &lt;- read_excel(file.choose())  \nBeer &lt;- read_excel(\"~/Documents/R-traening/Beerdata.xls”)  \nBeer &lt;- read_excel(\"~/Documents/R-traening/Beerdata.xlsx”)  \n\nlibrary(gdata)  \nBeer &lt;- read.xls(file.choose())  \nBeer &lt;- read.xls(\"~/Documents/R-traening/Beerdata.xls”)  \n\nlibrary(xlsx)  \nBeer &lt;- read.xlsx(file.choose(), sheetIndex = 1)  \nBeer &lt;- read.xlsx(\"~/Documents/R-traening/Beerdata.xls”, sheetIndex = 1)  \nBeer &lt;- read.xlsx(\"~/Documents/R-traening/Beerdata.xlsx”, sheetIndex = 1)  \n\nlibrary(rio)  \nBeer &lt;- import(file.choose())  \nBeer &lt;- import(\"~/Documents/R-traening/Beerdata.xls”)  \nBeer &lt;- import(\"~/Documents/R-traening/Beerdata.xlsx”)  \n\n\n\n1.3.4 Getting an overview of the dataset\nWhen the dataset is imported into R, you can use different commands to check that it was imported correctly. The commands are head(), str() and dim().\n\nhead() shows the first 6 rows in the dataset.\n\nstr() shows the types of the various columns, such as numeric and factor.\n\ndim() shows the dimensions of the data-matrix.\n\n\nCoffee &lt;- read.csv(file.choose(), header=TRUE, sep=\";\", dec=\",\")\nhead(Coffee)\n\n\n\n  Sample Assessor Replicate Intensity  Sour Bitter\n1    31C        1         1      9.30  6.90   6.75\n2    31C        1         2      8.70  8.10   7.95\n3    31C        1         3      9.75  8.70  10.20\n4    31C        1         4     11.70 10.95  11.40\n5    31C        2         1      8.70  5.70  11.40\n6    31C        2         2      8.70  9.30  10.35\n\n\n\nstr(Coffee)\n\n\n\n'data.frame':   192 obs. of  6 variables:\n $ Sample   : Factor w/ 6 levels \"31C\",\"37C\",\"44C\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ Assessor : int  1 1 1 1 2 2 2 2 3 3 ...\n $ Replicate: int  1 2 3 4 1 2 3 4 1 2 ...\n $ Intensity: num  9.3 8.7 9.75 11.7 8.7 ...\n $ Sour     : num  6.9 8.1 8.7 10.9 5.7 ...\n $ Bitter   : num  6.75 7.95 10.2 11.4 11.4 ...\n\n\n\ndim(Coffee) # show dimensions of data\n\n\n\n[1] 192   6",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R Basics</span>"
    ]
  },
  {
    "objectID": "chapters/week1/r_basics.html#scripts",
    "href": "chapters/week1/r_basics.html#scripts",
    "title": "1  R Basics",
    "section": "1.4 Scripts",
    "text": "1.4 Scripts\nWe highly recommend that you make your data analysis using a script. A script is simply a flat text file that is given the surname .R such that R can interpret the commands. Here you will have the commands needed to do the analysis from setting necessary functions, import of data, initial inspection, modeling and plots.\nEach analysis task is slightly different, however, almost always there is a set of generic tasks which is always needed. That is: cleaning up the workspace, loading packages, setting work directory, loading data and checking the data structure. That typically fills up the first 5-10 lines of code in every script as follows:\n\nrm(list = ls()) # remove all variables in environment\ngraphics.off() # delete all generated plots\n\nlibrary(ggplot2) # load package ggplot2\nlibrary(readxl) # load package readxl\n\nsetwd(\"~/MyComputer/Courses/FDA/Exercises/Week1\") # set working directory\n\ndata &lt;- read_excel(\"SomeData.xlsx”) # load file\n\nhead(data) # show first 6 lines of data\n\n# plot data\nggplot(df, aes(x, y, color = treatment)) + \n    geom_point()",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R Basics</span>"
    ]
  },
  {
    "objectID": "chapters/week1/r_basics.html#rmarkdown",
    "href": "chapters/week1/r_basics.html#rmarkdown",
    "title": "1  R Basics",
    "section": "1.5 Rmarkdown",
    "text": "1.5 Rmarkdown\nUsing RStudio makes it possible to combine scripts (your data analysis) with output (figures, tables and numbers) and narrative (the fairytale on why and discussions etc.) in ONE document. It is rather simple when you get used to how it works, and it really makes life much more easy. Both for this course, but also for all other tasks which involves data analysis and is to be presented as a report.\nIt is refereed to as reproducible data analysis, and is the opposite of Excel-Hell, where the latter is characterized by being non transparent, and really hard to figure out what happened between data and results.\nCheck it out by browsing Rmarkdown and try it yourself!",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R Basics</span>"
    ]
  },
  {
    "objectID": "chapters/week1/descriptive_statistics.html",
    "href": "chapters/week1/descriptive_statistics.html",
    "title": "2  Descriptive statistics",
    "section": "",
    "text": "2.1 Reading material",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Descriptive statistics</span>"
    ]
  },
  {
    "objectID": "chapters/week1/descriptive_statistics.html#reading-material",
    "href": "chapters/week1/descriptive_statistics.html#reading-material",
    "title": "2  Descriptive statistics",
    "section": "",
    "text": "Chapter 1 of Introduction to Statistics by Brockhoff\n\nEspecially section 1.1 to 1.4.\n\nVideo lecture on central metrics (mean and median).\nVideo lecture on dispersion (variance, standard deviation etc.)\nVideo lecture on both central metrics and dispersion.",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Descriptive statistics</span>"
    ]
  },
  {
    "objectID": "chapters/week1/descriptive_statistics.html#exercises",
    "href": "chapters/week1/descriptive_statistics.html#exercises",
    "title": "2  Descriptive statistics",
    "section": "2.2 Exercises",
    "text": "2.2 Exercises\n\n\n\n\n\n\nExercise 2.1 - Descriptive statistics by hand\n\n\n\n\n\n\nBelow (table 2.1) is listed a vector of ranking (Liking) of coffee served at 56°C by 52 consumers. The data is sorted.\n\n\n\n\nTable 2.1: Liking of coffee served at 56°C as ranked by 52 consumers.\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n\n2\n2\n3\n3\n4\n4\n4\n4\n4\n4\n\n\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n\n\n5\n6\n6\n6\n6\n6\n6\n6\n6\n6\n\n\n6\n6\n7\n7\n7\n7\n7\n7\n7\n7\n\n\n7\n7\n7\n7\n7\n7\n7\n8\n8\n8\n\n\n8\n9\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSome useful numbers:\n\\[\\sum{X} = 301\\]\n\\[\\sum{(X_i - \\bar{X})^2} = 122.7\\]\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nCalculate mean, variance, standard deviation, median and inner quartile range for this distribution of data.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 2.2 - Descriptive statistics\n\n\n\n\n\n\nServing temperature of coffee seems of importance as to how this drink is perceived. However, it is not totally clear how this relation is. In order to understand this, studies on the same type of coffee served at different temperature is conducted. In this exercise we are going to use the data from a consumer panel of 52 consumers, evaluating coffee served at six different temperatures on a set of sensorical descriptors leading to a total of \\(52 \\times 6 = 312\\) samples.\nIn the dataset the results are listed. Taking these data from A to Z involves descriptive analysis for understanding variation within judge, between judge and between different temperatures, further outlier detection, and finally determination of structure between sensorical descriptors. In this exercise we are only going through some of the initial descriptive steps.\nIn the table below (table 2.2) a subset of the data is shown.\n\n\n\n\nTable 2.2: A subset of the Results Consumer Test.xlsx data\n\n\n\n\n\n\n\nSample\nTemperatur\nAssessor\nServingOrder\nTemperatureJudgment\nLiking\nIntensity\nSour\nBitter\nSweet\nMale\nFemale\n\n\n\n\n1\n31C\n31\n1\n6\n2\n3\n4\n3\n4\n2\n1\n0\n\n\n2\n31C\n31\n2\n6\n2\n3\n7\n5\n8\n3\n1\n0\n\n\n3\n31C\n31\n3\n6\n1\n3\n2\n1\n4\n6\n0\n1\n\n\n4\n31C\n31\n4\n6\n1\n2\n5\n6\n4\n3\n1\n0\n\n\n5\n31C\n31\n5\n6\n2\n2\n2\n3\n2\n1\n1\n0\n\n\n6\n31C\n31\n6\n6\n2\n4\n3\n4\n2\n1\n1\n0\n\n\n307\n62C\n62\n47\n6\n8\n8\n8\n2\n8\n1\n0\n1\n\n\n308\n62C\n62\n48\n6\n6\n7\n7\n4\n3\n3\n0\n1\n\n\n309\n62C\n62\n49\n6\n5\n8\n6\n6\n4\n6\n1\n0\n\n\n310\n62C\n62\n50\n6\n6\n5\n7\n7\n7\n4\n0\n1\n\n\n311\n62C\n62\n51\n6\n7\n6\n8\n6\n7\n2\n1\n0\n\n\n312\n62C\n62\n52\n6\n6\n7\n6\n6\n7\n3\n1\n0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nImport the data\n\nBe aware that the function read.xls() is not in the base library, so you need to add the specific library to your computer.\n\nSubsample on one temperature.\n\nBelow (listing 2.1) is listed two alternatives for doing this.\n\n\n\n\n\n\n\n\n\n\nListing 2.1: Importing and subsampling based on temperature.\n\n\nCoffee &lt;- read_excel( \" Results Consumer Test . xlsx \" )\nCoffee_t44_v1 &lt;- Coffee[Coffee$Temperatur == 44,]\nCoffee_t44_v2 &lt;- subset(Coffee, Temperatur == 44)\n\nmean(Coffee_t44_v1$Liking)\n\n\n\n\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nCalculate the descriptive statistics for centrality (mean and median), dispersion (IQR, standard deviation and range) and extremes (min and max) for this distribution of datapoints for a single descriptor (e.g. )\nNow do it for all temperatures.\n\nYou should get something like the table below (table 2.3).\n\n\n\n\n\n\n\n\n\n\nTable 2.3: Summary table computed in R.\n\n\n\n\n\n\nTemp\nN\nMean\nMedian\nStd\nMin\nMax\n\n\n\n\n31\n52\n3.576923\n3\n1.649078\n1\n7\n\n\n37\n52\n4.750000\n5\n1.780890\n1\n7\n\n\n44\n52\n5.826923\n6\n1.605397\n2\n9\n\n\n50\n52\n5.961538\n6\n1.596092\n2\n8\n\n\n56\n52\n5.788462\n6\n1.550920\n2\n9\n\n\n62\n52\n6.173077\n6\n1.367998\n2\n8\n\n\n\n\n\n\n\n\n\n\nThis can be quite tedious, and result in a lot of coding. However, the function summary() and aggreggate() are very efficient in producing such results. Try to check out these functions and see if you can use those to generate summary statistics. Below are shown some code which does exactly what you want without too many lines of code.\n\n\n\n\nListing 2.2: Generating a summary table with aggregate().\n\n\n# Include only responses\nCoffeeDT &lt;- Coffee[,2:10]\n\n# Run aggregate for each type of summary\ntmpN &lt;-aggregate(CoffeeDT,by=list(CoffeeDT$Temperatur),FUN = 'length')\ntmpM&lt;-aggregate(CoffeeDT,by=list(CoffeeDT$Temperatur),FUN = 'mean')\ntmpM2&lt;-aggregate(CoffeeDT,by=list(CoffeeDT$Temperatur),FUN = 'median')\ntmpS&lt;-aggregate(CoffeeDT,by=list(CoffeeDT$Temperatur),FUN = 'sd')\ntmpMi&lt;-aggregate(CoffeeDT,by=list(CoffeeDT$Temperatur),FUN = 'min')\ntmpMx&lt;-aggregate(CoffeeDT,by=list(CoffeeDT$Temperatur),FUN = 'max')\n\n# Merge these into a dataset\ntmp &lt;- cbind(tmpM$Temperatur,tmpN$Liking,tmpM$Liking,tmpM2$Liking,\n             tmpS$Liking,tmpMi$Liking,tmpMx$Liking)\n\n# Add a meaningfull label for each coloumn\ncolnames(tmp) &lt;- c('Temp','N','Mean','Median','Std','Min','Max') \nprint(tmp)\n\n\n\n\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nThe above is done for , try to do it for some of the other responses.\n\nHINT: This can be done by repeating the code and exchange $Liking with e.g. $Bitter. However, putting this in a for loop is another option.\n\nWhat have you learned from analysing these data in terms of importance of serving temperature on the sensorical properties as percieved by consumers?\n\nHINT: You can run the code below to get a comprehensive overview. This is based on the mean aggreggate, but you might just as well check some of the other descriptive metrics. For instance, what does the standard deviation tells you about consumers in general, and does the type of sensorical attribute and serving temperature make a difference on the spread in scoring?\n\n\n\n\n\n\n\n\n\n\nListing 2.3: Code for plotting the results of listing 2.2.\n\n\nmatplot(tmpM[,2],tmpM[,6:10],type='l',lwd=3)\ntext(cbind(60,t(tmpM[6,6:10])),colnames(tmpM[,6:10]))\n\n\n\n\nYou might want to fix some of the labels in these figures. Check the documentation by typing ?matplot and see how to add meaning full stuff to the plot.",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Descriptive statistics</span>"
    ]
  },
  {
    "objectID": "chapters/week1/debugging.html",
    "href": "chapters/week1/debugging.html",
    "title": "3  Debugging - Getting R to work",
    "section": "",
    "text": "3.1 Reading material\nThe notes on debugging Debugging in R.pdf available through Absalon.",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Debugging - Getting R to work</span>"
    ]
  },
  {
    "objectID": "chapters/week1/debugging.html#exercises",
    "href": "chapters/week1/debugging.html#exercises",
    "title": "3  Debugging - Getting R to work",
    "section": "3.2 Exercises",
    "text": "3.2 Exercises\n\n\n\n\n\n\nExercise 3.1\n\n\n\n\n\n\nFor some of you, coding in Rstudio may seem simple. The aim with these debugging tasks is to train you to analyze the errors Rstudio gives you, and to give you some tools to use to avoid issues when coding in Rstudio. Many of the debugging exercises throughout the course will be related to the datasets used in other exercises given in the same week, so you might find it sensible to do the debugging-exercises first, to get to know the datasets and their potential issues, before the struggle starts.\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nWhy won’t Rstudio read in the file in the following cases?\n\n\n\n\n\n\ncoffee &lt;- read_excel(\"Results Consumer Test.xlsx\")\n\nError: could not find function “read_excel”\n\ncoffee &lt;- read_excel(\"Results Consumer Test\")\n\nError: “Results Consumer Test” does not exist in current working directory”\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nWhy isn’t the view-function working in the following case?\n\n\n\n\n\n\ncoffee &lt;- read_excel(\"Results Consumer Test.xlsx\")\nview(coffee)\n\nError: could not find function “view”\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nWhat is missing when you call for aggregate to do its calculations?\n\n\n\n\n\n\ncoffee &lt;- read_excel(\"Results Consumer Test.xlsx\")\ncoffee_ag &lt;- aggregate(by = list(coffee$Assessor, coffee$Sample), FUN = \"sd\")\n\nError in is.ts(x) : argument “x” is missing, with no default",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Debugging - Getting R to work</span>"
    ]
  },
  {
    "objectID": "chapters/week1/plotting.html",
    "href": "chapters/week1/plotting.html",
    "title": "4  Plotting",
    "section": "",
    "text": "4.1 Reading material",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Plotting</span>"
    ]
  },
  {
    "objectID": "chapters/week1/plotting.html#reading-material",
    "href": "chapters/week1/plotting.html#reading-material",
    "title": "4  Plotting",
    "section": "",
    "text": "Chapter 1 in Introduction to Statistics by Brockhoff\n\nEspecially section 1.5 to 1.6.\n\nggplot2 cheat sheet. It is a very nice cheat sheet on how to use ggplot2.\n\nCheat sheets for other libraries also exist. You can find them here.\n\nOnline lectures: There is a lot of how-to-plot on the web. Use it when you are getting stuck at a problem, or if you feel like being inspired.\n\nThese are nice and short:\n\nCharts Are Like Pasta - Data Visualization Part 1: Crash Course Statistics #5.\nPlots, Outliers, and Justin Timberlake: Data Visualization Part 2: Crash Course Statistics #6.\n\nThis one is pretty comprehensive:\n\nVisualising data with ggplot2. Lecture by Hadley Wickham.",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Plotting</span>"
    ]
  },
  {
    "objectID": "chapters/week1/plotting.html#exercises",
    "href": "chapters/week1/plotting.html#exercises",
    "title": "4  Plotting",
    "section": "4.2 Exercises",
    "text": "4.2 Exercises\nThe package ggplot2 for R is a versatile tool for producing various plots with the option of modifying them in great detail. The following exercises should guide you through the basic functionality of the ggplot function and show you how you can layer different geoms to produce the plots you want.\n\n\n\n\n\n\nBefore you start\n\n\n\nStart by importing a dataset and specify some necessary packages (If you have not installed them on your local drive, you might need to do so). The following lines of code will do the job, if you specify the correct working directory.\n\n# Load package for importing Excel-files\nlibrary(readxl) \n\n# Import coffee data set\ncoffee &lt;- read_excel(\"Results Consumer Test.xlsx\")\n\nThere are several ways of importing data into R, depending on which format you have them in. From excel, the read.xls() (from the gdata library) or read_excel() (from the readxl library) are two ways of doing it. In either case, make sure that the data is correctly imported, by comparing the imported data with the original - sometimes there are problems with decimals.\n\n\nAssume that you have a single response variable, for instance alcohol content of a series of various types of drinks, measurements of body weight from an nutritional experiment or content of antioxidants for a given product produced under different conditions. For all of these, the variable is continuous in form. As a starting point of every analysis an overview of the distribution of the variable of interest is crucial, and plotting the distribution facilitate insight into character of distribution (bell shaped, skewness, bi modal, zero inflated etc.) and single point information for outlier identification.\n\n\n\n\n\n\nExercise 4.1 - Plotting distributions with histograms\n\n\n\n\n\n\nThe histogram is the most basic representation of continuous data. A simple histogram can be produced in the following way with ggplot2.\n\nggplot(data = coffee, aes(Liking)) +\n  geom_histogram()\n\n\n\n\n\n\n\nFigure 4.1: A simple histogram of the “Liking” variable in the coffee consumer dataset.\n\n\n\n\n\nThere are numerous additional arguments that can be used to modify this plot. Check out the documentation or google it to see how it is done.\nThe documentation can be found by running ?geom_histogram() or by having the text cursor in the function name while pressing F1.\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nChange the color of the bars\nChange the bin width.\nChange the transparency of the bars (aka. alpha-value).\nAdd a title to the plot.\n\n\n\n\n\nThe data here are ”liking” of coffees at different temperatures, and so one might wish to infer this information in the histogram. This can be done by coloring by sample name.\n\nggplot(coffee, aes(Liking, fill = Sample)) +\n  geom_histogram(\n    position = \"dodge\", # Plot the colors side-by-side\n    binwidth = 0.8 # Set the width of the bins\n  )\n\n\n\n\n\n\n\nFigure 4.2: A histogram of the “Liking” variable in the coffee consumer dataset. The histogram is now colored by the sample name.\n\n\n\n\n\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nLook at figure 4.2, how does the temperature affect the liking?\n\n\n\n\n\nIf you do not want to overlay the histograms, it is possible to plot them as individual panels. Try running the following code and see what it does.\n\nggplot(coffee, aes(Liking)) +\n  geom_histogram() +\n  facet_wrap(~ Sample) # Wrap by sample\n\n\n\n\n\n\n\n\n\n\n\nExercise 4.2 - Plotting distributions with densitograms\n\n\n\n\n\n\nA densitogram is a smoothed extension of the histogram, and as such represents the same type of information. The bin width in the histogram controls the resolution, whereas the counterpart in the densitogram is the degree of smoothing. By changing the geom used in the ggplot call the plot is changed to a densitogram.\n\nggplot(coffee, aes(Liking)) +\n  geom_density()\n\n\n\n\n\n\n\nFigure 4.3: A densitogram of the “Liking” variable in the coffee consumer dataset.\n\n\n\n\n\nAgain, there are numerous additional arguments that can be used to modify the plot. Read the documentation and try some of them out.\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nModify the smoothing of the densitogram by changing the option adjust =\nTry to make the smoothing very refined (e.g. adjust = 0.3). Does this reflect the underlying distribution? What is a suitable smoothing option for these data?\n\n\n\n\n\nExactly as for the histogram, it is possible to infer additional information on the densitogram.\n\nggplot(coffee, aes(Liking, fill = Sample)) +\n  geom_density()\n\n\n\n\n\n\n\nFigure 4.4: A densitogram of the “Liking” variable in the coffee consumer dataset. The plot is now colored according to the “Sample” variable.\n\n\n\n\n\nThis plot is not optimal, as only the densitogram for temperature 62C is fully visible.\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nTry adjusting the transparency by of the plot by setting alpha = 0.5.\n\n\n\n\n\nThe colors used in the plot could be more intuitive as they refer to temperature. There are several ways the to add a different color scheme. One is to add a layer to the plot specifying either a predefined color scheme, a modification of a predefined color scheme or simply by specifying each of the colors used. Here we just use a predefined Red to Blue-palette from ColorBrewer.\n\nggplot(coffee, aes(Liking, fill = Sample)) +\n  geom_density() +\n  scale_fill_brewer(\n    palette = \"RdBu\"\n    )\n\n\n\n\n\n\n\nFigure 4.5: A densitogram of the “Liking” variable in the coffee consumer dataset. The plot is now colored according to the “Sample” variable using the Red to Blue-palette from ColorBrewer.\n\n\n\n\n\nHowever, the colors are in a counter intuitive direction. This can easily be corrected for by adding direction = -1 to the color scheme call.\n\nggplot(coffee, aes(Liking, fill = Sample)) +\n  geom_density() +\n  scale_fill_brewer(\n    palette = \"RdBu\",\n    direction = -1\n  )\n\n\n\n\n\n\n\nFigure 4.6: A densitogram of the “Liking” variable in the coffee consumer dataset. The plot is now colored according to the “Sample” variable using the Red to Blue-palette from ColorBrewer - this time in the “correct” direction.\n\n\n\n\n\nIf nothing seems to fit your ideal color-world, then you can simply specify the exact colors (HINT: try googling color codes for ggplot2 ).\n\ncustom_colormap &lt;- c(\"#0033FF\", \"#0099FF\",\"#00EEFF\", \n                     \"#FFCCCC\", \"#FF9999\",\"#FF0000\")\n\nggplot(coffee, aes(Liking, fill = Sample)) +\n  geom_density() +\n  scale_fill_manual(\n    values = custom_colormap\n  )\n\n\n\n\n\n\n\nFigure 4.7: A densitogram of the “Liking” variable in the coffee consumer dataset. The plot is now colored according to the “Sample” variable using manually specified colors.\n\n\n\n\n\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nTry to reconstruct these plots, and try to use other predefined color schemes. Further all these plots suffers from lack of transparency, so fix that as well.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 4.3 - Boxplot, Jitterplot and Violinplot\n\n\n\n\n\n\nIn the above, the different temperatures were infered on the plot by overlaying histograms. However, the x-axis can be used for keeping track of this information. This is especially useful when you are comparing more than four levels. The code below produces four different plots for this purpose.\n\nggplot(coffee, aes(Sample, Liking, fill = Sample)) +\n  geom_boxplot() +\n  scale_fill_brewer(\n    palette = \"RdBu\",\n    direction = -1\n  ) +\n  labs(title = \"Boxplot\") +\n  theme_linedraw()\n\n\n\n\n\n\n\nFigure 4.8: A boxplot of the “Liking” variable in the coffee consumer dataset.\n\n\n\n\n\n\nggplot(data = coffee, \n       aes(Sample, Liking, color = Sample, shape = Sample)\n) +\n  geom_jitter() +\n  labs(title = \"Jitter plot\") +\n  theme_light()\n\n\n\n\n\n\n\nFigure 4.9: A jitter plot of the “Liking” variable in the coffee consumer dataset.\n\n\n\n\n\n\nggplot(data = coffee, \n       aes(Sample, Liking, fill = Sample, shape = Sample)\n) +\n  geom_boxplot() +\n  geom_jitter() +\n  scale_fill_brewer(\n    palette = \"RdBu\",\n    direction = -1\n  ) +\n  labs(title = \"Box n' jitter\") +\n  theme_minimal()\n\n\n\n\n\n\n\nFigure 4.10: A box n’ jitter plot of the “Liking” variable in the coffee consumer dataset.\n\n\n\n\n\n\nggplot(data = coffee, \n       aes(Sample, Liking, fill = Sample, shape = Sample)\n) +\n  geom_violin() +\n  scale_fill_brewer(\n    palette = \"RdBu\",\n    direction = -1\n  ) +\n  labs(title = \"Violin plot\") +\n  theme_classic()\n\n\n\n\n\n\n\nFigure 4.11: A boxplot of the “Liking” variable in the coffee consumer dataset.\n\n\n\n\n\nThere are several things to notice from the plots above:\n\nThe title of a plot can be added by using labs(title = \"Title goes here) or ggtitle(\"Title goes here).\nThe background, and some other stuff, of the plots are different, and is inferred by adding + theme_XXX() (the default is theme is theme_gray()).\nIn the boxplot the colors are added by fill = Sample and in the jitter plot the colors are added by color = Sample. This is because the the points in the jitter plot do not have a “fill” (they are not “filled” with a color).\n\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nTry to reconstruct these plots.\nThe boxplot has several features, such as a box with a line in the middle, some so-called whiskers and also maybe a few actual points. Check out what these refer to in the data, and calculate them directly on data to verify that the computer is not off.\n\n\n\n\n\n\n\n\n\n\n\nHow to save plots\n\n\n\nThe plots can saved as high resolution files by using the ggsave-function. Unless otherwise specified the plots are saved in the same aspect ratio as seen in the plot window of RStudio.\n\n# Save plot as PNG with a resoultion of 300 DPI (standard print resolution)\nggsave(\"MyPlot.png\", dpi = 300) \n\n# Save plot as jpeg with a resoultion of 72 dpi\nggsave(\"MyPlot.jpeg\", dpi = 72) \n\n# Save plot as pdf\nggsave(\"MyPlot.pdf\") \n\nThe plot is saved in the current working directory.\nSometimes it can be necessarry to play around with the scaling factor scale =. Try increasing it if the text looks too small.\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 4.4 - Analysis of coffee serving temperature - data inspection\n\n\n\n\n\n\nServing temperature of coffee seems of importance on how this drink is perceived. However, it is not totally clear how this relation is. In order to understand this, studies on the same type of coffee served at different temperature is conducted. In this exercise we are going to use the data from a trained Panel of eight judges, evaluating coffee served at six different temperatures on a set of sensorical descriptors. Each judge is presented with each temperature in a total of four replicates leading to a total of \\(6 \\times 8 \\times 4 = 192\\) samples.\nIn the dataset Results Panel.xlsx the results are listed. Taking these data from A to Z involves descriptive analysis for understanding variation within judge, between judge and between different temperatures, further outlier detection, and finally determination of structure between sensorical descriptors. In this exercise we are only going to briefly explore the data with emphasis on uncertainty.\nThis is data from a trained panel, meaning each judge have been trained to be an objective instrument returning the same response when presented the same sample. However, there is always uncertainty in such responses, and especially when the instrument is a human being. We are interested in how big the deviation is between the four replicates, across judges and samples.\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nImport the data and check that it is matching the excel file using head().\nUse the summarise() or aggreggate() function to extract certain descriptive measures (e.g. mean or standard deviation) from the data.\nPlot this descriptive measure for a single descriptor across temperature (x-axis) and join the points from the same judge.\nWhat can you say about the individual judges? And is scoring more difficult for higher temperature than lower?\n\n\n\n\n\nThe code below does (some) of the job.\n\nClassic RTidyverse\n\n\n\n# Compute the mean of the replicates\ncoffee_ag &lt;- aggregate(coffee, \n                       by = list(coffee$Assessor, coffee$Sample), \n                       FUN = mean)\n\n# Rename some of the variables \ncoffee_ag &lt;- rename(coffee_ag, c(Judge = Group.1, Temp = Group.2))\n\n# Make an initial plot of the result\nggplot(coffee_ag, aes(Temp, Intensity, color = Temp)) +\n  geom_boxplot() +\n  geom_jitter()\n\n\n\n\ncoffee |&gt; \n  group_by(Assessor, Sample) |&gt;\n  summarise(across(\n    .cols = where(is.numeric), # Choose all numeric columns\n    .fns = mean # Compute mean\n  )) |&gt; \n  ggplot(aes(Temperatur, Intensity, color = Sample)) +\n  geom_boxplot() +\n  geom_jitter()",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Plotting</span>"
    ]
  },
  {
    "objectID": "chapters/week1/pca.html",
    "href": "chapters/week1/pca.html",
    "title": "5  Principal Component Analysis (PCA)",
    "section": "",
    "text": "5.1 PCA in brief\nPrincipal Component Analysis is a method for understanding multivariate data. By multivariate data we mean a set of samples/observations (\\(n\\)) which are characterized on a number of different features (\\(p\\)). For example:\nThe data is often arranged in a data table (referred to as \\(\\mathbf{X}\\)) with \\(n\\) rows (samples) and \\(p\\) columns (variables). For almost all real life applications such multivariate data are correlated. That is; some of the variables carry the same type of information, and the interesting information in these data is captured by this so-called correlation structure. A nice visualization of the correlation is done via a scatter plot of two variables. For a few variables (say \\(p = 5\\)) it is possible to interpret all combinations of two variables. If \\(p = 5\\) that amounts to \\(\\frac{p(p-1)}{2} = 10\\) plots. However, when \\(p\\) is high this becomes in-practical. PCA deals with this issue by compressing the data into a set of components:\n\\[\n\\mathbf{X} = \\mathbf{t}_1 \\mathbf{p}_1^T + \\mathbf{t}_2 \\mathbf{p}_2^T + \\cdots + \\mathbf{t}_k \\mathbf{p}_k^T + \\mathbf{E}\n\\]\nNotation wise, \\(\\mathbf{X} \\sim (n, p)\\) is a matrix, \\(\\mathbf{t_i} \\sim (n,1)\\) and \\(\\mathbf{p_i} \\sim (p,1)\\) are vectors.\nThe power of PCA is when the mathematical decomposition into scores and loadings is combined with visualization of these. That is:",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Principal Component Analysis (PCA)</span>"
    ]
  },
  {
    "objectID": "chapters/week1/pca.html#pca-in-brief",
    "href": "chapters/week1/pca.html#pca-in-brief",
    "title": "5  Principal Component Analysis (PCA)",
    "section": "",
    "text": "Different beers (\\(n = 20\\)) analyzed for \\(p = 60\\) chemical variables reflecting the aroma composition.\nSix coffee samples (\\(n = 6\\)) assessed on a set of sensorical descriptors (\\(p = 12\\)) by sensorical panel of eight judges (\\(n = 48\\)).\nA range of oil samples (\\(n = 40\\)) analyzed by near infrared spectroscopy (\\(p = 400\\)).\n\n\n\n\n\n\\(\\mathbf{t}_1\\) are scores for component 1 (\\(\\mathbf{t}_2\\) are scores for component 2 and so forth) and tells something about the multivariate sample distribution.\n\\(\\mathbf{p}_1\\) are loadings for component 1 (\\(\\mathbf{p}_2\\) are loadings for component 2 and so forth) and tells something about the multivariate correlation structure between the variables.\nA set of \\(\\mathbf{t}_1\\) and \\(\\mathbf{p}_1\\) is referred to as a component.\n\n\n\nScore plot - scatter plots of combination of scores (often \\(\\mathbf{t}_1\\) vs. \\(\\mathbf{t}_2\\)).\nLoading plot - scatter plots of combination of loadings (often \\(\\mathbf{p}_1\\) vs. \\(\\mathbf{p}_2\\)).\nBi plot - overlayed score- and loading plot.\n\n\n5.1.1 Example: Natural Phenolic Antioxidants for Meat Preservation - PCA\nThis data originates from a study investigating the effect of natural phenolic antioxidants against lipid and protein oxidation during sausage production and storage. Bologna-type sausages were prepared and treated with either green tea (GT) or rosemary extract (RE) as antioxidants, and a control batch was also included. The three types of sausages were evaluated by a sensory panel including 8 assessors, on 18 different descriptors within the categories smell, color, taste and texture. The sausages were evaluated immediately after production (week0) and after four weeks of storage (week4).\nData is from: Jongberg, Sisse, et al. ”Effect of green tea or rosemary extract on protein oxidation in Bologna type sausages prepared from oxidatively stressed pork.” Meat Science 93.3 (2013): 538-546.\nStep 1 - Load libraries\nNote that these need to be installed beforehand.\n\nlibrary(ggplot2)\nlibrary(ggbiplot)\n\nStep 2 - Load the data\nRemember to set your working directory.\n\nload(\"meat_data.RData\")\n\nStep 3 - Calculate the PCA model\nThe data frame X consists of \\(p = 20\\) columns, however only the last 18 are response variables, whereas the first two refers to the study design. In PCA only the response variables are used to calculate the model, whereas the design is used for e.g. coloring of the score plot.\n\nPCA &lt;- prcomp(X[,3:20], center = TRUE, scale. = TRUE)\n\nStep 4 - Plot the model\nThe ggbiplot() function nicely plots a so-called biplot. Below a lot of features are added to the plot, but ggbiplot(PCA) will produce something which is similar.\n\nggbiplot(PCA, groups = X$Treatment, \n         obs.scale = 1, var.scale = 1, \n         ellipse = T, ellipse.fill = F) + # Create the biplot\n  ggtitle(\"PCA on sensory data, colored according to treatment\") + # Add title\n  ylim(-4, 6) + # Set y-axis limits\n  xlim(-7, 6) + # Set x-axis limts\n  theme_bw() + # Set theme as \"black and white\"\n  theme(plot.title = element_text(hjust = 0.5, face = \"bold\")) # Adjust title \n\n\n\n\n\n\n\nFigure 5.1: A biplot on PC1 and PC2 based on the meat preservation sensory data. Colored according to treatment. GT = Green tea; RE = rosemary extract.\n\n\n\n\n\nIn figure 5.1 above you see a biplot of the PCA model on the sensory data on the sausages from both weeks. The points represent the scores for all samples, colored according to antioxidant treatment.The ellipse = T in the R-code draws ellipses representing the distribution of each treatment. The arrows indicate the loadings of the sensory variables. The scores can be used to evaluate the differences between samples, and the underlying reason behind the differences is interpreted through the directionality and magnitude of the loadings. It is evident that there is a difference between the control and treated samples, since they are grouped differently, primarily in the PC1 direction. When compared to the loadings it seems that the control samples are characterised by more old, and rancid tastes and smells, which are sensorical attributes of lipid oxidation. Furhter the control samples have a boiled egg texture compared to the treated samples. On the other hand the treated samples are more spicy and bitter/acidic in taste and grey in color.\nThere are not registered any greater difference between the two green tea and rosemary extract samples.\n\n\n5.1.2 Example: Near Infrared Spectroscopy of Marzipan - PCA\nThe following example illustrates how principal component analysis (PCA) can be used to explore your data. The dataset in this example consists of 32 measurements on marzipan bread (marzipanbrød) made from 9 different recipes. The measurements have been acquired using near infrared spectroscopy (NIR) where light is passed through a sample and the transmitted light analysed. The output measurement is a spectrum showing how much light the sample has absorbed at each wavelenght.\nStep 1 - Load packages and data\nWe start by loading the relevant libraries and importing the data.\n\nlibrary(ggplot2)\nload(\"Marzipan.RData\")\n\nStep 2 - Inspect raw spectra\nWe can now plot the spectra colored according to sugar content (figure 5.2)\n\nggplot(Xm, aes(wavelength, value, color = sugar)) +\n  geom_line() +\n  scale_colour_gradient(low = \"green\", high = \"red\") +\n  labs(title = \"Marzipan NIR spectrum\",\n       y = \"Absorbance\",\n       x = \"Wavelength (nm)\",\n       color = \"Sugar content (%)\") +\n  theme_bw()\n\n\n\n\n\n\n\nFigure 5.2: 32 NIR spectra measured on 9 different marcipan breads colored according to sugar content.\n\n\n\n\n\nLooking at the raw spectral data we see that there is a concentration gradient in the spectra when we colour according to the sugar content. It seems that the main variation in the spectra has something to due with the sugar content.\nWe can also plot the same spectra colored according to recipe (figure 5.3).\n\n\nCode\n# Extract recipe from variable name\nXm$recipe &lt;- substr(Xm$variable, 1, 1)\n\nggplot(Xm, aes(wavelength, value, color = recipe)) +\n  geom_line() +\n  labs(title = \"Marzipan NIR spectrum\",\n       y = \"Absorbance\",\n       x = \"Wavelength (nm)\",\n       color = \"Recipe\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nFigure 5.3: 32 NIR spectra measured on 9 different marcipan breads colored according to recipe.\n\n\n\n\n\nHere we see that we can distinguish some of the recipes from each other. This can be explained by the varying sugar content in the recipes. Also, if we look in the region below 1100 nm and into the visible (≈ 370 − 750 nm) we note that samples made with recipe c is different compared to the other samples.\nStep 3 - Calculate PCA model\nWe now make a PCA on the data and plot PC1 vs PC2 coloured according to sugar content.\n\n# Transposing the data and removing the wavelength column\nXt = t(X[,-1])\n\n# Making PCA on mean centered Xt\nmarzipan = prcomp(Xt, center = TRUE, scale = FALSE)\n\n# Extracting scores for plotting\nscores = data.frame(marzipan$x, sugar = Y$sugar)\n\n# Extracting % explained variance for plotting\nvarPC1 = round(summary(marzipan)$importance[2,1]*100)\nvarPC2 = round(summary(marzipan)$importance[2,2]*100)\n\nStep 4 - Plot the scores\nWe can now plot the scores colored according to sugar content (figure 5.4).\n\nggplot(scores, aes(PC1, PC2, color = sugar)) +\n  geom_point(size = 3) +\n  scale_color_gradient(low = \"green\", high = \"red\") +\n  labs(x = paste(\"PC1 - \",varPC1, \"%\", sep = \"\"), # Insert exp.var. as label\n       y = paste(\"PC1 - \",varPC2, \"%\", sep = \"\"),\n       color = \"Sugar content (%)\") +\n  theme_bw()\n\n\n\n\n\n\n\nFigure 5.4: Score plot for NIR spectra of marzipan bread colored by sugar content.\n\n\n\n\n\nWe see that PC1 explains 61% of the variation in the data and that it seems to capture the variation in the sugar content. The samples are ordered from left to right in increasing concentration. Also, a group of samples are laying away from the rest when looking at PC2 which is explaining 33% of the variation in the data.\nWe can also color the scores according to recipe (figure 5.5).\n\n# Extract recipe from sample name\nscores$recipe &lt;- substr(Y$sample, 1, 1)\n\nggplot(scores, aes(PC1, PC2, color = recipe)) +\n  geom_point(size = 3) +\n  labs(x = paste(\"PC1 - \",varPC1, \"%\", sep = \"\"), # Insert exp.var. as label\n       y = paste(\"PC1 - \",varPC2, \"%\", sep = \"\"),\n       color = \"Recipe\") +\n  theme_bw()\n\n\n\n\n\n\n\nFigure 5.5: Score plot for PCA based on the NIR spectra of marzipan bread colored by recipe.\n\n\n\n\n\nIf we look at PC2 we see that it is the samples from recipe c that is laying away from the other samples. What is the reason for that? Let us look at the loadings. We start by looking at the second loading (figure 5.6) as it is dividing the samples from recipe c from the other samples.\n\n# Extract loadings from PCA model\nloadings = as.data.frame(marzipan$rotation)\n\nggplot(loadings, aes(wl, PC2)) +\n  geom_line(linewidth = 1) +\n  labs(x = \"Wavelength (nm)\",\n       y = \"2nd loading\"\n       ) +\n  theme_bw()\n\n\n\n\n\n\n\nFigure 5.6: Loading plot for PCA based on the NIR spectra of marzipan bread.\n\n\n\n\n\nThe main contribution to PC2 is the peak around 550 nm. So the reason why the samples from recipe c is different from the other is related to colour. This actually makes sense as this recipe has cocoa powder added to the recipe which will influence the colour of the marzipan bread.\nLastly, we look at the first loading (figure 5.7).\n\nggplot(loadings, aes(wl, PC1)) +\n  geom_line(linewidth = 1) +\n  labs(x = \"Wavelength (nm)\",\n       y = \"1st loading\"\n       ) +\n  theme_bw()\n\n\n\n\n\n\n\nFigure 5.7: Loading plot for PCA based on the NIR spectra of marzipan bread.\n\n\n\n\n\nIt is not straight forward to see which peaks are related to sugar. However, the peaks around 1200, 1400, 1875 and 2100 nm has the highest magnitude and therefore the main reason for the sugar content gradient we see in PC1. Actually all 4 peaks are related to either the C-H (Carbon - Hydrogen) or O-H groups in sugar or O-H in water. You will learn more about assigning peaks to chemical information in other courses later on.",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Principal Component Analysis (PCA)</span>"
    ]
  },
  {
    "objectID": "chapters/week1/pca.html#reading-material",
    "href": "chapters/week1/pca.html#reading-material",
    "title": "5  Principal Component Analysis (PCA)",
    "section": "5.2 Reading material",
    "text": "5.2 Reading material\n\nVideos on PCA\n\nPCA main ideas by StatQuest\nPCA Introduction 1 by Rasmus Bro\nPCA Introduction 2 by Rasmus Bro\n\nChapter 2 in Biological Data analysis and Chemometrics by Brockhoff\nChapter 4 (4.1 to 4.5) in Chemometrics With R: Multivariate Data Analysis in the Natural Sciences and Life Sciences by Ron Wehrens (2012). Springer, Heidelberg. Available in Absalon.",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Principal Component Analysis (PCA)</span>"
    ]
  },
  {
    "objectID": "chapters/week1/pca.html#videos-on-pca",
    "href": "chapters/week1/pca.html#videos-on-pca",
    "title": "5  Principal Component Analysis (PCA)",
    "section": "5.3 Videos on PCA",
    "text": "5.3 Videos on PCA\n\n5.3.1 PCA main ideas - StatQuest\n\n\n\n5.3.2 PCA Introduction 1 - Rasmus Bro\n\n\n\n5.3.3 PCA Introduction 2 - Rasmus Bro",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Principal Component Analysis (PCA)</span>"
    ]
  },
  {
    "objectID": "chapters/week1/pca.html#exercises",
    "href": "chapters/week1/pca.html#exercises",
    "title": "5  Principal Component Analysis (PCA)",
    "section": "5.4 Exercises",
    "text": "5.4 Exercises\n\n\n\n\n\n\nExercise 5.1 - McDonalds Data\n\n\n\n\n\n\nThe purpose of this exercise is to get familiar with PCA on a small intuitive dataset. The data - McDonaldsScaled.xlsx - constitutes of different fast food products and their nutritional content.\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nRead in the data using an appropriate package / function (e.g. r read_excel() from the package readxl), and set up the data with row names etc.\n\n\n\n\n\n\nMcD &lt;- read_xlsx(\"McDonaldsScaled.xlsx\")\n\n# The first column has no name - change that\ncolnames(McD)[1] &lt;- \"Item\"\n\nhead(McD)\n\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nMake some initial descriptive plots for the five response variables, that indicate the distribution (center and spread).\nMake some bi-variate scatter plots examining the relation between different variables, and comment on whether this relation is obvious and further, which types of samples are responsible for the relation. You can use the ggplot2 with stat_smooth() for this. Set method = \"lm\" to choose a linear model and se = F to disable the confidence intervals around the line.\n\n\n\n\n\n\nggplot(McD, aes(Energy, Protein)) +\n  stat_smooth(method = \"lm\", se = F) + # Plot a linear model\n  geom_point(size = 3) +\n  geom_text(aes(label = Item)) +\n  theme_bw()\n\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nNow, make a PCA on the data. What does the two options center = ... and scale. = ... refer to?\n\n\n\n\n\n\nMcD_pca &lt;- prcomp(McD[2:6], center = TRUE, scale. = TRUE)\n\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nIf \\(X_1, X_2, .., X_{19}\\) is the protein variable in the dataset (i.e. \\(X_1\\) is the protein content of sample 1 and so forth), how do you calculate the centered and scaled representation of data, which could be called: \\(X_1^{auto}, X_2^{auto}, .., X_{19}^{auto}\\)?\nPlot the PCA results and comment on them. There are several ways of doing this. The first described here, is to zack out the parameters (scores and loadings) and then use the ggplot2 functionality to plot those. Try to comprehend what is actually produced from the list of functions listed below.\n\n\n\n\n\n\n# Zack out the individual parameters (scores and loadings and item names)\nscores &lt;- data.frame(McD_pca$x)\nloadings &lt;- data.frame(McD_pca$rotation)\nItems &lt;- McD$Item\n\n# Score plot\nggplot(scores, aes(PC1, PC2, label = Items)) +\n  geom_text()\n\n# Loading plot\nggplot(loadings, aes(PC1, PC2, label = row.names(loadings))) +\n  geom_text()\n\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nAlternatively you can utilize a package ggbiplot for making nice plots from PCA objects (see the first PCA example for more info on how to use this package).\n\n\n\n\n\n\nggbiplot(McD_pca, obs.scale = 1, var.scale = 1) +\n  geom_text(label = Items) +\n  theme_bw()\n\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nMake a vector that indicates the different fast food types (Burger, Drinks, etc.). You can either do this by extending the excel file with a column, or do it in R (see this below). Infer this class information on the plot as color (or maker shape or size).\n\n\n\n\n\n\nCategories &lt;- c(\n  rep(\"Burger\", 9),\n  rep(\"Drink\", 3),\n  rep(\"Icecream\", 3),\n  rep(\"Other\", 2),\n  rep(\"Salad\", 2)\n)\n\nggbiplot(McD_pca, obs.scale = 1, var.scale = 1) +\n  geom_text(aes(label = Items, color = Categories)) +\n  theme_bw()\n\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nTry to modify the PCA by removing scaling and/or centering. What happens to the plots of the results? What do you think is going on?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 5.2 - Analysis of coffee serving temperatura - PCA\n\n\n\n\n\n\nIn the dataset Results Panel.xlsx the sensorical results from a panel of eight judges, evaluating coffee served at six different temperatures each four times are listed. In this exercise, we are going to first average over judge and temperature followed by PCA to evaluate sensorical descriptor similarity as well as the effect of serving temperature on the perception of coffee.\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nAfter import of data, and initial sanity check, calculate the average response across the four replicates. Use the summarise() or aggreggate() function with mean functions to make a dataset with the average response for the six different temperatures for each judge. For an example on how to do this see Section A.3.\n\nHint: The number of samples should be reduced by a factor of 4.\n\nUse this data as input for construction of a PCA model. Which variables do you think should be included?\nMake a biplot of this PCA model and interpret it.\n\nWhich descriptors go together and which are oppositely correlated?\nAre there, from this analysis, a clear difference between the different serving temperatures?\nWhat do you think blurs the picture?\n\n\n\n\n\n\nThe code below can be used for inspiration. Be aware, that we need to set a series of dependencies in order for this to work.\n\nUsing summarise()Using aggregate()\n\n\n\n# Calculate mean per group\ncoffee_ag &lt;- coffee |&gt;\n  summarise(\n    across(where(is.numeric), mean), # Compute mean over numeric variables \n    .by = c(Assessor, Sample) # Group by judge and temp\n  )\n\n# Compute PCA model\ncoffee_pca &lt;- prcomp(coffee_ag[your_variables_here])\n\n# Plot PCA model\nggbiplot(coffee_pca, groups = coffee_ag$Temp,\n         ellipse = T, circle = T, ellipse.fill = F) +\n  scale_color_brewer(\n    palette = \"RdBu\", \n    direction = -1,\n    name = \"Temperature\"\n    ) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n# Define variables to group by\ngroup_by &lt;- list(\n  \"Judge\" = coffee$Assessor, \n  \"Temp\" = coffee$Sample\n  )\n\n# Calculate mean per group\ncoffee_ag &lt;- aggregate(coffee, group_by, mean)\n\n\n# Compute PCA model\ncoffee_pca &lt;- prcomp(coffee_ag[your_variables_here])\n\n# Plot PCA model\nggbiplot(coffee_pca, groups = coffee_ag$Temp,\n         ellipse = T, circle = T, ellipse.fill = F) +\n  scale_color_brewer(\n    palette = \"RdBu\", \n    direction = -1,\n    name = \"Temperature\"\n    ) +\n  theme(legend.position = \"bottom\")\n\n\n\n\nCheck out the ggbiplot() syntax (by ?ggbiplot). by adding stuff to the plot, it is modified to look exactly like you want it. Here we change legend appearance (for inferring temperature) and color scheme for the scores matching temperature. In order to also remove variation due to differences between judges, the dataset is compressed such that the rows reflect the average for each temperature (across judges and replicates). Then this dataset is used for constructing a PCA model and visualized.\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nuse the aggreggate() function to average across judges and replicates.\n\nHint: Modify the .by = argument in summarise() or the group_by-list in aggregate().\n\nMake a PCA model on this dataset. Visualise and interpret it.\nUse the code below to vizualize the model again - Can you figure out why this model on a data-matrix of 6 samples is different from the previous model (on a data-matrix of 48 samples)?\n\n\n\n\n\n\n# Plot PCA model\nggbiplot(coffee_pca, groups = coffee_ag$Temp,\n         ellipse = T, circle = T, ellipse.fill = F) +\n  scale_color_brewer(\n    palette = \"RdBu\", \n    direction = -1,\n    name = \"Temperature\"\n    ) +\n  theme(legend.position = \"bottom\")\n\nThe plots are hard to interpret because\n\nsome of the labels are masked and\nthe points are way to small.\n\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nUse the function xlim(c(low,high)) and geom_point(, aes(color = cofffee_ag$Temp) size = 5) and add them to the plot to fix these problems.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 5.3 - Wine aromas\n\n\n\n\n\n\nThis exercise will take you through plotting, descriptive stats and PCA. Wine based on the same grape variety (Cabernet Sauvignon) from four different countries (Argentina, Australia, Chile and South Africa) were analyzed for aroma compound composition with GC-MS (gas chromatography coupled with mass spectrometry). The dataset can be found in the file “Wine.xlsx”, and it will form the basis for working with basic descriptive statistics, plots and PCA.\n\nDescriptive statistics\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nStart by importing the dataset “Wine.xlsx” to R and try to get an overview of it.\n\nHint: use the summary() function in R and/or have a look at the raw data in the Excel file.\n\n\nHow many wines were analyzed from each country?\nHow many variables are there in the dataset, and how many constitutes the aroma profile?\n\n\n\n\n\n\nFor the descriptive statistics, only two of the aroma compounds are selected. Choose two on your own or make the calculations for the aroma compounds benzaldehyde (almond like aroma) and 3-Methylbutyl acetate (sweet fruit/banana like aroma).\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nCalculate mean, variance, standard deviation, median and inner quartile range for the selected aroma compounds from each of the four different countries.\n\nHint: it can be helpful to create a separate dataset for each country, which can be done with the function subset().\n\n\n\n\n\n\n\n\nPlots\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nMake a boxplot, a jitterplot and a combination of the two with all 4 countries in one plot. Use the R-commands from the notes as inspiration.\nWhat do you see? Discuss pros and cons of the different plots.\nAdjust the layout of your favorite plots (e.g. color, background, title etc.). Think about how the data is presented in the best way. Actually, it can be rather beneficial to specify a generic theme including title and label font size, background color of the plot etc, which then can be added to each plot produced.\n\n\n\n\n\n\n\nPCA\nWorking with a dataset with many variables, PCA provides a very nice tool to give an overview of the dataset. First we define the data we want to include in the analysis. With logical indexing wine[4:20] we choose the variables to include (i.e. we are only interested in analyzing the aroma compounds).\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nUse the `prcomp function to calculate a PCA model on scaled aroma data. What does the arguments scale. = T and center = T do the to the data?\n\nTry changing them to = F (false).\n\nMake a score plot and a loading plot.\n\n\n\n\n\n\n# Compute PCA model\nwine_pca &lt;- prcomp(wine[4:58], scale. = T, center = T)\n\n# Create score plot (biplot without arrows)\nggbiplot(wine_pca, groups = wine$Country, point.size = 3,\n         var.axes = F)\n\n# Extract loadings and loading names\nwine_loadings &lt;- data.frame(wine_pca$rotation)\nloading_names &lt;- rownames(wine_loadings)\n\n# Create \"manual\" loading plot\nggplot(wine_loadings, aes(PC1, PC2)) +\n  geom_text(aes(label = loading_names,\n                color = loading_names),\n            show.legend = F) +\n  theme_bw()\n\n# Create biplot\nggbiplot(wine_pca, groups = factor(wine$class), point.size = 3)\n\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nWhat do you see in the score and the loadings plot?\nCan you see a grouping of the data? If so, how are the groups different?",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Principal Component Analysis (PCA)</span>"
    ]
  },
  {
    "objectID": "chapters/week2/correlation.html",
    "href": "chapters/week2/correlation.html",
    "title": "6  Correlation",
    "section": "",
    "text": "6.1 Correlation and covariance - in short\nA covariance or correlation is a scalar measure for the association between two (response-) variables. Covariance bewteen two variables \\(X = (x_1, x_2, ..., x_n)\\) and \\(Y = (y_1, y_2, ..., y_n)\\) is defined as:\n\\[\n\\text{cov}_{XY} = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})}{n - 1}\n\\]\nCovariance depends on the scale of data (\\(X\\) and \\(Y\\)), and as such is hard to interpret. The correlation is however a scale in-variant version\n\\[\n\\text{corr}_{XY} = \\frac{\\text{cov}_{XY}}{s_X \\cdot s_Y},\n\\]\nwhere \\(s_X\\) and \\(s_Y\\) are the standard deviation for \\(X\\) and \\(Y\\) respectively (see 2.4.1 for details).\nDividing the covariance by the individual standard deviations put the correlation coefficient in the range between −1 and 1\n\\[\n-1 \\leq \\text{corr}_{XY} \\leq 1.\n\\]\nA correlation (and covariance) close to zero indicates that there are no association between the two variables (see figure 6.1).\nCode\nset.seed(4321)\n\nx &lt;- 1:30\nn &lt;- length(x)\n\ndata.frame(\n  \"x\" = x,\n  \"y1\" = x,\n  \"y2\" = x + rnorm(n, sd = 6),\n  \"y3\" = x + rnorm(n, sd = 10),\n  \"y4\" = rep(11, n) + rnorm(n, sd = 10),\n  \"y5\" = -x + rnorm(n, sd = 3),\n  \"y6\" = -x + rnorm(n, sd = 20)\n) |&gt; \n  pivot_longer(cols = !x) |&gt; \n  ggplot(aes(x, value)) +\n  geom_smooth(method = \"lm\", se = F, \n              linewidth = .6, color = \"black\") +\n  geom_point(size = 2, color = \"steelblue\", alpha = .5) +\n  stat_cor(\n    aes(label = after_stat(r.label)),\n    geom = \"label\",\n  ) +\n  labs(y = \"y\") +\n  facet_wrap(~ name, scales = \"free_y\") +\n  theme_bw() +\n  theme(\n    strip.background = element_blank(),\n    strip.text = element_blank()\n  )\n\n\n\n\n\n\n\n\nFigure 6.1: Correlation \\(R\\) between one variable x and different y variables.",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "chapters/week2/correlation.html#correlation-and-covariance---in-short",
    "href": "chapters/week2/correlation.html#correlation-and-covariance---in-short",
    "title": "6  Correlation",
    "section": "",
    "text": "6.1.1 Example: Natural Phenolic Antioxidants for Meat Preservation - Correlation\nThis example continues where we left of in example Section 5.1.1 with the sensory data on the meat sausages treated with green tea (GT) and rosemary extract (RE) or control.\nLoad the data and libraries\nRemember to set your working directory.\n\nlibrary(ggplot2)\nlibrary(gridExtra) # For showing multiple plots in a grid\nload(\"meat_data.RData\")\n\n\n\nCode\npca &lt;- prcomp(X[,3:20], center = TRUE, scale. = TRUE)\n\n# Define loadings to keep\nshow_loadings &lt;- c(\"Color_Pink\", \"Smell_Old\", \"Taste_Old\", \n                  \"Taste_Bitter\", \"Color_Grey\")\n\n# Remove loadings\npca$rotation[!rownames(pca$rotation) %in% show_loadings, ] &lt;- 0\n\nggbiplot(pca,\n         select = rownames(pca$rotation) %in% show_loadings,\n         alpha = 0, \n         varname.size = 4) +\n  labs(title = \"Loading plot\") +\n  theme_bw() +\n    theme(plot.title = element_text(hjust = .5, face = \"bold\"))\n\n\n\n\n\n\n\n\nFigure 6.2: Select loadings from the PCA model computed in the previous example.\n\n\n\n\n\nIn figure 6.2 the loading plot corresponding to some of the loadings found in the PCA model calculated in the previous example is shown. The selected variables show positive, negative and non-correlated loadings. In a PCA loading plot, loadings with opposite directions are negatively correlated, while loadings pointing in the same direction are possitively correlated. Loadings which are orthogonal (90 degree angle) with respect to each other, are not correlated. For the plot above, this means that:\n\nTaste_Old and Smell_Old are positively correlated.\nTaste_Bitter and Color_Pink are uncorrelated.\nColor_Pink and Color_Grey are negatively correlated.\n\nTo check if the correlation holds for the raw data, scatter plots are made for each of the encircled three examples. (Note that the interpretation of these correlations are only valid with respect to the variation described in the model)\nCreate scatter plots\n\n# Define a common theme\nmy_theme &lt;- theme_bw() +\n  theme(plot.title = element_text(hjust = .5, face = \"bold\"))\n\n# Plot 1\nplt1 &lt;- ggplot(X, aes(Smell_Old, Taste_Old)) +\n  geom_point(size = 2) +\n  stat_smooth(method = \"lm\", se = F) +\n  ggtitle(\"Positive correlation\") +\n  my_theme\n\n# Plot 2\nplt2 &lt;- ggplot(X, aes(Taste_Bitter, Color_Pink)) +\n  geom_point(size = 2) +\n  stat_smooth(method = \"lm\", se = F) +\n  ggtitle(\"No correlation\") +\n  my_theme\n\n# Plot 3\nplt3 &lt;- ggplot(X, aes(Color_Grey, Color_Pink)) +\n  geom_point(size = 2) +\n  stat_smooth(method = \"lm\", se = F) +\n  ggtitle(\"Negative correlation\") +\n  my_theme\n\n# Arrange the plots in a 1 x 3 grid (requires gridExtra package)\ngrid.arrange(plt1, plt2, plt3, ncol = 3)\n\n\n\n\n\n\n\nFigure 6.3: The between three pairs of select variables from the meat data.\n\n\n\n\n\nThe correlation values are calculated using the cor() function.\n\n# Calculate correlation\ncor1 &lt;- cor(X$Smell_Old, X$Taste_Old)\ncor2 &lt;- cor(X$Taste_Bitter, X$Color_Pink)\ncor3 &lt;- cor(X$Color_Grey, X$Color_Pink)\n\n# Display in a fancy way\ncat(\"Correlation coefficients\",\n    \"\\n Smell_Old vs Taste_Old: \\t\\t\", cor1,\n    \"\\n Taste_Bitter vs Color_Pink: \\t\", cor2,\n    \"\\n Color_Grey vs Color_Pink: \\t\\t\", cor3)\n\nCorrelation coefficients \n Smell_Old vs Taste_Old:         0.595475 \n Taste_Bitter vs Color_Pink:     0.007446373 \n Color_Grey vs Color_Pink:       -0.604673\n\n\nConclusions\nHere, it can be seen that the pink and grey color attributes, which are oppositely directed in the PCA loadings, display a moderate negative correlation in the raw data. The pink color and bitter taste, which are orthogonal to each other in the loadings, are not correlated at all. The old smell and old taste with similar directionality in the PCA are moderately postively correlated. With regards to food chemistry, do you think these conclusions make sense?",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "chapters/week2/correlation.html#reading-material",
    "href": "chapters/week2/correlation.html#reading-material",
    "title": "6  Correlation",
    "section": "6.2 Reading material",
    "text": "6.2 Reading material\n\nVideos on correlation and covariance\n\nCovariance, clearly explained\nPearson’s correlation, clearly explained\nCorrelation Doesn’t Equal Causation: Crash Course Statistics #8\nWhat is correlation?\n\nChapter 1.4.3 and 2.5 of Introduction to Statistics by Brockhoff\nChapter 2 in Biological Data analysis and Chemometrics by Brockhoff",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "chapters/week2/correlation.html#videos-on-correlation-and-covariance",
    "href": "chapters/week2/correlation.html#videos-on-correlation-and-covariance",
    "title": "6  Correlation",
    "section": "6.3 Videos on correlation and covariance",
    "text": "6.3 Videos on correlation and covariance\n\n6.3.1 Covariance, clearly explained - StatQuest\n\n\n\n6.3.2 Pearson’s correlation, clearly explained - StatQuest\n\n\n\n6.3.3 Correlation doesn’t equal causation - CrashCourse\n\n\n\n6.3.4 What is correlation - Melanie Maggard",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "chapters/week2/correlation.html#exercises",
    "href": "chapters/week2/correlation.html#exercises",
    "title": "6  Correlation",
    "section": "6.4 Exercises",
    "text": "6.4 Exercises\n\n\n\n\n\n\nExercise 6.1 - Correlation between aroma compounds\n\n\n\n\n\n\n\nCorrelation\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nWhat does a correlation coefficient of -1, 0 or +1 tell you?\nIn the table below the amount of Nonanal and Ethyl.2.methyl.propanoate in the six Argentine wines are listed. Fill out the blank spaces in the table and calculate the correlation coefficient (r).\n\nHelp can be found in example 1.19 in Introduction to Statistics by Brockhoff.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWine number (\\(i\\))\n1\n2\n3\n4\n5\n6\n\n\n\n\nNonanal (\\(X_i\\))\n0.003\n0.003\n0.005\n0.006\n0.008\n0.005\n\n\nEthyl.2.methyl.propanoate (\\(Y_i\\))\n0.106\n0.165\n0.150\n0.155\n0.149\n0.141\n\n\n\\(X_i - \\bar{X}\\)\n\n\n\n\n\n\n\n\n\\(Y_i - \\bar{Y}\\)\n\n\n\n\n\n\n\n\n\\((X_i - \\bar{X})(Y_i - \\bar{Y})\\)\n\n\n\n\n\n\n\n\n\n\nRemember that the sample covariance and correlation are given as\n\\[\n\\text{cov}_{XY} = s_{XY} = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})}{n - 1},\n\\]\n\\[\n\\text{corr}_{XY} = r_{XY} = \\frac{\\text{cov}_{XY}}{s_X \\cdot s_Y}.\n\\]\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nMake a plot with Nonanal vs. Ethyl.2.methyl.propanoate and calculate the correlation coefficient in R (use the function cor() in R). Include only wines from Argentina.\nCalculate the correlation coefficient for a, b, c and d, where you include wine data from all the countries.\n\nDiethyl.succinate and Ethyl.lactate\nEthyl.acetate and Ethanol\n2.Butanol and Ethyl.hexanoate\nBenzaldehyde and Hexyl.acetate\n\nWhat happens with the covariance and correlation if we multiply the Diethyl.succinate amount with 10 or −4?\nWhat happens with the covariance and correlation if we add 10 or 100 to the Diethyl.succinate amount?\n\n\n\n\n\n\n\nPCA\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nMake a PCA including wines from all the countries (similar to the one from week 1 on the same dataset).\n\nWhich variables are responsible for the grouping of the countries?\n\nCompare the calculated correlation coefficients in question 4 with the loading plot of the PCA in question 5.\n\nHow does the position of the variables in the loading plot make the correlation coefficients negative, positive, close to \\(\\pm\\) 1 or 0?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 6.2 - Covariance and Correlation - by hand\n\n\n\n\n\n\nCertain types of characteristics ”go together”, for instance will a variable that measures the chocolate smell of some food type be related to a variable measuring the chocolate taste of the same food type. We talk about these two types of information being correlated. Below is 48 corresponding measures of the sensorical attributes sour and roasted for coffees at different serving temperatures and different judges (each based on the average of four measurements). Calculate the correlation between these two variables based on the metrics listed below.\n\n\n\n\nObservation\nSour\nRoasted\n\n\n\n\n1\n1.70\n1.06\n\n\n2\n1.93\n1.10\n\n\n3\n3.59\n3.87\n\n\n4\n2.30\n1.86\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n45\n1.30\n1.89\n\n\n46\n0.26\n1.93\n\n\n47\n1.12\n1.05\n\n\n48\n3.84\n5.41\n\n\n\\(\\sum\\)\n91.59\n76.42\n\n\n\\(\\hat{\\sigma}^2\\)\n0.90\n0.94\n\n\n\\(\\sum_{XY}\\)\n\n172.73\n\n\n\n\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nCalculate mean and standard deviation for the two variables using the statistics listed below data. That is WITHOUT importing data into the computer!\nThe covariance between the variables is 0.57. Calculate the correlation between the two variables.\nWhat happens with the covariance and correlation if we multiply the Sour ratings with 2 or -3?\nWhat happens with the covariance and correlation if we add 10 or -1234 to the Roasted ratings?\nAs an upcoming coffee expert, why do you think these two attributes are correlated?\nHARD! Calculate \\(\\sum (X - \\bar{X})(Y - \\bar{Y})\\) from \\(\\sum XY\\), \\(\\sum X\\) and \\(\\sum Y\\) (where X is Sour and Y is Roasted).\n\nHint: You need to multiply out the product of the two parenthesis and reduce the resulting part using the relation between \\(\\bar{X} = \\sum(X) / n\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 6.3 - Correlation and PCA\n\n\n\n\n\n\nIn this exercise the sensorical data from ranking of coffee served at different temperatures are used. The aim is to see how correlations is the vehicle for PCA analysis. The data is named “Results Panel.xlsx”.\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nImport the data and remove the replicate effect by averaging over these.\nMake a scatter plot of the attribute Sour versus Roasted. Comment on what you see in terms of relation between these two variables.\nNow make a comprehensive figure where all the sensorical attributes (there are 8) are plotted against each other.\nCalculate all the pairvise correlations between the variables. How does this correspond with the figure?\nMake a PCA model on this dataset (same as in (ex-analysis-of-coffee-inspection?)).\nComment on the (dis-)similarity between the correlation matrix, the multiple pairwise scatterplot and the PCA model.\n\n\n\n\n\nBelow is some code which might be useful for this purpose (You need to install or add dependencies via install.packages(GGally) or library(GGally) in order for the functions to be recognized by R). In the pairwise scatterplot a straight line is added by + geom_abline(), try also to add a smooth curve by + geom_smooth(). What are the difference between those two representation of similarity between the variables?\n\nggplot(coffee_ag, aes(Roasted, Sour)) +\n  geom_abline(color = \"red\", linewidth = 1) +\n  geom_point(size = 2.5, color = \"steelblue\", alpha = .75) +\n  theme_bw()\n\nggpairs(coffee_ag, columns = 4:11)\n\n\n\n\n\n\n\n\n\n\n\nExercise 6.4 - Olive oil adulteration\n\n\n\n\n\n\nQuick detection of adulteration of oils is of growing importance, since high quality oils, such as olive oil, are becoming increasingly popular and expensive, increasing the incentive for adulterating with cheaper oils. Spectroscopic techniques are the preferred measurement choice because they are quick, often non-destructive and in many cases highly selective for oil characterization.\nThe purpose of this exercise is to introduce you to how multivariate techniques, such as PCA, can be applied on spectral datasets. They are in fact, very useful on datasets such as these, because of the very high number of variables that can be included in the modelling.\nThe samples in this dataset are mixtures of olive oil and thistle oil. Olive oil and thistle oil are almost exclusively made up of triglycerides, consisting of a glycerol backbone with three fatty acid chains attached. An example of a triglyceride is shown in the top right of figure 6.4. Fatty acids are characterized by the amount of unsaturation. Olive oil consists mostly of monounsaturated fatty acids, while thistle oil is largely comprised of polyunsaturated fatty acids (ie: they have more double bonds). Additionally a few of the samples were spiked with a free trans fatty acid. The structure of the added trans fatty acid is shown in the top left of figure 6.4.\nThe samples were measured with infrared (IR) spectroscopy. Some of the relevant peaks for oil characterization are shown in the raw spectra in figure 6.4. The dataset consists of 30 oil samples and 1794 variables. The first 1790 variables are the absorbance at 1790 wavelengths. The last 4 columns in the dataset describe the concentration of olive oil, thistle oil and transfat, and lastly the sample ID.\nSince it is a bit more difficult to work with spectral data in R, we have decided to be merciful and give you the data directly as an .RData file, which you simply get into R by load(\"OliveOilAdult.RData\"). Be sure to be in the correct working directory, or add the path to the load command.\n\n\n\n\n\n\nFigure 6.4: Top right) Triglyceride. Left part: Glycerol backbone. Right part from top to bottom: Unsaturated, monounsaturated and polyunsaturated fatty acid. Top left) Trans fatty acid. Bottom) Raw IR spectrum of mixed oil.",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "chapters/week2/normal_distribution.html",
    "href": "chapters/week2/normal_distribution.html",
    "title": "7  The Normal Distribution",
    "section": "",
    "text": "7.1 Section 1",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>The Normal Distribution</span>"
    ]
  },
  {
    "objectID": "chapters/week2/normal_distribution.html#section-1",
    "href": "chapters/week2/normal_distribution.html#section-1",
    "title": "7  The Normal Distribution",
    "section": "",
    "text": "7.1.1 Section 2",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>The Normal Distribution</span>"
    ]
  },
  {
    "objectID": "chapters/week2/normal_distribution.html#exercises",
    "href": "chapters/week2/normal_distribution.html#exercises",
    "title": "7  The Normal Distribution",
    "section": "7.2 Exercises",
    "text": "7.2 Exercises\n\n7.2.1 Test",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>The Normal Distribution</span>"
    ]
  },
  {
    "objectID": "chapters/appendix/data_wrangling.html",
    "href": "chapters/appendix/data_wrangling.html",
    "title": "Appendix A — Wrangling data in R",
    "section": "",
    "text": "A.1 The pipe operator\nIn R you have the opportunity to use a special syntax called the pipe operator |&gt;. It can be thought of as a pipe that “sends data downstream” to the next call in your script.\nIf we for example want to take the square root of some data df and afterwards show the first 6 values of that data with head() we can write\ndf |&gt;\n  sqrt() |&gt;\n  head()\nwhich is equivalent to\nhead(sqrt(df))\nThis might not make much sense for this example, but when things get a bit more complex the pipe operator can really help by making code easier to read.\nIf we for example want to format some data df and then plot it in ggplot2 we can write the following script\ndf |&gt;\n  pivot_longer(\n    cols = everything(),\n    names_to = \"group\",\n    names_prefix = \"X\",\n    values_to = \"response\"\n    ) |&gt;\n  mutate(group = str_c(\"Group \", group)) |&gt;\n  ggplot(aes(group, response)) +\n  geom_boxplot()\nWithout the pipe we would need to create several intermediate variables cluttering up our script and environment in the process.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Wrangling data in R</span>"
    ]
  },
  {
    "objectID": "chapters/appendix/data_wrangling.html#tidy-data",
    "href": "chapters/appendix/data_wrangling.html#tidy-data",
    "title": "Appendix A — Wrangling data in R",
    "section": "A.2 Tidy data",
    "text": "A.2 Tidy data\n\nA.2.1 What is tidy data?\n\n\nA.2.2 How to convert to tidy data",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Wrangling data in R</span>"
    ]
  },
  {
    "objectID": "chapters/appendix/data_wrangling.html#sec-summarise-by-group",
    "href": "chapters/appendix/data_wrangling.html#sec-summarise-by-group",
    "title": "Appendix A — Wrangling data in R",
    "section": "A.3 Summarise by group",
    "text": "A.3 Summarise by group\nSometimes we want to calculate a statistic per group. There are many different ways of doing this, and in these examples we are going to present a way possible ways.\nFor these examples we use the Palmer Penguins dataset. It consists of some data regarding the bill size, flipper length and body mass of three penguin species across three islands.\nLet us start by taking a look at our data. This can be done via the head() function or the glimpse() function from the Tidyverse.\n\nhead(penguins) # Show the first 6 rows of dataframe\n\n# A tibble: 6 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           NA            NA                  NA          NA\n5 Adelie  Torgersen           36.7          19.3               193        3450\n6 Adelie  Torgersen           39.3          20.6               190        3650\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\n\nglimpse(penguins)\n\nRows: 344\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male…\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\n\n\nA.3.1 One statistic per group\nIf we want to summarise one statistic, that could be the mean or the standard deviation, grouped by species or island (or both), we can do it in the following ways.\n\nUsing TidyverseUsing base R\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIf the pipe operator |&gt; is unkown to you see Section A.1.\n\n\nTo compute the mean of all numeric variables we can use the summarise() function from the dplyr package in Tidyverse. Remember to load the tidyverse package (library(tidyverse)) - this package includes dplyr as well as a whole lot of other nice packages.\nFor this example we want the mean for all numeric variables grouped by species.\n\npenguins |&gt;\n  drop_na() |&gt; # Remove rows with missing values\n  summarise(\n    across(\n      .cols = where(is.numeric), # Chose columns that are numeric \n      .fns = mean # Set the function we want to use\n           ),\n    .by = species # Group by species\n    )\n\n# A tibble: 3 × 6\n  species   bill_length_mm bill_depth_mm flipper_length_mm body_mass_g  year\n  &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt;\n1 Adelie              38.8          18.3              190.       3706. 2008.\n2 Gentoo              47.6          15.0              217.       5092. 2008.\n3 Chinstrap           48.8          18.4              196.       3733. 2008.\n\n\nBe aware that the function drops all non-numeric variables that are not part of the grouping. So the output of the above code is “missing” the island and sex variables. Also, the drop_na() function removes all rows with missing values - if this is not done all columns with missing values will return NA.\nIf we want to group by multiple variables, for example species and island, we just need to pass a vector of the variables to the .by = argument.\n\npenguins |&gt;\n  drop_na() |&gt; # Remove rows with missing values\n  summarise(\n    across(where(is.numeric), mean),\n    .by = c(island, species) # Group by island and species\n  )\n\n# A tibble: 5 × 7\n  island    species   bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;     &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;       &lt;dbl&gt;\n1 Torgersen Adelie              39.0          18.5              192.       3709.\n2 Biscoe    Adelie              39.0          18.4              189.       3710.\n3 Dream     Adelie              38.5          18.2              190.       3701.\n4 Biscoe    Gentoo              47.6          15.0              217.       5092.\n5 Dream     Chinstrap           48.8          18.4              196.       3733.\n# ℹ 1 more variable: year &lt;dbl&gt;\n\n\n\n\nTo compute the mean of all variables grouped by species we can use the aggregate() function. This is included in base R, so there is no need to load any packages.\n\n# Remove rows with missing values \npenguins_clean &lt;- na.omit(penguins)\n\naggregate(penguins_clean, list(species = penguins_clean$species), mean)\n\n    species species island bill_length_mm bill_depth_mm flipper_length_mm\n1    Adelie      NA     NA       38.82397      18.34726          190.1027\n2 Chinstrap      NA     NA       48.83382      18.42059          195.8235\n3    Gentoo      NA     NA       47.56807      14.99664          217.2353\n  body_mass_g sex     year\n1    3706.164  NA 2008.055\n2    3733.088  NA 2007.971\n3    5092.437  NA 2008.067\n\n\nBe aware that all non-numeric variables will return NA (and thus a lot of warnings). Also, the na.omit() function removes all rows with missing values - if this is not done all columns with missing values will return NA.\nIf we want to group by multiple variables, for example species and island, we just need to pass another grouping variable to the list.\n\naggregate(penguins_clean, \n          list(species = penguins_clean$species,\n               island = penguins_clean$island), \n          mean)\n\n    species    island species island bill_length_mm bill_depth_mm\n1    Adelie    Biscoe      NA     NA       38.97500      18.37045\n2    Gentoo    Biscoe      NA     NA       47.56807      14.99664\n3    Adelie     Dream      NA     NA       38.52000      18.24000\n4 Chinstrap     Dream      NA     NA       48.83382      18.42059\n5    Adelie Torgersen      NA     NA       39.03830      18.45106\n  flipper_length_mm body_mass_g sex     year\n1          188.7955    3709.659  NA 2008.136\n2          217.2353    5092.437  NA 2008.067\n3          189.9273    3701.364  NA 2008.018\n4          195.8235    3733.088  NA 2007.971\n5          191.5319    3708.511  NA 2008.021\n\n\n\n\n\n\n\nA.3.2 Create a summary table\nSometimes we want to create a summary table grouped by some variable. This can be done in the following ways.\n\nUsing TidyverseUsing base R\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIf the pipe operator |&gt; is unkown to you see Section A.1.\n\n\nTo compute a summary table of a numeric variable we can use the summarise() function from the dplyr package in Tidyverse. Remember to load the tidyverse package (library(tidyverse)) - this package includes dplyr as well as a whole lot of other nice packages.\nFor this example we want some summary statistics for the body_mass_g variable grouped by species.\n\npenguins |&gt;\n  drop_na() |&gt; # Remove rows with missing values\n  summarise(\n    N = length(body_mass_g),\n    Mean = mean(body_mass_g),\n    Median = median(body_mass_g),\n    Std = sd(body_mass_g),\n    IQR = IQR(body_mass_g),\n    .by = species # Group by species\n  )\n\n# A tibble: 3 × 6\n  species       N  Mean Median   Std   IQR\n  &lt;fct&gt;     &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Adelie      146 3706.   3700  459.  638.\n2 Gentoo      119 5092.   5050  501.  800 \n3 Chinstrap    68 3733.   3700  384.  462.\n\n\nBe aware that the drop_na() function removes all rows with missing values - if this is not done all columns with missing values will return NA.\nIf we want to group by multiple variables, for example species and island, we just need to pass another grouping variable to the list.\n\npenguins |&gt;\n  drop_na() |&gt; # Remove rows with missing values\n  summarise(\n    N = length(body_mass_g),\n    Mean = mean(body_mass_g),\n    Median = median(body_mass_g),\n    Std = sd(body_mass_g),\n    IQR = IQR(body_mass_g),\n    .by = c(island, species) # Group by island and species\n  )\n\n# A tibble: 5 × 7\n  island    species       N  Mean Median   Std   IQR\n  &lt;fct&gt;     &lt;fct&gt;     &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Torgersen Adelie       47 3709.   3700  452.  662.\n2 Biscoe    Adelie       44 3710.   3750  488.  588.\n3 Dream     Adelie       55 3701.   3600  449.  588.\n4 Biscoe    Gentoo      119 5092.   5050  501.  800 \n5 Dream     Chinstrap    68 3733.   3700  384.  462.\n\n\n\n\nTo compute a summary table of a numeric variable we can use the aggregate() function. This is included in base R, so there is no need to load any packages.\nFor this example we want some summary statistics for the body_mass_g variable grouped by species.\n\n# Remove rows with missing values \npenguins_clean &lt;- na.omit(penguins)\n\n# Define variables for grouping\nmy_groups &lt;- list(species = penguins_clean$species)\n\n# Compute summary stats\npenguins_n &lt;- aggregate(penguins_clean, my_groups, length)\npenguins_mean &lt;- aggregate(penguins_clean, my_groups, mean)\npenguins_median &lt;- aggregate(penguins_clean, my_groups, mean)\npenguins_std &lt;- aggregate(penguins_clean, my_groups, mean)\npenguins_iqr &lt;- aggregate(penguins_clean, my_groups, mean)\n\n# Collect everything in a dataframe\npenguins_summary &lt;- data.frame(\n  \"Species\" = penguins_n$species,\n  \"N\" = penguins_n$body_mass_g,\n  \"Mean\" = penguins_mean$body_mass_g,\n  \"Median\" = penguins_median$body_mass_g,\n  \"Std\" = penguins_std$body_mass_g,\n  \"IQR\" = penguins_iqr$body_mass_g\n  )\n\nprint(penguins_summary)\n\n    Species   N     Mean   Median      Std      IQR\n1    Adelie 146 3706.164 3706.164 3706.164 3706.164\n2 Chinstrap  68 3733.088 3733.088 3733.088 3733.088\n3    Gentoo 119 5092.437 5092.437 5092.437 5092.437\n\n\nIf we want to group by multiple variables, for example species and island, we just need to pass another grouping variable to the my_groups list.\n\n# Define variables for grouping\nmy_groups &lt;- list(species = penguins_clean$species,\n                  island = penguins_clean$island)\n\n# Compute summary stats\npenguins_n &lt;- aggregate(penguins_clean, my_groups, length)\npenguins_mean &lt;- aggregate(penguins_clean, my_groups, mean)\npenguins_median &lt;- aggregate(penguins_clean, my_groups, mean)\npenguins_std &lt;- aggregate(penguins_clean, my_groups, mean)\npenguins_iqr &lt;- aggregate(penguins_clean, my_groups, mean)\n\n# Collect everything in a dataframe\npenguins_summary &lt;- data.frame(\n  \"Species\" = penguins_n$species,\n  \"Island\" = penguins_n$island,\n  \"N\" = penguins_n$body_mass_g,\n  \"Mean\" = penguins_mean$body_mass_g,\n  \"Median\" = penguins_median$body_mass_g,\n  \"Std\" = penguins_std$body_mass_g,\n  \"IQR\" = penguins_iqr$body_mass_g\n)\n\nprint(penguins_summary)\n\n    Species    Island   N     Mean   Median      Std      IQR\n1    Adelie    Biscoe  44 3709.659 3709.659 3709.659 3709.659\n2    Gentoo    Biscoe 119 5092.437 5092.437 5092.437 5092.437\n3    Adelie     Dream  55 3701.364 3701.364 3701.364 3701.364\n4 Chinstrap     Dream  68 3733.088 3733.088 3733.088 3733.088\n5    Adelie Torgersen  47 3708.511 3708.511 3708.511 3708.511",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Wrangling data in R</span>"
    ]
  },
  {
    "objectID": "chapters/week1/week_1.html",
    "href": "chapters/week1/week_1.html",
    "title": "Week 1",
    "section": "",
    "text": "Hand-in assignment\nExercise 5.2 Analysis of coffee serving temperatura - PCA is to be handed in (through Absalon or as hard-copy Wednesday night). You are welcome to put in R-code in the assignment, but it is your argumentation and interpretation that are the most important. If you have problems with R, then try to write what you would have done if you did not experience problems with the machinery.",
    "crumbs": [
      "Week 1"
    ]
  },
  {
    "objectID": "chapters/week1/week_1.html#exercises",
    "href": "chapters/week1/week_1.html#exercises",
    "title": "Week 1",
    "section": "Exercises",
    "text": "Exercises\nFor Monday work through the following exercises:\n\nExercise 2.1\nExercise 2.2\nExercise 4.1\nExercise 4.2\nExercise 4.3\n\nFor Wednesday work through the following exercise:\n\nExercise 4.4\nExercise 5.3\n\nYou will most likely not be able to complete all exercises within the hours in the classroom, so we recommend that you use some time in advance to initiate the task.",
    "crumbs": [
      "Week 1"
    ]
  },
  {
    "objectID": "chapters/week1/week_1.html#case-i",
    "href": "chapters/week1/week_1.html#case-i",
    "title": "Week 1",
    "section": "Case I",
    "text": "Case I\nThe first, of a total of four cases, are described in the document “Case1.pdf”. You should work on the case in groups of four, and hand in a slide-show with voice no later than Thursday evening next week. Be aware, that a lot of the technical stuff can be zacked from the exercises, so you might want to finalize those in advance of the case.",
    "crumbs": [
      "Week 1"
    ]
  },
  {
    "objectID": "chapters/week2/week_2.html",
    "href": "chapters/week2/week_2.html",
    "title": "Week 2",
    "section": "",
    "text": "Hand-in assigment\nExercise 6.1 Correlation between aroma compounds questions 1 – 6 (not the PCA part) is to be handed in (through Absalon or as hard-copy Wednesday night).",
    "crumbs": [
      "Week 2"
    ]
  },
  {
    "objectID": "chapters/week2/week_2.html#exercises",
    "href": "chapters/week2/week_2.html#exercises",
    "title": "Week 2",
    "section": "Exercises",
    "text": "Exercises\nFor Monday work through the following exercises:\n\nExercise 6.2\nExercise 6.3\n\nFor Wednesday work through the following exercise:\n\nExercise 6.4\n(ex-normal-distribution?)\n(ex-who-height-and-weight-standard-normal-distribution?)",
    "crumbs": [
      "Week 2"
    ]
  }
]