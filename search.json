[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Analysis for Food Science",
    "section": "",
    "text": "Preface\nDuring the past decades the production of data in relation to research, production, consumer behavior, social network etc. has increased dramatically. Today we are faced with data structures which were unimaginable just 50 years ago. Traditionally, a system under investigation were characterized by a few samples associated with say one to five descriptors and, carefully selected, responses. Today all aspects of the classical system interrogation has blown up, such that we have many more samples (e.g. production monitoring every minute), more descriptors (e.g. consumer characteristics), and by far more response variables (e.g. high throughput omics technologies). Tools developed for handling traditional scenarios still pertain the corner of how to approach today’s data analytical challenges, however, by the development of computers, it is possible to carry out challenging mathematical procedures in no time and further produce visual graphics as resources for translating information into knowledge. Due to this fact, the traditional tools has gotten a makeover and new tools has been developed.\nFood is, as such, an extremely inherent part of the human life, although one could argue that so is e.g. cardiovascular biology and governmental policy making, these subjects either work autonomously or does not demand everyday mental capacity. Everyday all humans need to eat- and drink in some social context, pay attention to the perception of the meal, and further deal with the possible health- and emotional implications of this process. When studying food science all these aspects are relevant.\nFood science constitute a broad range of disciplines spanning controlled artificial model systems, over functional modification of real food matrices, production technology, to the relation between food- and meal composition, taste, perception and health. All by means of data.\nThese notes are thought to cover data analysis within food science. That is to; provide a general understanding of the purpose of data analysis, found a theoretical- and practical basis for understanding various numerical and graphical tools and couple generic tools to concrete issues within related disciplines. To this end by theory, examples and exercises.\nThe Book material used in these notes are mostly from the notes for the course; Introduction to Statistics at DTU by P.B. Brockhoff and co workers. Additionally there are relevant chapters from other sources. All exercises are custom made and deal with real problems within food science.\nWelcome to the course in Fødevaredataanalyse for second year bachelor students in Food Science and Technology - Hope you will enjoy learning about how to use data for getting insight on food systems.\nMorten Arendt Vils Rasmussen\nAugust 2024",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "chapters/week1/week_1.html",
    "href": "chapters/week1/week_1.html",
    "title": "Week 1",
    "section": "",
    "text": "Hand-in assignment\nThis first week is going to introduce basic descriptive tools for getting a primary overview of data. These are divided into representative numerics, which we call descriptive statistics and various plots. Especially for the plotting part you will be needing a computer program. We strongly encourage you to get familiar with R, that, although not being as intuitive point-and-click as the widely used programs such as excel, is capable of conducting almost any type of sophisticated analysis you may wish, and further will strengthen you to become familiar with a scientific programming language - a generic competence useful whenever working with information.\nIncluded in the week 1 notes are material related to working in R; Installing packages, importing data, working in scripts and debugging your code. This material you should try to cover briefly and then use it when you get stuck on a problem throughout the course (and after). However, there is a huge amount of videos, tutorials, etc. on the web which you can also use, and we encourage you to get familiar with these resources as well. Simply type your problem in google and check if others have experienced something similar.\nExercise 5.2 Analysis of coffee serving temperatura - PCA is to be handed in (through Absalon or as hard-copy Wednesday night). You are welcome to put in R-code in the assignment, but it is your argumentation and interpretation that are the most important. If you have problems with R, then try to write what you would have done if you did not experience problems with the machinery.",
    "crumbs": [
      "Week 1"
    ]
  },
  {
    "objectID": "chapters/week1/week_1.html#exercises",
    "href": "chapters/week1/week_1.html#exercises",
    "title": "Week 1",
    "section": "Exercises",
    "text": "Exercises\nFor Monday work through the following exercises:\n\nExercise 2.1\nExercise 2.2\nExercise 4.1\nExercise 4.2\nExercise 4.3\n\nFor Wednesday work through the following exercises:\n\nExercise 4.4\nExercise 5.3\n\nYou will most likely not be able to complete all exercises within the hours in the classroom, so we recommend that you use some time in advance to initiate the task.",
    "crumbs": [
      "Week 1"
    ]
  },
  {
    "objectID": "chapters/week1/week_1.html#case-i",
    "href": "chapters/week1/week_1.html#case-i",
    "title": "Week 1",
    "section": "Case I",
    "text": "Case I\nThe first, of a total of four cases, are described in the document “Case1.pdf”. You should work on the case in groups of four, and hand in a slide-show with voice no later than Thursday evening next week. Be aware, that a lot of the technical stuff can be zacked from the exercises, so you might want to finalize those in advance of the case.",
    "crumbs": [
      "Week 1"
    ]
  },
  {
    "objectID": "chapters/week1/r_basics.html",
    "href": "chapters/week1/r_basics.html",
    "title": "1  R Basics",
    "section": "",
    "text": "1.1 Installing packages\nA package in R is a set of commands which are not a part of the base-set in R. Many of the R-commands which are used throughout this course requires a certain package to be installed on the computer/Mac. It is a good idea to get familiar with Installing packages and loading them onto your R-script mainly so you won’t be missing them at the exercises, casework or examination.\nIn R there are two important commands concerning installation of packages.\nFor example: install.packages(’readxl’) Installs the package readxl on the computer and remove.packages(’readxl’) uninstalls the package readxl from the computer.",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R Basics</span>"
    ]
  },
  {
    "objectID": "chapters/week1/r_basics.html#installing-packages",
    "href": "chapters/week1/r_basics.html#installing-packages",
    "title": "1  R Basics",
    "section": "",
    "text": "install.packages() installs the target package on your computer.\nremove.packages() uninstalls the package from your computer.\n\n\n\n1.1.1 Loading a package\nWhen the packages are installed on the computer, you can load them onto your workspace/script at every occasion you initiate your analysis in R. To do this, you use the library() command. library() points at a package-library stored on your computer. Everytime you open a new session of R, you need to load the needed packages again.\nFor example, library(readxl) Loads the package readxl onto the workspace.\nWhen you load a package, you might get warning messages like the following:\n\nlibrary(readxl)\n\nAdvarselsbesked:\npakke ‘readxl’ blev bygget under R version 3.1.3",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R Basics</span>"
    ]
  },
  {
    "objectID": "chapters/week1/r_basics.html#sec-r-basics-working-directory",
    "href": "chapters/week1/r_basics.html#sec-r-basics-working-directory",
    "title": "1  R Basics",
    "section": "1.2 Working directory",
    "text": "1.2 Working directory\nIn R you are using something called a working directory or wd for short. This is the folder on your computer in which R saves and finds the projects that you are working on. This also makes it easier to load datasets. The working directory can be changed in R either manually or through code. getwd() and setwd() are the two important commands for changing the working directory.\n\ngetwd() # Show the current working directory\n\n“/Users/madsbjorlie/Documents/Statistik/Exercises/Week 1”\n\nsetwd(\"~/Documents/R-træning\") # Change the working directory \ngetwd() # Show the current working directory\n\n“/Users/madsbjorlie/Documents/R-træning”",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R Basics</span>"
    ]
  },
  {
    "objectID": "chapters/week1/r_basics.html#importing-a-dataset",
    "href": "chapters/week1/r_basics.html#importing-a-dataset",
    "title": "1  R Basics",
    "section": "1.3 Importing a dataset",
    "text": "1.3 Importing a dataset\nThroughout this course you will need to import a lot of data into R. Getting familiar with the following packages and commands will help minimize your R-related frustration. Datasets can be imported into R in numerous ways. Like changing the working directory, it can be done both manually and through coding. We recommend doing it through coding since this makes it easier to maintain an overview.\nAlmost all of the datasets that will be handed out in this course will be in both the excel file-type .xlsx, as well as the .RData format. R is also capable of importing text-files such as .txt or .csv.\n.xlsx-files are Microsoft Excel’s standard project file type, whereas .csv-files are short for comma separated values and is a term for text-files where the values are separated by a comma (or in the Danish Excel, a semicolon).\nYou can either load .RData files, import datasets through R’s inherent commands or use some data-import packages to import file-types such as .xlsx or .xls. Both methods works fine and which one you will use depends on your personal preference.\n\n1.3.1 Importing an .RData file\nIf someone imported and stored the data as an .RData file, you can simply import it using the load() function. For this you do not need any libraries.\n\nload(file.choose())\nload(\"~/Documents/..../Beer GCMS.RData”)\n\nThe only difference in comparison with the import-methods below, is that you do not “pipe” (the &lt;- function) the object into something you name yourself. The data object will retain the name as it was saved with. However, if you like your objects to be named something special (like X), then simply just add a line below the load() where you define it: e.g., X &lt;- beer.\n\n\n1.3.2 Importing a dataset through R’s own commands\nAs a default, R can not import Excel-files such as .xls and .xlsx. To use R’s read.csv() function, you need to save the Excel dataset as a .csv file. This is done by choosing (in Excel) and then selecting the .csv file-type. This might seem a bit tedious, but it eliminates the demand for other packages.\nread.csv() imports the dataset specified in the parenthesis. This can be done in two ways: by typing the path to the file on your computer or by using the command file.choose() which corresponds to opening a new file. If the dataset is in the working directory, you do not have to type the full path, but just the file-name.\nFor example:\n\nBeer &lt;- read.csv(file.choose(), header=TRUE, sep=\";\", dec=\",\")  \nBeer &lt;- read.csv(”Beerdata.csv”, header=TRUE, sep=\";\", dec=\",\")  \nBeer &lt;- read.csv(\"~/Documents/R-traening/Øldata.csv”, header=TRUE, sep=\";\", dec=\",\")\n\nThe different arguments: header =, sep =, and dec = tells R how to import the data. header=TRUE tells R that the first row in the dataset is not a part of the data itself but carries the variable names. sep=”;” defines which separator the document uses. By using Danish Excel, this will always be semicolon. This can be checked by opening the dataset in NotePad on windows or TextEditor on Mac. dec=”,” defines which symbol is used for decimals. It is necessary to make sure that the dataset in R is separated by a full stop rather than a comma. This can be checked by using summary commands after the data has been imported.\n\n\n1.3.3 Importing a dataset using packages\nBy using various packages, it is possible to import Excel-documents directly into R. This can be quite handy, but some of the packages will not run on Mac or on Windows due to other programs missing. The most commonly used data-import packages are: gdata, readxl, xlsx and rio. gdata requires Perl which is default on Mac and Linux but not on Windows and therefor it will not run on Windows unless it is installed. The following is a couple of examples using the various packages.\n\nlibrary(readxl)  \nBeer &lt;- read_excel(file.choose())  \nBeer &lt;- read_excel(\"~/Documents/R-traening/Beerdata.xls”)  \nBeer &lt;- read_excel(\"~/Documents/R-traening/Beerdata.xlsx”)  \n\nlibrary(gdata)  \nBeer &lt;- read.xls(file.choose())  \nBeer &lt;- read.xls(\"~/Documents/R-traening/Beerdata.xls”)  \n\nlibrary(xlsx)  \nBeer &lt;- read.xlsx(file.choose(), sheetIndex = 1)  \nBeer &lt;- read.xlsx(\"~/Documents/R-traening/Beerdata.xls”, sheetIndex = 1)  \nBeer &lt;- read.xlsx(\"~/Documents/R-traening/Beerdata.xlsx”, sheetIndex = 1)  \n\nlibrary(rio)  \nBeer &lt;- import(file.choose())  \nBeer &lt;- import(\"~/Documents/R-traening/Beerdata.xls”)  \nBeer &lt;- import(\"~/Documents/R-traening/Beerdata.xlsx”)  \n\n\n\n1.3.4 Getting an overview of the dataset\nWhen the dataset is imported into R, you can use different commands to check that it was imported correctly. The commands are head(), str() and dim().\n\nhead() shows the first 6 rows in the dataset.\n\nstr() shows the types of the various columns, such as numeric and factor.\n\ndim() shows the dimensions of the data-matrix.\n\n\nCoffee &lt;- read.csv(file.choose(), header=TRUE, sep=\";\", dec=\",\")\nhead(Coffee)\n\n\n\n  Sample Assessor Replicate Intensity  Sour Bitter\n1    31C        1         1      9.30  6.90   6.75\n2    31C        1         2      8.70  8.10   7.95\n3    31C        1         3      9.75  8.70  10.20\n4    31C        1         4     11.70 10.95  11.40\n5    31C        2         1      8.70  5.70  11.40\n6    31C        2         2      8.70  9.30  10.35\n\n\n\nstr(Coffee)\n\n\n\n'data.frame':   192 obs. of  6 variables:\n $ Sample   : Factor w/ 6 levels \"31C\",\"37C\",\"44C\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ Assessor : int  1 1 1 1 2 2 2 2 3 3 ...\n $ Replicate: int  1 2 3 4 1 2 3 4 1 2 ...\n $ Intensity: num  9.3 8.7 9.75 11.7 8.7 ...\n $ Sour     : num  6.9 8.1 8.7 10.9 5.7 ...\n $ Bitter   : num  6.75 7.95 10.2 11.4 11.4 ...\n\n\n\ndim(Coffee) # show dimensions of data\n\n\n\n[1] 192   6",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R Basics</span>"
    ]
  },
  {
    "objectID": "chapters/week1/r_basics.html#scripts",
    "href": "chapters/week1/r_basics.html#scripts",
    "title": "1  R Basics",
    "section": "1.4 Scripts",
    "text": "1.4 Scripts\nWe highly recommend that you make your data analysis using a script. A script is simply a flat text file that is given the surname .R such that R can interpret the commands. Here you will have the commands needed to do the analysis from setting necessary functions, import of data, initial inspection, modeling and plots.\nEach analysis task is slightly different, however, almost always there is a set of generic tasks which is always needed. That is: cleaning up the workspace, loading packages, setting work directory, loading data and checking the data structure. That typically fills up the first 5-10 lines of code in every script as follows:\n\nrm(list = ls()) # remove all variables in environment\ngraphics.off() # delete all generated plots\n\nlibrary(ggplot2) # load package ggplot2\nlibrary(readxl) # load package readxl\n\nsetwd(\"~/MyComputer/Courses/FDA/Exercises/Week1\") # set working directory\n\ndata &lt;- read_excel(\"SomeData.xlsx\") # load file\n\nhead(data) # show first 6 lines of data\n\n# plot data\nggplot(df, aes(x, y, color = treatment)) + \n    geom_point()",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R Basics</span>"
    ]
  },
  {
    "objectID": "chapters/week1/r_basics.html#rmarkdown",
    "href": "chapters/week1/r_basics.html#rmarkdown",
    "title": "1  R Basics",
    "section": "1.5 Rmarkdown",
    "text": "1.5 Rmarkdown\nUsing RStudio makes it possible to combine scripts (your data analysis) with output (figures, tables and numbers) and narrative (the fairytale on why and discussions etc.) in ONE document. It is rather simple when you get used to how it works, and it really makes life much more easy. Both for this course, but also for all other tasks which involves data analysis and is to be presented as a report.\nIt is refereed to as reproducible data analysis, and is the opposite of Excel-Hell, where the latter is characterized by being non transparent, and really hard to figure out what happened between data and results.\nCheck it out by browsing Rmarkdown and try it yourself!",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R Basics</span>"
    ]
  },
  {
    "objectID": "chapters/week1/descriptive_statistics.html",
    "href": "chapters/week1/descriptive_statistics.html",
    "title": "2  Descriptive statistics",
    "section": "",
    "text": "2.1 Reading material",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Descriptive statistics</span>"
    ]
  },
  {
    "objectID": "chapters/week1/descriptive_statistics.html#reading-material",
    "href": "chapters/week1/descriptive_statistics.html#reading-material",
    "title": "2  Descriptive statistics",
    "section": "",
    "text": "Chapter 1 of Introduction to Statistics by Brockhoff\n\nEspecially section 1.1 to 1.4.\n\nVideo lecture on central metrics (mean and median).\nVideo lecture on dispersion (variance, standard deviation etc.)\nVideo lecture on both central metrics and dispersion.",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Descriptive statistics</span>"
    ]
  },
  {
    "objectID": "chapters/week1/descriptive_statistics.html#exercises",
    "href": "chapters/week1/descriptive_statistics.html#exercises",
    "title": "2  Descriptive statistics",
    "section": "2.2 Exercises",
    "text": "2.2 Exercises\n\n\n\n\n\n\nExercise 2.1 - Descriptive statistics by hand\n\n\n\n\n\n\nBelow (table 2.1) is listed a vector of ranking (Liking) of coffee served at 56°C by 52 consumers. The data is sorted.\n\n\n\n\nTable 2.1: Liking of coffee served at 56°C as ranked by 52 consumers.\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n\n2\n2\n3\n3\n4\n4\n4\n4\n4\n4\n\n\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n\n\n5\n6\n6\n6\n6\n6\n6\n6\n6\n6\n\n\n6\n6\n7\n7\n7\n7\n7\n7\n7\n7\n\n\n7\n7\n7\n7\n7\n7\n7\n8\n8\n8\n\n\n8\n9\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSome useful numbers:\n\\[\\sum{X} = 301\\]\n\\[\\sum{(X_i - \\bar{X})^2} = 122.7\\]\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nCalculate mean, variance, standard deviation, median and inner quartile range for this distribution of data.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 2.2 - Descriptive statistics\n\n\n\n\n\n\nServing temperature of coffee seems of importance as to how this drink is perceived. However, it is not totally clear how this relation is. In order to understand this, studies on the same type of coffee served at different temperature is conducted. In this exercise we are going to use the data from a consumer panel of 52 consumers, evaluating coffee served at six different temperatures on a set of sensorical descriptors leading to a total of \\(52 \\times 6 = 312\\) samples.\nIn the dataset the results are listed. Taking these data from A to Z involves descriptive analysis for understanding variation within judge, between judge and between different temperatures, further outlier detection, and finally determination of structure between sensorical descriptors. In this exercise we are only going through some of the initial descriptive steps.\nIn the table below (table 2.2) a subset of the data is shown.\n\n\n\n\nTable 2.2: A subset of the Results Consumer Test.xlsx data\n\n\n\n\n\n\n\nSample\nTemperatur\nAssessor\nServingOrder\nTemperatureJudgment\nLiking\nIntensity\nSour\nBitter\nSweet\nMale\nFemale\n\n\n\n\n1\n31C\n31\n1\n6\n2\n3\n4\n3\n4\n2\n1\n0\n\n\n2\n31C\n31\n2\n6\n2\n3\n7\n5\n8\n3\n1\n0\n\n\n3\n31C\n31\n3\n6\n1\n3\n2\n1\n4\n6\n0\n1\n\n\n4\n31C\n31\n4\n6\n1\n2\n5\n6\n4\n3\n1\n0\n\n\n5\n31C\n31\n5\n6\n2\n2\n2\n3\n2\n1\n1\n0\n\n\n6\n31C\n31\n6\n6\n2\n4\n3\n4\n2\n1\n1\n0\n\n\n307\n62C\n62\n47\n6\n8\n8\n8\n2\n8\n1\n0\n1\n\n\n308\n62C\n62\n48\n6\n6\n7\n7\n4\n3\n3\n0\n1\n\n\n309\n62C\n62\n49\n6\n5\n8\n6\n6\n4\n6\n1\n0\n\n\n310\n62C\n62\n50\n6\n6\n5\n7\n7\n7\n4\n0\n1\n\n\n311\n62C\n62\n51\n6\n7\n6\n8\n6\n7\n2\n1\n0\n\n\n312\n62C\n62\n52\n6\n6\n7\n6\n6\n7\n3\n1\n0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nImport the data\n\nBe aware that the function read.xls() is not in the base library, so you need to add the specific library to your computer.\n\nSubsample on one temperature.\n\nBelow (listing 2.1) is listed two alternatives for doing this.\n\n\n\n\n\n\n\n\n\n\nListing 2.1: Importing and subsampling based on temperature.\n\n\nCoffee &lt;- read_excel( \" Results Consumer Test . xlsx \" )\nCoffee_t44_v1 &lt;- Coffee[Coffee$Temperatur == 44,]\nCoffee_t44_v2 &lt;- subset(Coffee, Temperatur == 44)\n\nmean(Coffee_t44_v1$Liking)\n\n\n\n\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nCalculate the descriptive statistics for centrality (mean and median), dispersion (IQR, standard deviation and range) and extremes (min and max) for this distribution of datapoints for a single descriptor (e.g. )\nNow do it for all temperatures.\n\nYou should get something like the table below (table 2.3).\n\n\n\n\n\n\n\n\n\n\nTable 2.3: Summary table computed in R.\n\n\n\n\n\n\nTemp\nN\nMean\nMedian\nStd\nMin\nMax\n\n\n\n\n31\n52\n3.576923\n3\n1.649078\n1\n7\n\n\n37\n52\n4.750000\n5\n1.780890\n1\n7\n\n\n44\n52\n5.826923\n6\n1.605397\n2\n9\n\n\n50\n52\n5.961538\n6\n1.596092\n2\n8\n\n\n56\n52\n5.788462\n6\n1.550920\n2\n9\n\n\n62\n52\n6.173077\n6\n1.367998\n2\n8\n\n\n\n\n\n\n\n\n\n\nThis can be quite tedious, and result in a lot of coding. However, the function summary() and aggreggate() are very efficient in producing such results. Try to check out these functions and see if you can use those to generate summary statistics. Below are shown some code which does exactly what you want without too many lines of code.\n\n\n\n\nListing 2.2: Generating a summary table with aggregate().\n\n\n# Include only responses\nCoffeeDT &lt;- Coffee[,2:10]\n\n# Run aggregate for each type of summary\ntmpN &lt;-aggregate(CoffeeDT,by=list(CoffeeDT$Temperatur),FUN = 'length')\ntmpM&lt;-aggregate(CoffeeDT,by=list(CoffeeDT$Temperatur),FUN = 'mean')\ntmpM2&lt;-aggregate(CoffeeDT,by=list(CoffeeDT$Temperatur),FUN = 'median')\ntmpS&lt;-aggregate(CoffeeDT,by=list(CoffeeDT$Temperatur),FUN = 'sd')\ntmpMi&lt;-aggregate(CoffeeDT,by=list(CoffeeDT$Temperatur),FUN = 'min')\ntmpMx&lt;-aggregate(CoffeeDT,by=list(CoffeeDT$Temperatur),FUN = 'max')\n\n# Merge these into a dataset\ntmp &lt;- cbind(tmpM$Temperatur,tmpN$Liking,tmpM$Liking,tmpM2$Liking,\n             tmpS$Liking,tmpMi$Liking,tmpMx$Liking)\n\n# Add a meaningfull label for each coloumn\ncolnames(tmp) &lt;- c('Temp','N','Mean','Median','Std','Min','Max') \nprint(tmp)\n\n\n\n\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nThe above is done for , try to do it for some of the other responses.\n\nHINT: This can be done by repeating the code and exchange $Liking with e.g. $Bitter. However, putting this in a for loop is another option.\n\nWhat have you learned from analysing these data in terms of importance of serving temperature on the sensorical properties as percieved by consumers?\n\nHINT: You can run the code below to get a comprehensive overview. This is based on the mean aggreggate, but you might just as well check some of the other descriptive metrics. For instance, what does the standard deviation tells you about consumers in general, and does the type of sensorical attribute and serving temperature make a difference on the spread in scoring?\n\n\n\n\n\n\n\n\n\n\nListing 2.3: Code for plotting the results of listing 2.2.\n\n\nmatplot(tmpM[,2],tmpM[,6:10],type='l',lwd=3)\ntext(cbind(60,t(tmpM[6,6:10])),colnames(tmpM[,6:10]))\n\n\n\n\nYou might want to fix some of the labels in these figures. Check the documentation by typing ?matplot and see how to add meaning full stuff to the plot.",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Descriptive statistics</span>"
    ]
  },
  {
    "objectID": "chapters/week1/debugging.html",
    "href": "chapters/week1/debugging.html",
    "title": "3  Debugging - Getting R to work",
    "section": "",
    "text": "3.1 Reading material\nWhen using any computer program you now and then encounter that it does not do as intended. However, it is so, that the program do exactly what it is told, which might not be in line with the task you anticipate conducted. R work by interpreting commands which are either written directly on the command line, or in the form of lines in a script which then is submitted to the R compiler. Sometimes instead of producing nice results and plots, R returns red stuff on the screen. Debugging is the process of figuring out why that is so, and change the code to do as anticipated.\nWe will throughout the course get used to debug code, why the learning objectives for this set of skills stretches over several weeks. In detail you should:\nThe notes on debugging Debugging in R.pdf available through Absalon.",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Debugging - Getting R to work</span>"
    ]
  },
  {
    "objectID": "chapters/week1/debugging.html#exercises",
    "href": "chapters/week1/debugging.html#exercises",
    "title": "3  Debugging - Getting R to work",
    "section": "3.2 Exercises",
    "text": "3.2 Exercises\n\n\n\n\n\n\nExercise 3.1\n\n\n\n\n\n\nFor some of you, coding in Rstudio may seem simple. The aim with these debugging tasks is to train you to analyze the errors Rstudio gives you, and to give you some tools to use to avoid issues when coding in Rstudio. Many of the debugging exercises throughout the course will be related to the datasets used in other exercises given in the same week, so you might find it sensible to do the debugging-exercises first, to get to know the datasets and their potential issues, before the struggle starts.\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nWhy won’t Rstudio read in the file in the following cases?\n\n\n\n\n\n\ncoffee &lt;- read_excel(\"Results Consumer Test.xlsx\")\n\nError: could not find function “read_excel”\n\ncoffee &lt;- read_excel(\"Results Consumer Test\")\n\nError: “Results Consumer Test” does not exist in current working directory”\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nWhy isn’t the view-function working in the following case?\n\n\n\n\n\n\ncoffee &lt;- read_excel(\"Results Consumer Test.xlsx\")\nview(coffee)\n\nError: could not find function “view”\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nWhat is missing when you call for aggregate to do its calculations?\n\n\n\n\n\n\ncoffee &lt;- read_excel(\"Results Consumer Test.xlsx\")\ncoffee_ag &lt;- aggregate(by = list(coffee$Assessor, coffee$Sample), FUN = \"sd\")\n\nError in is.ts(x) : argument “x” is missing, with no default",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Debugging - Getting R to work</span>"
    ]
  },
  {
    "objectID": "chapters/week1/plotting.html",
    "href": "chapters/week1/plotting.html",
    "title": "4  Plotting",
    "section": "",
    "text": "4.1 Reading material",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Plotting</span>"
    ]
  },
  {
    "objectID": "chapters/week1/plotting.html#reading-material",
    "href": "chapters/week1/plotting.html#reading-material",
    "title": "4  Plotting",
    "section": "",
    "text": "Chapter 1 in Introduction to Statistics by Brockhoff\n\nEspecially section 1.5 to 1.6.\n\nggplot2 cheat sheet. It is a very nice cheat sheet on how to use ggplot2.\n\nCheat sheets for other libraries also exist. You can find them here.\n\nOnline lectures: There is a lot of how-to-plot on the web. Use it when you are getting stuck at a problem, or if you feel like being inspired.\n\nThese are nice and short:\n\nCharts Are Like Pasta - Data Visualization Part 1: Crash Course Statistics #5.\nPlots, Outliers, and Justin Timberlake: Data Visualization Part 2: Crash Course Statistics #6.\n\nThis one is pretty comprehensive:\n\nVisualising data with ggplot2. Lecture by Hadley Wickham.",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Plotting</span>"
    ]
  },
  {
    "objectID": "chapters/week1/plotting.html#exercises",
    "href": "chapters/week1/plotting.html#exercises",
    "title": "4  Plotting",
    "section": "4.2 Exercises",
    "text": "4.2 Exercises\nThe package ggplot2 for R is a versatile tool for producing various plots with the option of modifying them in great detail. The following exercises should guide you through the basic functionality of the ggplot function and show you how you can layer different geoms to produce the plots you want.\n\n\n\n\n\n\nBefore you start\n\n\n\nStart by importing a dataset and specify some necessary packages (If you have not installed them on your local drive, you might need to do so). The following lines of code will do the job, if you specify the correct working directory.\n\n# Load package for importing Excel-files\nlibrary(readxl) \n\n# Import coffee data set\ncoffee &lt;- read_excel(\"Results Consumer Test.xlsx\")\n\nThere are several ways of importing data into R, depending on which format you have them in. From excel, the read.xls() (from the gdata library) or read_excel() (from the readxl library) are two ways of doing it. In either case, make sure that the data is correctly imported, by comparing the imported data with the original - sometimes there are problems with decimals.\n\n\nAssume that you have a single response variable, for instance alcohol content of a series of various types of drinks, measurements of body weight from an nutritional experiment or content of antioxidants for a given product produced under different conditions. For all of these, the variable is continuous in form. As a starting point of every analysis an overview of the distribution of the variable of interest is crucial, and plotting the distribution facilitate insight into character of distribution (bell shaped, skewness, bi modal, zero inflated etc.) and single point information for outlier identification.\n\n\n\n\n\n\nExercise 4.1 - Plotting distributions with histograms\n\n\n\n\n\n\nThe histogram is the most basic representation of continuous data. A simple histogram can be produced in the following way with ggplot2.\n\nggplot(data = coffee, aes(Liking)) +\n  geom_histogram()\n\n\n\n\n\n\n\nFigure 4.1: A simple histogram of the “Liking” variable in the coffee consumer dataset.\n\n\n\n\n\nThere are numerous additional arguments that can be used to modify this plot. Check out the documentation or google it to see how it is done.\nThe documentation can be found by running ?geom_histogram() or by having the text cursor in the function name while pressing F1.\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nChange the color of the bars\nChange the bin width.\nChange the transparency of the bars (aka. alpha-value).\nAdd a title to the plot.\n\n\n\n\n\nThe data here are ”liking” of coffees at different temperatures, and so one might wish to infer this information in the histogram. This can be done by coloring by sample name.\n\nggplot(coffee, aes(Liking, fill = Sample)) +\n  geom_histogram(\n    position = \"dodge\", # Plot the colors side-by-side\n    binwidth = 0.8 # Set the width of the bins\n  )\n\n\n\n\n\n\n\nFigure 4.2: A histogram of the “Liking” variable in the coffee consumer dataset. The histogram is now colored by the sample name.\n\n\n\n\n\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nLook at figure 4.2, how does the temperature affect the liking?\n\n\n\n\n\nIf you do not want to overlay the histograms, it is possible to plot them as individual panels. Try running the following code and see what it does.\n\nggplot(coffee, aes(Liking)) +\n  geom_histogram() +\n  facet_wrap(~ Sample) # Wrap by sample\n\n\n\n\n\n\n\n\n\n\n\nExercise 4.2 - Plotting distributions with densitograms\n\n\n\n\n\n\nA densitogram is a smoothed extension of the histogram, and as such represents the same type of information. The bin width in the histogram controls the resolution, whereas the counterpart in the densitogram is the degree of smoothing. By changing the geom used in the ggplot call the plot is changed to a densitogram.\n\nggplot(coffee, aes(Liking)) +\n  geom_density()\n\n\n\n\n\n\n\nFigure 4.3: A densitogram of the “Liking” variable in the coffee consumer dataset.\n\n\n\n\n\nAgain, there are numerous additional arguments that can be used to modify the plot. Read the documentation and try some of them out.\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nModify the smoothing of the densitogram by changing the option adjust =\nTry to make the smoothing very refined (e.g. adjust = 0.3). Does this reflect the underlying distribution? What is a suitable smoothing option for these data?\n\n\n\n\n\nExactly as for the histogram, it is possible to infer additional information on the densitogram.\n\nggplot(coffee, aes(Liking, fill = Sample)) +\n  geom_density()\n\n\n\n\n\n\n\nFigure 4.4: A densitogram of the “Liking” variable in the coffee consumer dataset. The plot is now colored according to the “Sample” variable.\n\n\n\n\n\nThis plot is not optimal, as only the densitogram for temperature 62C is fully visible.\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nTry adjusting the transparency by of the plot by setting alpha = 0.5.\n\n\n\n\n\nThe colors used in the plot could be more intuitive as they refer to temperature. There are several ways the to add a different color scheme. One is to add a layer to the plot specifying either a predefined color scheme, a modification of a predefined color scheme or simply by specifying each of the colors used. Here we just use a predefined Red to Blue-palette from ColorBrewer.\n\nggplot(coffee, aes(Liking, fill = Sample)) +\n  geom_density() +\n  scale_fill_brewer(\n    palette = \"RdBu\"\n    )\n\n\n\n\n\n\n\nFigure 4.5: A densitogram of the “Liking” variable in the coffee consumer dataset. The plot is now colored according to the “Sample” variable using the Red to Blue-palette from ColorBrewer.\n\n\n\n\n\nHowever, the colors are in a counter intuitive direction. This can easily be corrected for by adding direction = -1 to the color scheme call.\n\nggplot(coffee, aes(Liking, fill = Sample)) +\n  geom_density() +\n  scale_fill_brewer(\n    palette = \"RdBu\",\n    direction = -1\n  )\n\n\n\n\n\n\n\nFigure 4.6: A densitogram of the “Liking” variable in the coffee consumer dataset. The plot is now colored according to the “Sample” variable using the Red to Blue-palette from ColorBrewer - this time in the “correct” direction.\n\n\n\n\n\nIf nothing seems to fit your ideal color-world, then you can simply specify the exact colors (HINT: try googling color codes for ggplot2 ).\n\ncustom_colormap &lt;- c(\"#0033FF\", \"#0099FF\",\"#00EEFF\", \n                     \"#FFCCCC\", \"#FF9999\",\"#FF0000\")\n\nggplot(coffee, aes(Liking, fill = Sample)) +\n  geom_density() +\n  scale_fill_manual(\n    values = custom_colormap\n  )\n\n\n\n\n\n\n\nFigure 4.7: A densitogram of the “Liking” variable in the coffee consumer dataset. The plot is now colored according to the “Sample” variable using manually specified colors.\n\n\n\n\n\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nTry to reconstruct these plots, and try to use other predefined color schemes. Further all these plots suffers from lack of transparency, so fix that as well.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 4.3 - Boxplot, Jitterplot and Violinplot\n\n\n\n\n\n\nIn the above, the different temperatures were infered on the plot by overlaying histograms. However, the x-axis can be used for keeping track of this information. This is especially useful when you are comparing more than four levels. The code below produces four different plots for this purpose.\n\nggplot(coffee, aes(Sample, Liking, fill = Sample)) +\n  geom_boxplot() +\n  scale_fill_brewer(\n    palette = \"RdBu\",\n    direction = -1\n  ) +\n  labs(title = \"Boxplot\") +\n  theme_linedraw()\n\n\n\n\n\n\n\nFigure 4.8: A boxplot of the “Liking” variable in the coffee consumer dataset.\n\n\n\n\n\n\nggplot(data = coffee, \n       aes(Sample, Liking, color = Sample, shape = Sample)\n) +\n  geom_jitter() +\n  labs(title = \"Jitter plot\") +\n  theme_light()\n\n\n\n\n\n\n\nFigure 4.9: A jitter plot of the “Liking” variable in the coffee consumer dataset.\n\n\n\n\n\n\nggplot(data = coffee, \n       aes(Sample, Liking, fill = Sample, shape = Sample)\n) +\n  geom_boxplot() +\n  geom_jitter() +\n  scale_fill_brewer(\n    palette = \"RdBu\",\n    direction = -1\n  ) +\n  labs(title = \"Box n' jitter\") +\n  theme_minimal()\n\n\n\n\n\n\n\nFigure 4.10: A box n’ jitter plot of the “Liking” variable in the coffee consumer dataset.\n\n\n\n\n\n\nggplot(data = coffee, \n       aes(Sample, Liking, fill = Sample, shape = Sample)\n) +\n  geom_violin() +\n  scale_fill_brewer(\n    palette = \"RdBu\",\n    direction = -1\n  ) +\n  labs(title = \"Violin plot\") +\n  theme_classic()\n\n\n\n\n\n\n\nFigure 4.11: A boxplot of the “Liking” variable in the coffee consumer dataset.\n\n\n\n\n\nThere are several things to notice from the plots above:\n\nThe title of a plot can be added by using labs(title = \"Title goes here) or ggtitle(\"Title goes here).\nThe background, and some other stuff, of the plots are different, and is inferred by adding + theme_XXX() (the default is theme is theme_gray()).\nIn the boxplot the colors are added by fill = Sample and in the jitter plot the colors are added by color = Sample. This is because the the points in the jitter plot do not have a “fill” (they are not “filled” with a color).\n\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nTry to reconstruct these plots.\nThe boxplot has several features, such as a box with a line in the middle, some so-called whiskers and also maybe a few actual points. Check out what these refer to in the data, and calculate them directly on data to verify that the computer is not off.\n\n\n\n\n\n\n\n\n\n\n\nHow to save plots\n\n\n\nThe plots can saved as high resolution files by using the ggsave-function. Unless otherwise specified the plots are saved in the same aspect ratio as seen in the plot window of RStudio.\n\n# Save plot as PNG with a resoultion of 300 DPI (standard print resolution)\nggsave(\"MyPlot.png\", dpi = 300) \n\n# Save plot as jpeg with a resoultion of 72 dpi\nggsave(\"MyPlot.jpeg\", dpi = 72) \n\n# Save plot as pdf\nggsave(\"MyPlot.pdf\") \n\nThe plot is saved in the current working directory.\nSometimes it can be necessarry to play around with the scaling factor scale =. Try increasing it if the text looks too small.\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 4.4 - Analysis of coffee serving temperature - data inspection\n\n\n\n\n\n\nServing temperature of coffee seems of importance on how this drink is perceived. However, it is not totally clear how this relation is. In order to understand this, studies on the same type of coffee served at different temperature is conducted. In this exercise we are going to use the data from a trained Panel of eight judges, evaluating coffee served at six different temperatures on a set of sensorical descriptors. Each judge is presented with each temperature in a total of four replicates leading to a total of \\(6 \\times 8 \\times 4 = 192\\) samples.\nIn the dataset Results Panel.xlsx the results are listed. Taking these data from A to Z involves descriptive analysis for understanding variation within judge, between judge and between different temperatures, further outlier detection, and finally determination of structure between sensorical descriptors. In this exercise we are only going to briefly explore the data with emphasis on uncertainty.\nThis is data from a trained panel, meaning each judge have been trained to be an objective instrument returning the same response when presented the same sample. However, there is always uncertainty in such responses, and especially when the instrument is a human being. We are interested in how big the deviation is between the four replicates, across judges and samples.\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nImport the data and check that it is matching the excel file using head().\nUse the summarise() or aggreggate() function to extract certain descriptive measures (e.g. mean or standard deviation) from the data.\nPlot this descriptive measure for a single descriptor across temperature (x-axis) and join the points from the same judge.\nWhat can you say about the individual judges? And is scoring more difficult for higher temperature than lower?\n\n\n\n\n\nThe code below does (some) of the job.\n\nClassic RTidyverse\n\n\n\n# Compute the mean of the replicates\ncoffee_ag &lt;- aggregate(coffee, \n                       by = list(coffee$Assessor, coffee$Sample), \n                       FUN = mean)\n\n# Rename some of the variables \ncoffee_ag &lt;- rename(coffee_ag, c(Judge = Group.1, Temp = Group.2))\n\n# Make an initial plot of the result\nggplot(coffee_ag, aes(Temp, Intensity, color = Temp)) +\n  geom_boxplot() +\n  geom_jitter()\n\n\n\n\ncoffee |&gt; \n  group_by(Assessor, Sample) |&gt;\n  summarise(across(\n    .cols = where(is.numeric), # Choose all numeric columns\n    .fns = mean # Compute mean\n  )) |&gt; \n  ggplot(aes(Temperatur, Intensity, color = Sample)) +\n  geom_boxplot() +\n  geom_jitter()",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Plotting</span>"
    ]
  },
  {
    "objectID": "chapters/week1/pca.html",
    "href": "chapters/week1/pca.html",
    "title": "5  Principal Component Analysis (PCA)",
    "section": "",
    "text": "5.1 PCA in brief\nPrincipal Component Analysis is a method for understanding multivariate data. By multivariate data we mean a set of samples/observations (\\(n\\)) which are characterized on a number of different features (\\(p\\)). For example:\nThe data is often arranged in a data table (referred to as \\(\\mathbf{X}\\)) with \\(n\\) rows (samples) and \\(p\\) columns (variables). For almost all real life applications such multivariate data are correlated. That is; some of the variables carry the same type of information, and the interesting information in these data is captured by this so-called correlation structure. A nice visualization of the correlation is done via a scatter plot of two variables. For a few variables (say \\(p = 5\\)) it is possible to interpret all combinations of two variables. If \\(p = 5\\) that amounts to \\(\\frac{p(p-1)}{2} = 10\\) plots. However, when \\(p\\) is high this becomes in-practical. PCA deals with this issue by compressing the data into a set of components:\n\\[\n\\mathbf{X} = \\mathbf{t}_1 \\mathbf{p}_1^T + \\mathbf{t}_2 \\mathbf{p}_2^T + \\cdots + \\mathbf{t}_k \\mathbf{p}_k^T + \\mathbf{E}\n\\]\nNotation wise, \\(\\mathbf{X} \\sim (n, p)\\) is a matrix, \\(\\mathbf{t_i} \\sim (n,1)\\) and \\(\\mathbf{p_i} \\sim (p,1)\\) are vectors.\nThe power of PCA is when the mathematical decomposition into scores and loadings is combined with visualization of these. That is:",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Principal Component Analysis (PCA)</span>"
    ]
  },
  {
    "objectID": "chapters/week1/pca.html#pca-in-brief",
    "href": "chapters/week1/pca.html#pca-in-brief",
    "title": "5  Principal Component Analysis (PCA)",
    "section": "",
    "text": "Different beers (\\(n = 20\\)) analyzed for \\(p = 60\\) chemical variables reflecting the aroma composition.\nSix coffee samples (\\(n = 6\\)) assessed on a set of sensorical descriptors (\\(p = 12\\)) by sensorical panel of eight judges (\\(n = 48\\)).\nA range of oil samples (\\(n = 40\\)) analyzed by near infrared spectroscopy (\\(p = 400\\)).\n\n\n\n\n\n\\(\\mathbf{t}_1\\) are scores for component 1 (\\(\\mathbf{t}_2\\) are scores for component 2 and so forth) and tells something about the multivariate sample distribution.\n\\(\\mathbf{p}_1\\) are loadings for component 1 (\\(\\mathbf{p}_2\\) are loadings for component 2 and so forth) and tells something about the multivariate correlation structure between the variables.\nA set of \\(\\mathbf{t}_1\\) and \\(\\mathbf{p}_1\\) is referred to as a component.\n\n\n\nScore plot - scatter plots of combination of scores (often \\(\\mathbf{t}_1\\) vs. \\(\\mathbf{t}_2\\)).\nLoading plot - scatter plots of combination of loadings (often \\(\\mathbf{p}_1\\) vs. \\(\\mathbf{p}_2\\)).\nBi plot - overlayed score- and loading plot.\n\n\n\n\n\n\n\nExample 5.1 - Natural phenolic antioxidants for meat preservation - PCA\n\n\n\n\n\n\nThis data originates from a study investigating the effect of natural phenolic antioxidants against lipid and protein oxidation during sausage production and storage. Bologna-type sausages were prepared and treated with either green tea (GT) or rosemary extract (RE) as antioxidants, and a control batch was also included. The three types of sausages were evaluated by a sensory panel including 8 assessors, on 18 different descriptors within the categories smell, color, taste and texture. The sausages were evaluated immediately after production (week0) and after four weeks of storage (week4).\nData is from: Jongberg, Sisse, et al. ”Effect of green tea or rosemary extract on protein oxidation in Bologna type sausages prepared from oxidatively stressed pork.” Meat Science 93.3 (2013): 538-546.\nStep 1 - Load libraries\nNote that these need to be installed beforehand.\n\nlibrary(ggplot2)\nlibrary(ggbiplot)\n\nStep 2 - Load the data\nRemember to set your working directory.\n\nload(\"meat_data.RData\")\n\nStep 3 - Calculate the PCA model\nThe data frame X consists of \\(p = 20\\) columns, however only the last 18 are response variables, whereas the first two refers to the study design. In PCA only the response variables are used to calculate the model, whereas the design is used for e.g. coloring of the score plot.\n\nPCA &lt;- prcomp(X[,3:20], center = TRUE, scale. = TRUE)\n\nStep 4 - Plot the model\nThe ggbiplot() function nicely plots a so-called biplot. Below a lot of features are added to the plot, but ggbiplot(PCA) will produce something which is similar.\n\nggbiplot(PCA, groups = X$Treatment, \n         obs.scale = 1, var.scale = 1, \n         ellipse = T, ellipse.fill = F) + # Create the biplot\n  ggtitle(\"PCA on sensory data, colored according to treatment\") + # Add title\n  ylim(-4, 6) + # Set y-axis limits\n  xlim(-7, 6) + # Set x-axis limts\n  theme_bw() + # Set theme as \"black and white\"\n  theme(plot.title = element_text(hjust = 0.5, face = \"bold\")) # Adjust title \n\n\n\n\n\n\n\nFigure 5.1: A biplot on PC1 and PC2 based on the meat preservation sensory data. Colored according to treatment. GT = Green tea; RE = rosemary extract.\n\n\n\n\n\nIn figure 5.1 above you see a biplot of the PCA model on the sensory data on the sausages from both weeks. The points represent the scores for all samples, colored according to antioxidant treatment.The ellipse = T in the R-code draws ellipses representing the distribution of each treatment. The arrows indicate the loadings of the sensory variables. The scores can be used to evaluate the differences between samples, and the underlying reason behind the differences is interpreted through the directionality and magnitude of the loadings. It is evident that there is a difference between the control and treated samples, since they are grouped differently, primarily in the PC1 direction. When compared to the loadings it seems that the control samples are characterised by more old, and rancid tastes and smells, which are sensorical attributes of lipid oxidation. Furhter the control samples have a boiled egg texture compared to the treated samples. On the other hand the treated samples are more spicy and bitter/acidic in taste and grey in color.\nThere are not registered any greater difference between the two green tea and rosemary extract samples.\n\n\n\n\n\n\n\n\n\n\n\nExample 5.2 - Near infrared spectroscopy of marzipan - PCA\n\n\n\n\n\n\nThe following example illustrates how principal component analysis (PCA) can be used to explore your data. The dataset in this example consists of 32 measurements on marzipan bread (marzipanbrød) made from 9 different recipes. The measurements have been acquired using near infrared spectroscopy (NIR) where light is passed through a sample and the transmitted light analysed. The output measurement is a spectrum showing how much light the sample has absorbed at each wavelenght.\nStep 1 - Load packages and data\nWe start by loading the relevant libraries and importing the data.\n\nlibrary(ggplot2)\nload(\"Marzipan.RData\")\n\nStep 2 - Inspect raw spectra\nWe can now plot the spectra colored according to sugar content (figure 5.2)\n\nggplot(Xm, aes(wavelength, value, color = sugar)) +\n  geom_line() +\n  scale_colour_gradient(low = \"green\", high = \"red\") +\n  labs(title = \"Marzipan NIR spectrum\",\n       y = \"Absorbance\",\n       x = \"Wavelength (nm)\",\n       color = \"Sugar content (%)\") +\n  theme_bw()\n\n\n\n\n\n\n\nFigure 5.2: 32 NIR spectra measured on 9 different marcipan breads colored according to sugar content.\n\n\n\n\n\nLooking at the raw spectral data we see that there is a concentration gradient in the spectra when we colour according to the sugar content. It seems that the main variation in the spectra has something to due with the sugar content.\nWe can also plot the same spectra colored according to recipe (figure 5.3).\n\n\nCode\n# Extract recipe from variable name\nXm$recipe &lt;- substr(Xm$variable, 1, 1)\n\nggplot(Xm, aes(wavelength, value, color = recipe)) +\n  geom_line() +\n  labs(title = \"Marzipan NIR spectrum\",\n       y = \"Absorbance\",\n       x = \"Wavelength (nm)\",\n       color = \"Recipe\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nFigure 5.3: 32 NIR spectra measured on 9 different marcipan breads colored according to recipe.\n\n\n\n\n\nHere we see that we can distinguish some of the recipes from each other. This can be explained by the varying sugar content in the recipes. Also, if we look in the region below 1100 nm and into the visible (≈ 370 − 750 nm) we note that samples made with recipe c is different compared to the other samples.\nStep 3 - Calculate PCA model\nWe now make a PCA on the data and plot PC1 vs PC2 coloured according to sugar content.\n\n# Transposing the data and removing the wavelength column\nXt = t(X[,-1])\n\n# Making PCA on mean centered Xt\nmarzipan = prcomp(Xt, center = TRUE, scale = FALSE)\n\n# Extracting scores for plotting\nscores = data.frame(marzipan$x, sugar = Y$sugar)\n\n# Extracting % explained variance for plotting\nvarPC1 = round(summary(marzipan)$importance[2,1]*100)\nvarPC2 = round(summary(marzipan)$importance[2,2]*100)\n\nStep 4 - Plot the scores\nWe can now plot the scores colored according to sugar content (figure 5.4).\n\nggplot(scores, aes(PC1, PC2, color = sugar)) +\n  geom_point(size = 3) +\n  scale_color_gradient(low = \"green\", high = \"red\") +\n  labs(x = paste(\"PC1 - \",varPC1, \"%\", sep = \"\"), # Insert exp.var. as label\n       y = paste(\"PC1 - \",varPC2, \"%\", sep = \"\"),\n       color = \"Sugar content (%)\") +\n  theme_bw()\n\n\n\n\n\n\n\nFigure 5.4: Score plot for NIR spectra of marzipan bread colored by sugar content.\n\n\n\n\n\nWe see that PC1 explains 61% of the variation in the data and that it seems to capture the variation in the sugar content. The samples are ordered from left to right in increasing concentration. Also, a group of samples are laying away from the rest when looking at PC2 which is explaining 33% of the variation in the data.\nWe can also color the scores according to recipe (figure 5.5).\n\n# Extract recipe from sample name\nscores$recipe &lt;- substr(Y$sample, 1, 1)\n\nggplot(scores, aes(PC1, PC2, color = recipe)) +\n  geom_point(size = 3) +\n  labs(x = paste(\"PC1 - \",varPC1, \"%\", sep = \"\"), # Insert exp.var. as label\n       y = paste(\"PC1 - \",varPC2, \"%\", sep = \"\"),\n       color = \"Recipe\") +\n  theme_bw()\n\n\n\n\n\n\n\nFigure 5.5: Score plot for PCA based on the NIR spectra of marzipan bread colored by recipe.\n\n\n\n\n\nIf we look at PC2 we see that it is the samples from recipe c that is laying away from the other samples. What is the reason for that? Let us look at the loadings. We start by looking at the second loading (figure 5.6) as it is dividing the samples from recipe c from the other samples.\n\n# Extract loadings from PCA model\nloadings = as.data.frame(marzipan$rotation)\n\nggplot(loadings, aes(wl, PC2)) +\n  geom_line(linewidth = 1) +\n  labs(x = \"Wavelength (nm)\",\n       y = \"2nd loading\"\n       ) +\n  theme_bw()\n\n\n\n\n\n\n\nFigure 5.6: Loading plot for PCA based on the NIR spectra of marzipan bread.\n\n\n\n\n\nThe main contribution to PC2 is the peak around 550 nm. So the reason why the samples from recipe c is different from the other is related to colour. This actually makes sense as this recipe has cocoa powder added to the recipe which will influence the colour of the marzipan bread.\nLastly, we look at the first loading (figure 5.7).\n\nggplot(loadings, aes(wl, PC1)) +\n  geom_line(linewidth = 1) +\n  labs(x = \"Wavelength (nm)\",\n       y = \"1st loading\"\n       ) +\n  theme_bw()\n\n\n\n\n\n\n\nFigure 5.7: Loading plot for PCA based on the NIR spectra of marzipan bread.\n\n\n\n\n\nIt is not straight forward to see which peaks are related to sugar. However, the peaks around 1200, 1400, 1875 and 2100 nm has the highest magnitude and therefore the main reason for the sugar content gradient we see in PC1. Actually all 4 peaks are related to either the C-H (Carbon - Hydrogen) or O-H groups in sugar or O-H in water. You will learn more about assigning peaks to chemical information in other courses later on.",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Principal Component Analysis (PCA)</span>"
    ]
  },
  {
    "objectID": "chapters/week1/pca.html#reading-material",
    "href": "chapters/week1/pca.html#reading-material",
    "title": "5  Principal Component Analysis (PCA)",
    "section": "5.2 Reading material",
    "text": "5.2 Reading material\n\nVideos on PCA\n\nPCA main ideas by StatQuest\nPCA Introduction 1 by Rasmus Bro\nPCA Introduction 2 by Rasmus Bro\n\nChapter 2 in Biological Data analysis and Chemometrics by Brockhoff\nChapter 4 (4.1 to 4.5) in Chemometrics With R: Multivariate Data Analysis in the Natural Sciences and Life Sciences by Ron Wehrens (2012). Springer, Heidelberg. Available in Absalon.",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Principal Component Analysis (PCA)</span>"
    ]
  },
  {
    "objectID": "chapters/week1/pca.html#videos-on-pca",
    "href": "chapters/week1/pca.html#videos-on-pca",
    "title": "5  Principal Component Analysis (PCA)",
    "section": "5.3 Videos on PCA",
    "text": "5.3 Videos on PCA\n\n5.3.1 PCA main ideas - StatQuest\n\n\n\n5.3.2 PCA Introduction 1 - Rasmus Bro\n\n\n\n5.3.3 PCA Introduction 2 - Rasmus Bro",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Principal Component Analysis (PCA)</span>"
    ]
  },
  {
    "objectID": "chapters/week1/pca.html#exercises",
    "href": "chapters/week1/pca.html#exercises",
    "title": "5  Principal Component Analysis (PCA)",
    "section": "5.4 Exercises",
    "text": "5.4 Exercises\n\n\n\n\n\n\nExercise 5.1 - McDonalds Data\n\n\n\n\n\n\nThe purpose of this exercise is to get familiar with PCA on a small intuitive dataset. The data - McDonaldsScaled.xlsx - constitutes of different fast food products and their nutritional content.\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nRead in the data using an appropriate package / function (e.g. r read_excel() from the package readxl), and set up the data with row names etc.\n\n\n\n\n\n\nMcD &lt;- read_xlsx(\"McDonaldsScaled.xlsx\")\n\n# The first column has no name - change that\ncolnames(McD)[1] &lt;- \"Item\"\n\nhead(McD)\n\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nMake some initial descriptive plots for the five response variables, that indicate the distribution (center and spread).\nMake some bi-variate scatter plots examining the relation between different variables, and comment on whether this relation is obvious and further, which types of samples are responsible for the relation. You can use the ggplot2 with stat_smooth() for this. Set method = \"lm\" to choose a linear model and se = F to disable the confidence intervals around the line.\n\n\n\n\n\n\nggplot(McD, aes(Energy, Protein)) +\n  stat_smooth(method = \"lm\", se = F) + # Plot a linear model\n  geom_point(size = 3) +\n  geom_text(aes(label = Item)) +\n  theme_bw()\n\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nNow, make a PCA on the data. What does the two options center = ... and scale. = ... refer to?\n\n\n\n\n\n\nMcD_pca &lt;- prcomp(McD[2:6], center = TRUE, scale. = TRUE)\n\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nIf \\(X_1, X_2, .., X_{19}\\) is the protein variable in the dataset (i.e. \\(X_1\\) is the protein content of sample 1 and so forth), how do you calculate the centered and scaled representation of data, which could be called: \\(X_1^{auto}, X_2^{auto}, .., X_{19}^{auto}\\)?\nPlot the PCA results and comment on them. There are several ways of doing this. The first described here, is to zack out the parameters (scores and loadings) and then use the ggplot2 functionality to plot those. Try to comprehend what is actually produced from the list of functions listed below.\n\n\n\n\n\n\n# Zack out the individual parameters (scores and loadings and item names)\nscores &lt;- data.frame(McD_pca$x)\nloadings &lt;- data.frame(McD_pca$rotation)\nItems &lt;- McD$Item\n\n# Score plot\nggplot(scores, aes(PC1, PC2, label = Items)) +\n  geom_text()\n\n# Loading plot\nggplot(loadings, aes(PC1, PC2, label = row.names(loadings))) +\n  geom_text()\n\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nAlternatively you can utilize a package ggbiplot for making nice plots from PCA objects (see the first PCA example for more info on how to use this package).\n\n\n\n\n\n\nggbiplot(McD_pca, obs.scale = 1, var.scale = 1) +\n  geom_text(label = Items) +\n  theme_bw()\n\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nMake a vector that indicates the different fast food types (Burger, Drinks, etc.). You can either do this by extending the excel file with a column, or do it in R (see this below). Infer this class information on the plot as color (or maker shape or size).\n\n\n\n\n\n\nCategories &lt;- c(\n  rep(\"Burger\", 9),\n  rep(\"Drink\", 3),\n  rep(\"Icecream\", 3),\n  rep(\"Other\", 2),\n  rep(\"Salad\", 2)\n)\n\nggbiplot(McD_pca, obs.scale = 1, var.scale = 1) +\n  geom_text(aes(label = Items, color = Categories)) +\n  theme_bw()\n\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nTry to modify the PCA by removing scaling and/or centering. What happens to the plots of the results? What do you think is going on?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 5.2 - Analysis of coffee serving temperatura - PCA\n\n\n\n\n\n\nIn the dataset Results Panel.xlsx the sensorical results from a panel of eight judges, evaluating coffee served at six different temperatures each four times are listed. In this exercise, we are going to first average over judge and temperature followed by PCA to evaluate sensorical descriptor similarity as well as the effect of serving temperature on the perception of coffee.\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nAfter import of data, and initial sanity check, calculate the average response across the four replicates. Use the summarise() or aggreggate() function with mean functions to make a dataset with the average response for the six different temperatures for each judge. For an example on how to do this see Section A.4.\n\nHint: The number of samples should be reduced by a factor of 4.\n\nUse this data as input for construction of a PCA model. Which variables do you think should be included?\nMake a biplot of this PCA model and interpret it.\n\nWhich descriptors go together and which are oppositely correlated?\nAre there, from this analysis, a clear difference between the different serving temperatures?\nWhat do you think blurs the picture?\n\n\n\n\n\n\nThe code below can be used for inspiration. Be aware, that we need to set a series of dependencies in order for this to work.\n\nUsing summarise()Using aggregate()\n\n\n\n# Calculate mean per group\ncoffee_ag &lt;- coffee |&gt;\n  summarise(\n    across(where(is.numeric), mean), # Compute mean over numeric variables \n    .by = c(Assessor, Sample) # Group by judge and temp\n  )\n\n# Compute PCA model\ncoffee_pca &lt;- prcomp(coffee_ag[your_variables_here])\n\n# Plot PCA model\nggbiplot(coffee_pca, groups = coffee_ag$Temp,\n         ellipse = T, circle = T, ellipse.fill = F) +\n  scale_color_brewer(\n    palette = \"RdBu\", \n    direction = -1,\n    name = \"Temperature\"\n    ) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n# Define variables to group by\ngroup_by &lt;- list(\n  \"Judge\" = coffee$Assessor, \n  \"Temp\" = coffee$Sample\n  )\n\n# Calculate mean per group\ncoffee_ag &lt;- aggregate(coffee, group_by, mean)\n\n\n# Compute PCA model\ncoffee_pca &lt;- prcomp(coffee_ag[your_variables_here])\n\n# Plot PCA model\nggbiplot(coffee_pca, groups = coffee_ag$Temp,\n         ellipse = T, circle = T, ellipse.fill = F) +\n  scale_color_brewer(\n    palette = \"RdBu\", \n    direction = -1,\n    name = \"Temperature\"\n    ) +\n  theme(legend.position = \"bottom\")\n\n\n\n\nCheck out the ggbiplot() syntax (by ?ggbiplot). by adding stuff to the plot, it is modified to look exactly like you want it. Here we change legend appearance (for inferring temperature) and color scheme for the scores matching temperature. In order to also remove variation due to differences between judges, the dataset is compressed such that the rows reflect the average for each temperature (across judges and replicates). Then this dataset is used for constructing a PCA model and visualized.\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nuse the aggreggate() function to average across judges and replicates.\n\nHint: Modify the .by = argument in summarise() or the group_by-list in aggregate().\n\nMake a PCA model on this dataset. Visualise and interpret it.\nUse the code below to vizualize the model again - Can you figure out why this model on a data-matrix of 6 samples is different from the previous model (on a data-matrix of 48 samples)?\n\n\n\n\n\n\n# Plot PCA model\nggbiplot(coffee_pca, groups = coffee_ag$Temp,\n         ellipse = T, circle = T, ellipse.fill = F) +\n  scale_color_brewer(\n    palette = \"RdBu\", \n    direction = -1,\n    name = \"Temperature\"\n    ) +\n  theme(legend.position = \"bottom\")\n\nThe plots are hard to interpret because\n\nsome of the labels are masked and\nthe points are way to small.\n\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nUse the function xlim(c(low,high)) and geom_point(, aes(color = cofffee_ag$Temp) size = 5) and add them to the plot to fix these problems.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 5.3 - Wine aromas\n\n\n\n\n\n\nThis exercise will take you through plotting, descriptive stats and PCA. Wine based on the same grape variety (Cabernet Sauvignon) from four different countries (Argentina, Australia, Chile and South Africa) were analyzed for aroma compound composition with GC-MS (gas chromatography coupled with mass spectrometry). The dataset can be found in the file “Wine.xlsx”, and it will form the basis for working with basic descriptive statistics, plots and PCA.\n\nDescriptive statistics\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nStart by importing the dataset “Wine.xlsx” to R and try to get an overview of it.\n\nHint: use the summary() function in R and/or have a look at the raw data in the Excel file.\n\n\nHow many wines were analyzed from each country?\nHow many variables are there in the dataset, and how many constitutes the aroma profile?\n\n\n\n\n\n\nFor the descriptive statistics, only two of the aroma compounds are selected. Choose two on your own or make the calculations for the aroma compounds benzaldehyde (almond like aroma) and 3-Methylbutyl acetate (sweet fruit/banana like aroma).\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nCalculate mean, variance, standard deviation, median and inner quartile range for the selected aroma compounds from each of the four different countries.\n\nHint: it can be helpful to create a separate dataset for each country, which can be done with the function subset().\n\n\n\n\n\n\n\n\nPlots\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nMake a boxplot, a jitterplot and a combination of the two with all 4 countries in one plot. Use the R-commands from the notes as inspiration.\nWhat do you see? Discuss pros and cons of the different plots.\nAdjust the layout of your favorite plots (e.g. color, background, title etc.). Think about how the data is presented in the best way. Actually, it can be rather beneficial to specify a generic theme including title and label font size, background color of the plot etc, which then can be added to each plot produced.\n\n\n\n\n\n\n\nPCA\nWorking with a dataset with many variables, PCA provides a very nice tool to give an overview of the dataset. First we define the data we want to include in the analysis. With logical indexing wine[4:20] we choose the variables to include (i.e. we are only interested in analyzing the aroma compounds).\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nUse the `prcomp function to calculate a PCA model on scaled aroma data. What does the arguments scale. = T and center = T do the to the data?\n\nTry changing them to = F (false).\n\nMake a score plot and a loading plot.\n\n\n\n\n\n\n# Compute PCA model\nwine_pca &lt;- prcomp(wine[4:58], scale. = T, center = T)\n\n# Create score plot (biplot without arrows)\nggbiplot(wine_pca, groups = wine$Country, point.size = 3,\n         var.axes = F)\n\n# Extract loadings and loading names\nwine_loadings &lt;- data.frame(wine_pca$rotation)\nloading_names &lt;- rownames(wine_loadings)\n\n# Create \"manual\" loading plot\nggplot(wine_loadings, aes(PC1, PC2)) +\n  geom_text(aes(label = loading_names,\n                color = loading_names),\n            show.legend = F) +\n  theme_bw()\n\n# Create biplot\nggbiplot(wine_pca, groups = factor(wine$class), point.size = 3)\n\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nWhat do you see in the score and the loadings plot?\nCan you see a grouping of the data? If so, how are the groups different?",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Principal Component Analysis (PCA)</span>"
    ]
  },
  {
    "objectID": "chapters/week2/week_2.html",
    "href": "chapters/week2/week_2.html",
    "title": "Week 2",
    "section": "",
    "text": "Hand-in assigment\nThis week is going to focus on three very central subjects, namely correlation, the normal distribution and in relation to this the Central Limit Theorem.\nExercise 6.1 Correlation between aroma compounds questions 1 – 6 (not the PCA part) is to be handed in (through Absalon or as hard-copy Wednesday night).",
    "crumbs": [
      "Week 2"
    ]
  },
  {
    "objectID": "chapters/week2/week_2.html#exercises",
    "href": "chapters/week2/week_2.html#exercises",
    "title": "Week 2",
    "section": "Exercises",
    "text": "Exercises\nFor Monday work through the following exercises:\n\nExercise 6.2\nExercise 6.3\n\nFor Wednesday work through the following exercises:\n\nExercise 6.4\nExercise 7.1\nExercise 7.3",
    "crumbs": [
      "Week 2"
    ]
  },
  {
    "objectID": "chapters/week2/correlation.html",
    "href": "chapters/week2/correlation.html",
    "title": "6  Correlation",
    "section": "",
    "text": "6.1 Correlation and covariance - in short\nA covariance or correlation is a scalar measure for the association between two (response-) variables. Covariance bewteen two variables \\(X = (x_1, x_2, ..., x_n)\\) and \\(Y = (y_1, y_2, ..., y_n)\\) is defined as:\n\\[\n\\text{cov}_{XY} = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})}{n - 1}\n\\]\nCovariance depends on the scale of data (\\(X\\) and \\(Y\\)), and as such is hard to interpret. The correlation is however a scale in-variant version\n\\[\n\\text{corr}_{XY} = \\frac{\\text{cov}_{XY}}{s_X \\cdot s_Y},\n\\]\nwhere \\(s_X\\) and \\(s_Y\\) are the standard deviation for \\(X\\) and \\(Y\\) respectively (see 2.4.1 for details).\nDividing the covariance by the individual standard deviations put the correlation coefficient in the range between −1 and 1\n\\[\n-1 \\leq \\text{corr}_{XY} \\leq 1.\n\\]\nA correlation (and covariance) close to zero indicates that there are no association between the two variables (see figure 6.1).\nCode\nset.seed(4321)\n\nx &lt;- 1:30\nn &lt;- length(x)\n\ndata.frame(\n  \"x\" = x,\n  \"y1\" = x,\n  \"y2\" = x + rnorm(n, sd = 6),\n  \"y3\" = x + rnorm(n, sd = 10),\n  \"y4\" = rep(11, n) + rnorm(n, sd = 10),\n  \"y5\" = -x + rnorm(n, sd = 3),\n  \"y6\" = -x + rnorm(n, sd = 20)\n) |&gt; \n  pivot_longer(cols = !x) |&gt; \n  ggplot(aes(x, value)) +\n  geom_smooth(method = \"lm\", se = F, \n              linewidth = .6, color = \"black\") +\n  geom_point(size = 2, color = \"steelblue\", alpha = .5) +\n  stat_cor(\n    aes(label = after_stat(r.label)),\n    geom = \"label\",\n  ) +\n  labs(y = \"y\") +\n  facet_wrap(~ name, scales = \"free_y\") +\n  theme_bw() +\n  theme(\n    strip.background = element_blank(),\n    strip.text = element_blank()\n  )\n\n\n\n\n\n\n\n\nFigure 6.1: Correlation \\(R\\) between one variable x and different y variables.",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "chapters/week2/correlation.html#correlation-and-covariance---in-short",
    "href": "chapters/week2/correlation.html#correlation-and-covariance---in-short",
    "title": "6  Correlation",
    "section": "",
    "text": "Example 6.1 - Natural phenolic antioxidants for meat preservation - Correlation\n\n\n\n\n\n\nThis example continues where we left of in example ?sec-example-meat-pca with the sensory data on the meat sausages treated with green tea (GT) and rosemary extract (RE) or control.\nLoad the data and libraries\nRemember to set your working directory.\n\nlibrary(ggplot2)\nlibrary(gridExtra) # For showing multiple plots in a grid\nload(\"meat_data.RData\")\n\n\n\nCode\npca &lt;- prcomp(X[,3:20], center = TRUE, scale. = TRUE)\n\n# Define loadings to keep\nshow_loadings &lt;- c(\"Color_Pink\", \"Smell_Old\", \"Taste_Old\", \n                  \"Taste_Bitter\", \"Color_Grey\")\n\n# Remove loadings\npca$rotation[!rownames(pca$rotation) %in% show_loadings, ] &lt;- 0\n\nggbiplot(pca,\n         select = rownames(pca$rotation) %in% show_loadings,\n         alpha = 0, \n         varname.size = 4) +\n  labs(title = \"Loading plot\") +\n  theme_bw() +\n    theme(plot.title = element_text(hjust = .5, face = \"bold\"))\n\n\n\n\n\n\n\n\nFigure 6.2: Select loadings from the PCA model computed in the previous example.\n\n\n\n\n\nIn figure 6.2 the loading plot corresponding to some of the loadings found in the PCA model calculated in the previous example is shown. The selected variables show positive, negative and non-correlated loadings. In a PCA loading plot, loadings with opposite directions are negatively correlated, while loadings pointing in the same direction are possitively correlated. Loadings which are orthogonal (90 degree angle) with respect to each other, are not correlated. For the plot above, this means that:\n\nTaste_Old and Smell_Old are positively correlated.\nTaste_Bitter and Color_Pink are uncorrelated.\nColor_Pink and Color_Grey are negatively correlated.\n\nTo check if the correlation holds for the raw data, scatter plots are made for each of the encircled three examples. (Note that the interpretation of these correlations are only valid with respect to the variation described in the model)\nCreate scatter plots\n\n# Define a common theme\nmy_theme &lt;- theme_bw() +\n  theme(plot.title = element_text(hjust = .5, face = \"bold\"))\n\n# Plot 1\nplt1 &lt;- ggplot(X, aes(Smell_Old, Taste_Old)) +\n  geom_point(size = 2) +\n  stat_smooth(method = \"lm\", se = F) +\n  ggtitle(\"Positive correlation\") +\n  my_theme\n\n# Plot 2\nplt2 &lt;- ggplot(X, aes(Taste_Bitter, Color_Pink)) +\n  geom_point(size = 2) +\n  stat_smooth(method = \"lm\", se = F) +\n  ggtitle(\"No correlation\") +\n  my_theme\n\n# Plot 3\nplt3 &lt;- ggplot(X, aes(Color_Grey, Color_Pink)) +\n  geom_point(size = 2) +\n  stat_smooth(method = \"lm\", se = F) +\n  ggtitle(\"Negative correlation\") +\n  my_theme\n\n# Arrange the plots in a 1 x 3 grid (requires gridExtra package)\ngrid.arrange(plt1, plt2, plt3, ncol = 3)\n\n\n\n\n\n\n\nFigure 6.3: The between three pairs of select variables from the meat data.\n\n\n\n\n\nThe correlation values are calculated using the cor() function.\n\n# Calculate correlation\ncor1 &lt;- cor(X$Smell_Old, X$Taste_Old)\ncor2 &lt;- cor(X$Taste_Bitter, X$Color_Pink)\ncor3 &lt;- cor(X$Color_Grey, X$Color_Pink)\n\n# Display in a fancy way\ncat(\"Correlation coefficients\",\n    \"\\n Smell_Old vs Taste_Old: \\t\\t\", cor1,\n    \"\\n Taste_Bitter vs Color_Pink: \\t\", cor2,\n    \"\\n Color_Grey vs Color_Pink: \\t\\t\", cor3)\n\nCorrelation coefficients \n Smell_Old vs Taste_Old:         0.595475 \n Taste_Bitter vs Color_Pink:     0.007446373 \n Color_Grey vs Color_Pink:       -0.604673\n\n\nConclusions\nHere, it can be seen that the pink and grey color attributes, which are oppositely directed in the PCA loadings, display a moderate negative correlation in the raw data. The pink color and bitter taste, which are orthogonal to each other in the loadings, are not correlated at all. The old smell and old taste with similar directionality in the PCA are moderately postively correlated. With regards to food chemistry, do you think these conclusions make sense?",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "chapters/week2/correlation.html#reading-material",
    "href": "chapters/week2/correlation.html#reading-material",
    "title": "6  Correlation",
    "section": "6.2 Reading material",
    "text": "6.2 Reading material\n\nVideos on correlation and covariance\n\nCovariance, clearly explained\nPearson’s correlation, clearly explained\nCorrelation Doesn’t Equal Causation: Crash Course Statistics #8\nWhat is correlation?\n\nChapter 1.4.3 and 2.5 of Introduction to Statistics by Brockhoff\nChapter 2 in Biological Data analysis and Chemometrics by Brockhoff",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "chapters/week2/correlation.html#videos-on-correlation-and-covariance",
    "href": "chapters/week2/correlation.html#videos-on-correlation-and-covariance",
    "title": "6  Correlation",
    "section": "6.3 Videos on correlation and covariance",
    "text": "6.3 Videos on correlation and covariance\n\n6.3.1 Covariance, clearly explained - StatQuest\n\n\n\n6.3.2 Pearson’s correlation, clearly explained - StatQuest\n\n\n\n6.3.3 Correlation doesn’t equal causation - CrashCourse\n\n\n\n6.3.4 What is correlation - Melanie Maggard",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "chapters/week2/correlation.html#exercises",
    "href": "chapters/week2/correlation.html#exercises",
    "title": "6  Correlation",
    "section": "6.4 Exercises",
    "text": "6.4 Exercises\n\n\n\n\n\n\nExercise 6.1 - Correlation between aroma compounds\n\n\n\n\n\n\n\nCorrelation\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nWhat does a correlation coefficient of -1, 0 or +1 tell you?\nIn the table below the amount of Nonanal and Ethyl.2.methyl.propanoate in the six Argentine wines are listed. Fill out the blank spaces in the table and calculate the correlation coefficient (r).\n\nHelp can be found in example 1.19 in Introduction to Statistics by Brockhoff.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWine number (\\(i\\))\n1\n2\n3\n4\n5\n6\n\n\n\n\nNonanal (\\(X_i\\))\n0.003\n0.003\n0.005\n0.006\n0.008\n0.005\n\n\nEthyl.2.methyl.propanoate (\\(Y_i\\))\n0.106\n0.165\n0.150\n0.155\n0.149\n0.141\n\n\n\\(X_i - \\bar{X}\\)\n\n\n\n\n\n\n\n\n\\(Y_i - \\bar{Y}\\)\n\n\n\n\n\n\n\n\n\\((X_i - \\bar{X})(Y_i - \\bar{Y})\\)\n\n\n\n\n\n\n\n\n\n\nRemember that the sample covariance and correlation are given as\n\\[\n\\text{cov}_{XY} = s_{XY} = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})}{n - 1},\n\\]\n\\[\n\\text{corr}_{XY} = r_{XY} = \\frac{\\text{cov}_{XY}}{s_X \\cdot s_Y}.\n\\]\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nMake a plot with Nonanal vs. Ethyl.2.methyl.propanoate and calculate the correlation coefficient in R (use the function cor() in R). Include only wines from Argentina.\nCalculate the correlation coefficient for a, b, c and d, where you include wine data from all the countries.\n\nDiethyl.succinate and Ethyl.lactate\nEthyl.acetate and Ethanol\n2.Butanol and Ethyl.hexanoate\nBenzaldehyde and Hexyl.acetate\n\nWhat happens with the covariance and correlation if we multiply the Diethyl.succinate amount with 10 or −4?\nWhat happens with the covariance and correlation if we add 10 or 100 to the Diethyl.succinate amount?\n\n\n\n\n\n\n\nPCA\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nMake a PCA including wines from all the countries (similar to the one from week 1 on the same dataset).\n\nWhich variables are responsible for the grouping of the countries?\n\nCompare the calculated correlation coefficients in question 4 with the loading plot of the PCA in question 5.\n\nHow does the position of the variables in the loading plot make the correlation coefficients negative, positive, close to \\(\\pm\\) 1 or 0?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 6.2 - Covariance and Correlation - by hand\n\n\n\n\n\n\nCertain types of characteristics ”go together”, for instance will a variable that measures the chocolate smell of some food type be related to a variable measuring the chocolate taste of the same food type. We talk about these two types of information being correlated. Below is 48 corresponding measures of the sensorical attributes sour and roasted for coffees at different serving temperatures and different judges (each based on the average of four measurements). Calculate the correlation between these two variables based on the metrics listed below.\n\n\n\n\nObservation\nSour\nRoasted\n\n\n\n\n1\n1.70\n1.06\n\n\n2\n1.93\n1.10\n\n\n3\n3.59\n3.87\n\n\n4\n2.30\n1.86\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n45\n1.30\n1.89\n\n\n46\n0.26\n1.93\n\n\n47\n1.12\n1.05\n\n\n48\n3.84\n5.41\n\n\n\\(\\sum\\)\n91.59\n76.42\n\n\n\\(\\hat{\\sigma}^2\\)\n0.90\n0.94\n\n\n\\(\\sum_{XY}\\)\n\n172.73\n\n\n\n\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nCalculate mean and standard deviation for the two variables using the statistics listed below data. That is WITHOUT importing data into the computer!\nThe covariance between the variables is 0.57. Calculate the correlation between the two variables.\nWhat happens with the covariance and correlation if we multiply the Sour ratings with 2 or -3?\nWhat happens with the covariance and correlation if we add 10 or -1234 to the Roasted ratings?\nAs an upcoming coffee expert, why do you think these two attributes are correlated?\nHARD! Calculate \\(\\sum (X - \\bar{X})(Y - \\bar{Y})\\) from \\(\\sum XY\\), \\(\\sum X\\) and \\(\\sum Y\\) (where X is Sour and Y is Roasted).\n\nHint: You need to multiply out the product of the two parenthesis and reduce the resulting part using the relation between \\(\\bar{X} = \\sum(X) / n\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 6.3 - Correlation and PCA\n\n\n\n\n\n\nIn this exercise the sensorical data from ranking of coffee served at different temperatures are used. The aim is to see how correlations is the vehicle for PCA analysis. The data is named “Results Panel.xlsx”.\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nImport the data and remove the replicate effect by averaging over these.\nMake a scatter plot of the attribute Sour versus Roasted. Comment on what you see in terms of relation between these two variables.\nNow make a comprehensive figure where all the sensorical attributes (there are 8) are plotted against each other.\nCalculate all the pairvise correlations between the variables. How does this correspond with the figure?\nMake a PCA model on this dataset (same as in (ex-analysis-of-coffee-inspection?)).\nComment on the (dis-)similarity between the correlation matrix, the multiple pairwise scatterplot and the PCA model.\n\n\n\n\n\nBelow is some code which might be useful for this purpose (You need to install or add dependencies via install.packages(GGally) or library(GGally) in order for the functions to be recognized by R). In the pairwise scatterplot a straight line is added by + geom_abline(), try also to add a smooth curve by + geom_smooth(). What are the difference between those two representation of similarity between the variables?\n\nggplot(coffee_ag, aes(Roasted, Sour)) +\n  geom_abline(color = \"red\", linewidth = 1) +\n  geom_point(size = 2.5, color = \"steelblue\", alpha = .75) +\n  theme_bw()\n\nggpairs(coffee_ag, columns = 4:11)\n\n\n\n\n\n\n\n\n\n\n\nExercise 6.4 - Olive oil adulteration\n\n\n\n\n\n\nQuick detection of adulteration of oils is of growing importance, since high quality oils, such as olive oil, are becoming increasingly popular and expensive, increasing the incentive for adulterating with cheaper oils. Spectroscopic techniques are the preferred measurement choice because they are quick, often non-destructive and in many cases highly selective for oil characterization.\nThe purpose of this exercise is to introduce you to how multivariate techniques, such as PCA, can be applied on spectral datasets. They are in fact, very useful on datasets such as these, because of the very high number of variables that can be included in the modelling.\nThe samples in this dataset are mixtures of olive oil and thistle oil. Olive oil and thistle oil are almost exclusively made up of triglycerides, consisting of a glycerol backbone with three fatty acid chains attached. An example of a triglyceride is shown in the top right of figure 6.4. Fatty acids are characterized by the amount of unsaturation. Olive oil consists mostly of monounsaturated fatty acids, while thistle oil is largely comprised of polyunsaturated fatty acids (ie: they have more double bonds). Additionally a few of the samples were spiked with a free trans fatty acid. The structure of the added trans fatty acid is shown in the top left of figure 6.4.\nThe samples were measured with infrared (IR) spectroscopy. Some of the relevant peaks for oil characterization are shown in the raw spectra in figure 6.4. The dataset consists of 30 oil samples and 1794 variables. The first 1790 variables are the absorbance at 1790 wavelengths. The last 4 columns in the dataset describe the concentration of olive oil, thistle oil and transfat, and lastly the sample ID.\nSince it is a bit more difficult to work with spectral data in R, we have decided to be merciful and give you the data directly as an .RData file, which you simply get into R by load(\"OliveOilAdult.RData\"). Be sure to be in the correct working directory, or add the path to the load command.\n\n\n\n\n\n\nFigure 6.4: Top right) Triglyceride. Left part: Glycerol backbone. Right part from top to bottom: Unsaturated, monounsaturated and polyunsaturated fatty acid. Top left) Trans fatty acid. Bottom) Raw IR spectrum of mixed oil.\n\n\n\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nLoad the “oliveoil” dataset in R. To be able to plot the raw spectral data, run the following R code. Use the dim() command to understand the difference between input and output to melt() on how the data is organised in the two formats.\n\n\n\n\n\n\nlibrary(reshape2)\n\n# Melt before plotting\nfor_raw &lt;- melt(Oliveoil, \n                id.vars = c(\"sample_id\", \"oliveoil\", \"thistleoil\", \"transfat\"))\n\n# Define correct x-axis with wavelength vector\nfor_raw$wl &lt;- sort(rep(wl, nrow(Oliveoil)))\n\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nNow make a plot of the raw data. Use the R-code below as a base, and customize it to your liking. Use the handy plotly package to plot an interactive plot that we can zoom in.\n\n\n\n\n\n\nlibrary(ggplot2)\nlibrary(plotly)\n\nraw &lt;- ggplot(for_raw, aes(wl, value, group = sample_id)) +\n  geom_line()\n\nggplotly(raw)\n\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nInspect the raw data plot.\n\nHow does the raw data look? Are there any outliers? in ggplotly you can identify specific samples by hovering the curser over them.\nTry to color according to the different types of oil, one at a time. Then, zoom in on some of the peaks highlighted in figure 6.4. Can you observe a correlation between these peaks and the oil concentration?\n\nNow that you have a feel for the raw data, try to estimate the scores and loadings for a PCA model. Why do we only center, and not autoscale the spectral data?\n\n\n\n\n\n\n# Define PCA\noliveoil_pca &lt;- prcomp(Oliveoil[1:1790], center = T, scale = F)\n\nscores &lt;- data.frame(oliveoil_pca$x)\nloads &lt;- data.frame(oliveoil_pca$rotation)\n\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nUsing ggplot() and ggplotly(), make a plot of the PC1 vs PC2 scores, and color according to oliveoil content.\nMake two separate loading plots with first PC1 loads vs the wavelength (wl) vector and then PC2 loads vs. the wavelength. It is very important that the loadings are plotted separately, and not against each other when working with spectral data. Can you see why?\nInspect the scoreplot. Are there any outlying samples? If so, identify them. Did you notice this outlier in the raw spectra plot? Remove the outlier by using the code below and inserting the correct sample id number.\n\n\n\n\n\n\n# Remove outlier\n\nOliveoil &lt;- Oliveoil[-c(\" insert outlier ID number here\"), ]\n\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nNow rerun your whole script and see how the removal of the outlier has changed the score and loading plots.\nUsing the score plot, try to colour them by different oil types and elucidate what information about the samples is being described by the first and second PCA component, respectively.\nInspect the loading plots and compare them to the information in figure 6.4. Can you re-find some of the important peaks in the loadings? And how does this relate to the findings in the scoreplot?.\n\nHint: Remember that the dataset is mean centred, so the peaks will be positive or negative. Negative scores have a high absorbance in the negative loading ”peaks”, and positive scores have a high absorbance in the positive loading ”peaks”.",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "chapters/week2/normal_distribution.html",
    "href": "chapters/week2/normal_distribution.html",
    "title": "7  The normal distribution",
    "section": "",
    "text": "7.1 Estimation of \\(\\mu\\) and \\(\\sigma\\)\nIf \\(X\\) is normaly distributed with mean \\(\\mu\\) and variance \\(\\sigma^2\\), then the probability density function for a given draw from that distribution \\(x\\) is given by\n\\[\nP(x) = \\frac{1}{{\\sigma \\sqrt {2\\pi } }}e^{{{ - \\left( {x - \\mu } \\right)^2 } {2\\sigma ^2 }}}\n\\]\nFurther, the cumulative density function can be found by integration of this formula \\[\nP(X \\leq x) = \\frac{1}{{\\sigma \\sqrt {2\\pi } }} \\int_{-\\inf}^x e^{{{ - \\left( {x - \\mu } \\right)^2 } {2\\sigma ^2 }}} \\, dx\n\\]\nThe short notation for this distribution is\n\\[\nX \\sim \\mathcal{N}(\\mu,\\sigma^2)\n\\]\nMore often than not, the distribution is unknown. That is the values of \\(\\mu\\) and \\(\\sigma\\) are unknown and must then be estimated from data.\n\\(\\mu\\) characterizes the center of the distribution, and is naturally estimated by the mean-value of the data-points (\\(\\bar{X}\\)). \\(\\sigma\\) reflects the spread around the mean, and is in a similar fashion estimated by the standard deviation (\\(\\hat{\\sigma}\\) or \\(s\\)).\n\\[\n\\hat{\\mu} = \\bar{X} = \\sum_{i = 1}^n X_i / n = (X_1 + X_2 + X_3 + \\ldots + X_n) / n\n\\]\n\\[\n\\hat{\\sigma}^2 = s^2 = \\sum_{i = 1}^n (X_i - \\bar{X})^2 / (n-1) = ((X_1 - \\bar{X})^2 + (X_2 - \\bar{X})^2 + (X_3 - \\bar{X})^2 + \\ldots + (X_n - \\bar{X})^2 ) / (n-1)\n\\]",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>The normal distribution</span>"
    ]
  },
  {
    "objectID": "chapters/week2/normal_distribution.html#estimation-of-mu-and-sigma",
    "href": "chapters/week2/normal_distribution.html#estimation-of-mu-and-sigma",
    "title": "7  The normal distribution",
    "section": "",
    "text": "Example 7.1 - Effect of caffeine on activity\n\n\n\n\n\n\nCaffeine is a central neurvous system stimulant, which can have several positive- and negative effects. In this study, the level of activity is up for examination, and for this purpose a model-system; the runing activity of mice in a wheel reflected as the number of rounds pr minutes recorded over 7 minutes. Here a total of 249 mice from two species; one breed to be high in running performance, and one control, where given either Water, Gatorade or Red Bull followed by measuring their voluentary wheel run activiy. Of interest is the average actvity within each mouse type and for each caffeine type and how large the spread is.\nThe data is listed in a table with 249 rows (here, the first five samples are shown):\nMISSING DATA\n\n\n\n\n\n\n\n\n\n\n\nExample 7.2 - Effect of caffeine on activity - probability\n\n\n\n\n\n\nMISSING DATA",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>The normal distribution</span>"
    ]
  },
  {
    "objectID": "chapters/week2/normal_distribution.html#confidence-interval-for-mu",
    "href": "chapters/week2/normal_distribution.html#confidence-interval-for-mu",
    "title": "7  The normal distribution",
    "section": "7.2 Confidence interval for \\(\\mu\\)",
    "text": "7.2 Confidence interval for \\(\\mu\\)\nThe confidence interval for the center of a normal distribution (\\(\\mu\\)) is calculated as follows:\n\\[\nCI_{\\mu,1-\\alpha}:  \\hat{\\mu} \\pm t_{1-\\alpha/2,df} \\cdot \\hat{\\sigma} / \\sqrt(n)\n\\] \\[\n\\bar{X} \\pm t_{1-\\alpha/2,df} \\cdot s / \\sqrt(n)\n\\]\nwhere \\(t_{1-\\alpha/2,df}\\) is a fractile, a number, which determines the coverage. Here, \\(\\alpha\\) is the left ot part. I.e. if a \\(90 \\%\\) confidence interval is wanted, the left out part is \\(\\alpha = 0.10\\). \\(n\\) is the number of samples on which the mean (\\(\\bar{X}\\)) is estimated, and \\(df\\) is the degrees of freedom, which refers to how well the standard deviation is estimated.\nFor instance, if one needs a \\(95 \\%\\) confidence interval (\\(\\alpha = 0.05\\)) based on a sample of \\(20\\) observations \\(t_{1-\\alpha/2,df} = t_{0.975,20-1} = 2.093\\).\nIt is further noticed that the interval is symmetric around the mean, and that the more samples (\\(n\\)) the lower the spread as the standard deviation is divided by \\(\\sqrt{n}\\).\n\n\n\n\n\n\nExample 7.3 - Effect of caffeine on activity - confidence intervals\n\n\n\n\n\n\nMISSING DATA",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>The normal distribution</span>"
    ]
  },
  {
    "objectID": "chapters/week2/normal_distribution.html#reading-material",
    "href": "chapters/week2/normal_distribution.html#reading-material",
    "title": "7  The normal distribution",
    "section": "7.3 Reading material",
    "text": "7.3 Reading material\n\nVideos giving short condensed introduction to the normal distribution:\n\nThe Normal Distribution\nAn Introduction to the Normal Distribution\nThe Normal Distribution, Clearly Explained\n\nVideo on confidence intervals:\n\nConfidence intervals\n\nChapter 2 and 3 of Introduction to Statistics by Brockhoff\n\nespecially 2.1 to 2.4 (you can skip 2.2 on discrete distributions, which is the subject of week 4) and 3.1 - 3.1.6.\n\n\n\n7.3.1 The Normal Distribution - CrashCourse\n\n\n\n7.3.2 An Introduction to the Normal Distribution - jbstatistics\n\n\n\n7.3.3 The Normal Distribution, Clearly Explained - StatQuest\n\n\n\n7.3.4 Confidence Intervals - CrashCourse",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>The normal distribution</span>"
    ]
  },
  {
    "objectID": "chapters/week2/normal_distribution.html#exercises",
    "href": "chapters/week2/normal_distribution.html#exercises",
    "title": "7  The normal distribution",
    "section": "7.4 Exercises",
    "text": "7.4 Exercises\n\n\n\n\n\n\nExercise 7.1 - Normal Distribution\n\n\n\n\n\n\nSerum cholestorol level is a biomarker of health in relation to a range of life-style related diseases. A population of adults have mean values of \\(178 mg/100mL\\) and \\(207 mg/100mL\\), standard deviations of \\(31 mg/100mL\\) and \\(37 mg/100mL\\) in males and females respectively.\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nDraw curves (by hand - with pen on paper!) and plug in the parameters for these distributions.\nWhat percentage are below \\(150, 178\\) and \\(200 mg/100 mL\\) respectively (for males and females separately)?\nWhat percentage are above \\(140 mg/100 mL\\)\n\n\n\n\n\nA level above \\(240 mg/100 mL\\) is considered high risk, whereas the range between \\(200\\) and \\(239 mg/100 mL\\) is considered borderline high risk.\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nHow big a proportion is considered at high risk (for males and females seperately)?\nHow big a proportion is considered borderline high risk (for males and females seperately)?\nNow assume that the above metrics describing the distribution (mean and standard deviation) is estimated based on samples of \\(30\\) males and \\(30\\) females. What it the changes to the calculations above?\n\n\n\n\n\nA population consists of \\(40\\%\\) males and \\(60\\%\\) females.\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nWhat is the propability that a random person from that population have a serum cholesterol level higher than \\(240 mg/100mL\\).\n\n\n\n\n\nUse that the mean and variance of a sum of two populations is the sum of the means and variance respectively. That is; Let \\(X\\) and \\(Y\\) be two indepedent random variables with mean \\(\\mu_X\\) and \\(\\mu_y\\) and variance \\(\\sigma^2_X\\) and \\(\\sigma^2_Y\\) respectively. And let:\n\\[\nZ = c_1X+ c_2Y\n\\]\nThen the mean and variance of \\(Z\\) is: \\[\n\\mu_Z = c_1\\mu_X + c_2\\mu_Y\n\\] \\[\n\\sigma^2_Z = c_1\\sigma^2_X + c_2\\sigma^2_Y\n\\]\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nWhat is the population mean and standard deviation of a population made up of \\(40\\%\\) males and \\(60\\%\\) females?\nUse this mean and standard deviation to calculate the probability of a random person from that population having a serum cholesterol level higher than \\(240 mg/100mL\\). How is that different from question 7? and why so?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 7.2 - Transformations and the normal distribution\n\n\n\n\n\n\nThe normal distribution (also known as the Gaussian distribution) are central in biology as such, and in data analysis especially.\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nWhy do we (sometimes) transform data before making statistical analysis?\nMake a histogram plot and a qqplot (use qqnorm() and qqline() functions) on the variable Ethyl.pyruvate from the “Wine” data set. Are the data normal distributed?\nTry to Log transform the variable Ethyl.pyruvate. Make a new histogram plot and a corresponding qqplot. Compare with question 1.\n\nUse the command par(mfrow = c(1,2)) (in front of your plotting commands) to get the plots side by side which makes it easier to compare the results.\nAdd a suitable title on both plots using the argument main=\"a very nice title\".\n\nDoes it seem possible to get all samples to match the same distribution?\nDo the same for the variable Benxyl.alcohol.\nDoes the log transformation work? What is the problem?\nTry the square root transformation.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 7.3 - WHO height and weight - standard normal distribution\n\n\n\n\n\n\nHeight and weight are important measures of growth during childhood. However, the largest factor impacting these measures are naturally the age of the child. In order to be able to compare children with slightly different ages such data are transformed using so-called growth curves. These are provided by the WHO and are constructed to represent the world wide distribution at specific ages for boys and girl. In this exercise you are supposed to take height and weight measures from African children living under different circumstances, transform them using WHO numbers for the world wide distribution, and further look for explanatory variables causing differences in growth.\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nStart by importing the GrowthData.xlsx dataset into R. Check that all the info from the excel file was imported correctly using the commands dim() and head(). You can ’chop’ the dataframe in R to isolate the variables of importance (length, sex, age etc.).\nUsing the following graphs (figure 7.1), find the mean and standard deviation for boys and girls at age 15 months.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Z-scores for the length-to-age ratio for girls\n\n\n\n\n\n\n\n\n\n\n\n(b) Z-scores for the length-to-age ratio for boys.\n\n\n\n\n\n\n\nFigure 7.1: Z-scores for the length-to-age ratio for girls and boys aged 0 to 2 years. Source: WHO. Click on the images to enlargen them.\n\n\n\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nBased on the dataset imported, calculate mean and standard deviation for the girls and the boys, how does it relate to the data from WHO?\n\n\n\n\n\nYou can use the command X &lt;- X[complete.cases(X),] to remove the empty rows in the dataframe X, or google how to compute descriptive statistics in the case of missing values.\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nWhat is the Z-value? How can you use it to relate the dataset to the WHO data?\nCalculate the Z-scores for all the boys and girls.\n\n\n\n\n\nThis can be done by creating a new vector inserting the length-vector from the previous dataframe to the equation for Z-score.\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nWhat does it mean that a child is above or below 0 in Z-score? How many children has a Z-score below -2 and -3?\nHow many children are longer/taller than the world average?\nInvestigate whether some of the other variables (source of water primary_watersource or treatment of water water_treatment_type) impact the length at 15 months. Use ggplot() to do the plotting, and then use ggplot(...) + facet_wrap(~SEX) to split the plot in two corresponding to the variable SEX. Are the trends similar for the two sexes?",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>The normal distribution</span>"
    ]
  },
  {
    "objectID": "chapters/week2/central_limit_theorem.html",
    "href": "chapters/week2/central_limit_theorem.html",
    "title": "8  Central limit theorem",
    "section": "",
    "text": "8.1 Reading material\nIn short, the central limit theorem (CLT) says, that the central parameter (e.g. the mean) obtained from a sample (stikprøve) from ANY distribution is normal distributed with variance equal to the sample variance divided by the sample size. That is:\n\\[\n\\bar{X} \\sim \\mathcal{N}(\\mu,\\sigma^2/n) \\quad for \\: n \\rightarrow \\infty\n\\]\nAs we are dealing with samples of finite size, and further needs to estimate the parameters (mean and variance) based on this sample, the Normal distribution is exchanged by the T-distribution. The T-distribution take the uncertainty of having finite data into account. That is, if \\(X_1,X_2,..,X_n\\) is randomly sampled from some distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\) (approximated by \\(s_X^2\\)), then:\n\\[\n\\frac{\\bar{X} - \\mu}{s_X/\\sqrt{n}} \\sim \\mathcal{T}(df) \\quad for \\: finite \\: n\n\\]\nWhere \\(df\\) (degrees of freedom) is equal to how well the variance is estimated (usually \\(df = n-1\\)).",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Central limit theorem</span>"
    ]
  },
  {
    "objectID": "chapters/week2/central_limit_theorem.html#reading-material",
    "href": "chapters/week2/central_limit_theorem.html#reading-material",
    "title": "8  Central limit theorem",
    "section": "",
    "text": "Videos on the central limit theorem:\n\nThe Central Limit Theorem, Clearly Explained\nWhat is the central limit theorem? And why is it so hard to understand?\n\nChapter 8.3 in Introduction to Probability and Statistics Using R by J.G. Kern\n\nAvailable on Absalon\n\n\n\n8.1.1 The Central Limit Theorem, Clearly Explained - StatQuest\n\n\n\n8.1.2 What is the central limit theorem? And why is it so hard to understand? - MrNystrom",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Central limit theorem</span>"
    ]
  },
  {
    "objectID": "chapters/week2/central_limit_theorem.html#exercises",
    "href": "chapters/week2/central_limit_theorem.html#exercises",
    "title": "8  Central limit theorem",
    "section": "8.2 Exercises",
    "text": "8.2 Exercises\n\n\n\n\n\n\nExercise 8.1 - Quality control and central limit theorem\n\n\n\n\n\n\nEnsuring the end product quality in food production is naturally of utmost interest. In doing so, the production is monitored at several critical points, in order to check that it is . This is referred to as statistical process control (SPC) and is under the umbrella of process analytical technology (PAT).\nIn short, a critical point in a production is monitored by a so-called control card, that register a value, e.g. pH, temperature, or other essential product/production parameters. When a point is above or below certain limits, there is an alarm. However, due to natural variation, now and then such alarms occur without there being any systematic faults. These are called false positives.\nA cheese production monitors the 24 hours pH for every production batch within a day. Under normal conditions the false alarms follow a distribution with mean \\(\\mu = 3\\) and variance \\(\\sigma^2 = 3\\) (the distribution is a Poisson distribution, but that is actually not so important here). The production runs very well, so the monitoring of these alarms are only used for retrospective follow up. Suddenly there are reclamations based on the last years production, and the boss want you to check up on whether it is the 24 hours pH causing the trouble.\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nYou check the control card and finds that during the last year there were on average \\(3.2\\) alarms. Does this indicate problems with the 24 hours pH?\nUse the rpois() to generate data from \\(1000\\) years, and check whether your results match.\nImagine that the aggregated measure were over a month (30 days) or over a week (7 days) or as low as average over 2 days. How does this affect the agreement between the analytical probability (via central limit theorem) and the simulated probability (based on the actual underlying distribution)?\n\n\n\n\n\nYou might want to use the following code as inspiration for how it can be done in R.\n\nn &lt;- ? # How many samples to average over\nl &lt;- ? # Population mean\ns &lt;- ? # Population standard deviation\nx &lt;- ? # The odd observation\n\n1 - pnorm(x, mean = ?, sd = ?)\n\n# Simulation \nk &lt;- 10000 # The number of e.g. years to simulate\n\nX &lt;- matrix(rpois(n*k, l), k, n) # Simulate data\n\nmX &lt;- apply(X, 1, mean) # Compute row-wise mean\n\nhist(mX)\n\nsum(mX &gt; x) / length(mX)",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Central limit theorem</span>"
    ]
  },
  {
    "objectID": "chapters/week3/week_3.html",
    "href": "chapters/week3/week_3.html",
    "title": "Week 3",
    "section": "",
    "text": "Hand-in assigment\nIn this week we are introducing the concept of inferential statistics. That is to be able to answer a specific question based on observed data. Biological or scientific questions are, in statistical terms, formulated as a hypothesis, which can be tested using data. One of the most widely used tests, are the t-test for comparison of the mean from two distributions.\nExercise 9.3 Diet and Fat metabolism - T test - in R is to be handed in (through absalon or as hard-copy Wednesday night). You are welcome to put in R-code in the assignment, but it is your argumentation and interpretation that are the most important.",
    "crumbs": [
      "Week 3"
    ]
  },
  {
    "objectID": "chapters/week3/week_3.html#hand-in-assigment",
    "href": "chapters/week3/week_3.html#hand-in-assigment",
    "title": "Week 3",
    "section": "",
    "text": "Important\n\n\n\nComplete Exercise 9.2 prior to the hand-in assignment to make sure you understand what R does during a t-test.",
    "crumbs": [
      "Week 3"
    ]
  },
  {
    "objectID": "chapters/week3/week_3.html#exercises",
    "href": "chapters/week3/week_3.html#exercises",
    "title": "Week 3",
    "section": "Exercises",
    "text": "Exercises\nFor Monday work through the following exercises:\n\nExercise 9.1\nExercise 9.2\nExercise 9.4\nExercise 9.5\n\nFor Wednesday work through the following exercises:\n\nExercise 9.7\nExercise 9.8\nExercise 10.1",
    "crumbs": [
      "Week 3"
    ]
  },
  {
    "objectID": "chapters/week3/week_3.html#case-ii",
    "href": "chapters/week3/week_3.html#case-ii",
    "title": "Week 3",
    "section": "Case II",
    "text": "Case II\nThe second case is described in the document “Case II.pdf”. You should work on the case in groups of four, and hand in a slide-show with voice no later than Thursday evening next week.",
    "crumbs": [
      "Week 3"
    ]
  },
  {
    "objectID": "chapters/week3/t_test.html",
    "href": "chapters/week3/t_test.html",
    "title": "9  T-test",
    "section": "",
    "text": "9.1 Independent T-test",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>T-test</span>"
    ]
  },
  {
    "objectID": "chapters/week3/t_test.html#independent-t-test",
    "href": "chapters/week3/t_test.html#independent-t-test",
    "title": "9  T-test",
    "section": "",
    "text": "9.1.1 Models of two samples\nFirst of all the data are characterized by two models, here normally distributed samples:\n\\[\nX_{11},X_{12},X_{13},...,X_{1n_1} \\sim \\mathcal{N}(\\mu_1,\\sigma_1^2)\n\\]\n\\[\nX_{21},X_{22},X_{23},...,X_{2n_2} \\sim \\mathcal{N}(\\mu_2,\\sigma_2^2)\n\\]\n\n\n9.1.2 Hypothesis\nThe research question goes on the difference between the two samples. Such as question is formalized statistically using the parameters of the two models under investigation and called a hypothesis. Further, a question of is formalized as the opposite; similarity.\nThat is the null hypothesis of no difference between the two sample means is formalized as:\n\\[\nH0: \\mu_1 = \\mu_2\n\\]\nIf this hypothesis turns out not to be true. That is; there is a difference between the two distribution means, then this is referred to as the alternative hypothesis:\n\\[\nHA: \\mu_1 \\neq \\mu_2\n\\]\nor directional:\n\\[\nH_A: \\mu_1&gt;\\mu_2 \\ \\ \\ or \\ \\ \\  H_A: \\mu_1&lt;\\mu_2\n\\]\n\\[\nH_A: \\mu_1&lt;\\mu_2\n\\]\n\n\n9.1.3 Test statistic\nFrom these data, a t-statistic \\(t_{obs}\\) can be calculated:\n\\[\nt_{obs} = \\frac{\\bar{X_1} - \\bar{X_2}}{s_{pooled}\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}\n\\]\nwhere \\(s_{pooled}\\) is the pooled standard deviation, which is simply a weighted mean of the variances.\n\\[\ns_{pooled}^2 = \\frac{(n_1 - 1)s_1^2 + (n_2-1)s_2^2}{n_1 + n_2-2}\n\\]\nAlternatively, this formulation is equivalent:\n\\[\nt_{obs} = \\frac{\\bar{X_1} - \\bar{X_2}}{\\sqrt{\\frac{s_{X_1}^2}{n_1} + \\frac{s_{X_2}^2}{n_2}}}\n\\]\n\n\n9.1.4 Test probability\nThis t-statistics for the non-directional two-sided test can be translated into a probability under the null hypothesis (of no difference).\n\\[\nP = 2 \\cdot P(T_{df} \\geq |t_{obs}|) = 2 \\cdot (1 - P(T_{df} \\leq |t_{obs}|))\n\\]\nAlternatively, one can calculate a confidence interval for the differences between the means, which leads to the same interpretation of the results:\n\\[\n\\bar{X_1} - \\bar{X_2} \\pm t_{1 - \\alpha/2}s_{X_{pooled}}\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}\n\\]\nThe standard deviation used here (\\(s_{X_{pooled}}\\)) is based on weighted average of the individual sample variances.\n\n\n\n\n\n\nExample 9.1 - Effect of caffeine on activity - hypothesis test\n\n\n\n\n\n\nIn (exa-effect-of-caffeine?) concerning the relation between caffeine and voluntary activity of mice, the running activity of mice in a wheel is recorded. In this example we wish to compare the mean activity of the two groups with either water- or Red bull as caffeine source in control runner female mice, and give a confidence interval for these means.\nAs such, from (exa-effect-of-caffeine-conf-int?) the activity seems higher in the Red Bull treated group. The question we wish to answer is: Is there a difference between the two population means. In statistical terms this question is formalized as a hypothesis.\nFirst, models for the data is suggested\n\\[\nX_1 \\sim \\mathcal{N}(\\mu_1,\\sigma^2)\n\\] \\[\nX_2 \\sim \\mathcal{N}(\\mu_2,\\sigma^2)\n\\]\nNote, that the spread \\((\\sigma)\\) is assumed to be similar in the two groups.\nHypothesis\nIf we are interested in a difference, then we formulate the opposite, that is; the two population means are equal.\n\\[\nH0: \\mu_1 = \\mu_2\n\\]\nIf this turns out not to be true, then the alternative is suggested to be:\n\\[\nHA: \\mu_1 \\neq \\mu_2\n\\]\nA statistical test done via first constructing a measure of how far from the \\(H0\\) the data is. This is referred to as the test-statistic and then secondly this measure is translated in to a probability.\nTest statistic\n\\(H0\\) sets the two means to be equal, so naturally a measure of how well this fits with data is reflected as the difference between the observed averages:\n\\[\n\\bar{X}_1 - \\bar{X}_2\n\\]\nwhere a large (positive or negative) value indicates descrepancy from \\(H0\\).\nThis distance depends on the scale of the data, why some kind of normalization needs to be encountered. Further, the number of samples used to calculate the means should also weight in. In total this leads to the t-test statistic.\n\\[\nt_{obs} = \\frac{\\bar{X}_1 -\\bar{X}_2}{s_{pooled} \\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}\n\\]\nWe see that almost all the ingredients for calculating this is given via the descriptive statistics. The only thing needed is the pooled standard deviation (\\(s_{pooled}\\)), which is simply a weighted mean of the variances.\n\\[\\begin{align}\ns_{pooled}^2 &= \\frac{(n_1 - 1)s_1^2 + (n_2-1)s_2^2}{n_1 + n_2-2} \\\\\n&= \\frac{(11 - 1) \\cdot 2.1^2 + (9-1)\\cdot 2.1^2}{11 + 9-2} \\\\\n&= 2.1^2   \n\\end{align}\\]\nThis leads to:\n\\[\\begin{align}\nt_{obs} &= \\frac{\\bar{X}_1 -\\bar{X}_2}{s_{pooled} \\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}} \\\\\n&= \\frac{8.6 -10.2}{2.1 \\sqrt{\\frac{1}{11} + \\frac{1}{9}}} \\\\\n&= 1.61   \n\\end{align}\\]\nP-value\nThe test statistics is translated to a probability by the following:\n\\[\nP = 2 \\cdot P(T_{df} \\geq |t_{obs}|) = 2 \\cdot (1 - P(T_{df} \\leq |t_{obs}|))\n\\]\nwhich corresponds to the colored areas shown in figure 9.1.\n\n\nCode\n# Dummy data to make geom_area actually plot\ndf &lt;- data.frame(x = 1, y = 42)\n\n# Arguments for pt function\ndtArgs &lt;- list(df = 20 - 2)\n\n# Critical values\nlwrCrit &lt;- qt(0.05/2, 30)\nuprCrit &lt;- qt(1 - 0.05/2, 30)\n\n# Plot\nggplot(df, aes(x, y)) +\n  xlim(-4, 4) +\n  geom_function(fun = dt, args = dtArgs, \n                linewidth = 1\n  ) +\n  geom_area(stat = \"function\", fun = dt, args = dtArgs, # Lower crit region\n            xlim = c(-5, lwrCrit), \n            fill = \"darkgreen\", alpha =.5\n  ) +\n  geom_area(stat = \"function\", fun = dt, args = dtArgs, # Upper crit region\n            xlim = c(uprCrit, 5), \n            fill = \"darkgreen\", alpha =.5\n  ) +\n  labs(x = \"t-statistic\",\n       y = \"Probability\"\n  ) +\n  theme_bw()\n\n\n\n\n\n\n\n\nFigure 9.1: The critical regions of the t-statistic for a t-test with \\(\\alpha = 0.05\\) (i.e. the total area of the shaded areas is equal to \\(0.05\\)) and \\(df = 20 - 2\\).\n\n\n\n\n\nThis has no analytical solution, but can be assessed from tables or via pt() in R:\n\nt_obs &lt;- 1.610\ndf &lt;- 20 - 2\n\n2 * (1 - pt(t_obs, df))\n\n[1] 0.1247948\n\n\nIn conclusion; Although a difference in activity is observed when mice are given Red Bull compared to water, it is Not unlikely (\\(p = 0.12\\)) that this could be due to chance. Actually this would happen one out of eight times.\nThe entire procedure can be done in R:\nMISSING DATA",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>T-test</span>"
    ]
  },
  {
    "objectID": "chapters/week3/t_test.html#paired-t-test",
    "href": "chapters/week3/t_test.html#paired-t-test",
    "title": "9  T-test",
    "section": "9.2 Paired T-test",
    "text": "9.2 Paired T-test\nA special case, is where the two samples (\\(X_{11},X_{12},X_{13},...,X_{1n}\\) and \\(X_{21},X_{22},X_{23},...,X_{2n}\\)) are paired. That is for example measurements before and after treatment on the same set of samples or when the same assessor in a sensorical panel scores two different product types.\nThe question of interest is still the same; namely is there a difference between the two populations, however, the statistics is slightly different.\n\n9.2.1 Model\nThe model of the data is extended with a sequence of differences\n\\[\\begin{align}\n    & D_1,D_2,\\ldots,D_n  \\nonumber  \\\\\n=   & (X_{12} - X_{11}), (X_{22} - X_{21}), \\ldots , (X_{n2} - X_{n1}) \\\\\n\\sim & \\mathcal{N}(\\mu_{D},\\sigma_{D}^2)  \\nonumber\n\\end{align}\\]\nWhere \\(D_i\\) is the difference between the responses for each of the paired observations \\(i\\) (\\(i = 1,2,\\ldots,n\\)), and \\(s_D\\) is the observed standard deviation calculated on \\(D_1,D_2,\\ldots,D_n\\).\n\n\n9.2.2 Hypothesis\nThe hypothesis of similarity is then formalized on \\(\\mu_D\\):\n\\[\\begin{equation}\nH0: \\mu_D = 0\n\\end{equation}\\]\nOften with the alternative of a difference:\n\\[\\begin{equation}\nHA: \\mu_D \\neq 0\n\\end{equation}\\]\n\n\n9.2.3 Test Statistic\n\\[\\begin{equation}\nt_{obs} = \\frac{\\bar{D} }{s_{D} / \\sqrt{n}}\n\\end{equation}\\]\nWhich is translated to a p-value through a \\(\\mathcal{T}_{df}\\) with \\(df = n-1\\) degrees of freedom.\nAs such, the test statistics only sees the original data via paired differences, and hence become a specialty of the single-sample case.\n\n\n\n\n\n\nExample 9.2 - Natural phenolic antioxidants for meat preservation - paired T-test\n\n\n\n\n\n\nThis example is based on the sensory data on the meat sausages treated with green tea (GT) and rosemary extract (RE) or control, used previously in a PCA example ?sec-example-meat-pca.\nT-tests can be used to evaluate the effects (the response of a single variable) of two treatments against each other. Here, we use the week 4 data, and compare the rosemary extract treatment to the control.\nLoad the data and libraries\nRemember to set your working directory.\n\nlibrary(ggplot2)\nload(\"meat_data.RData\")\n\nPlot data\nFirst the data under investigation is plotted - only week 4, control, and RE samples are used (figure 9.2).\n\n# Subset data such that Treatment = \"Control\" or \"RE\" AND week = 4 \ndf &lt;- X[X$Treatment %in% c(\"Control\", \"RE\") & X$week == 4, ]\n\nggplot(df, aes(Treatment, Texture_Boiled_egg_white, group = Assessor)) +\n  geom_point() +\n  geom_line() +\n  theme_bw()\n\n\n\n\n\n\n\nFigure 9.2: Paired sets of data points for the boiled egg texture variable in week 4 of the experiment. Each line represents a single assessor scoring both samples.\n\n\n\n\n\nThe figure clearly shows that there is huge variation related to Assessor (connected by lines). Although the distributions of the two groups are overlapping, ALL assessors scores the RE sample lower than the Control sample.\nDefine subsets of data\n\n# Subset data such that Treatment = \"Control\" AND week = 4\ncontrolWeek4 &lt;- X[X$Treatment == \"Control\" & X$week == 4, ]\n\n# Subset data such that Treatment = \"RE\" AND week = 4\nREWeek4 &lt;- X[X$Treatment == \"RE\" & X$week == 4, ]\n\nBe aware that the Assessors are ordered ensuring pairing in the two arrays controlWeek4 and REWeek4. Take precaution, as this might not always be the case!\nModel of data\nAs it is the same assesors that are used for evaluation of both products the model of the data becomes:\n\\[\\begin{equation}\nX_{control} \\sim \\mathcal{N}(\\mu_{control},\\sigma_{control}^2) \\ \\ \\text{and} \\ \\ X_{RE} \\sim \\mathcal{N}(\\mu_{RE},\\sigma_{RE}^2)\n\\end{equation}\\]\n\\[\\begin{align}\nD_1,D_2,\\ldots,D_n \\sim \\mathcal{N}(\\mu_{D},\\sigma_{D}^2)\n\\end{align}\\]\nwhere \\(D_i\\) is the difference between rosemary extract- and control treated sausages with respect to the sensorical score (texture - boiled egg) for assessor \\(i\\) (\\(i = 1,2,\\ldots,n\\))\n\\[\\begin{align}\n& D_1,D_2,\\ldots,D_n = (X_{1,RE} - X_{1,control}), (X_{2,RE} - X_{2,control}), \\ldots , (X_{n,RE} - X_{n,control}).\n\\end{align}\\]\n*Hypothesis\nIf we are interested in a difference, then we formulate the opposite, that is; on average the difference between sensorical scores are \\(0\\).\n\\[\\begin{equation}\nH0: \\mu_D = 0\n\\end{equation}\\]\nIf this turns out not to be true, then the alternative is suggested to be:\n\\[\\begin{equation}\nHA: \\mu_D \\neq 0\n\\end{equation}\\]\nT-test on the boiled egg texture variable\nThe T-test should be a paired T-test, since it is the same assosors tasting each of the treated sausages.\n\nt.test(controlWeek4$Texture_Boiled_egg_white,\n       REWeek4$Texture_Boiled_egg_white,\n       paired = T\n       )\n\n\n    Paired t-test\n\ndata:  controlWeek4$Texture_Boiled_egg_white and REWeek4$Texture_Boiled_egg_white\nt = 3.7747, df = 7, p-value = 0.00694\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n 1.20123 5.23002\nsample estimates:\nmean difference \n       3.215625 \n\n\nThe p-value is \\(p = 0.007\\) and the null-hypothosis must therefore be rejected meaning there is a very significant difference in boiled egg texture between the control and rosemary extracted treated samples.\nT-test on the salty taste variable\nIn a similar fashion another sensorical variable is tested; namely salty taste.\n\nt.test(controlWeek4$Taste_Salty,\n       REWeek4$Taste_Salty,\n       paired = T\n       )\n\n\n    Paired t-test\n\ndata:  controlWeek4$Taste_Salty and REWeek4$Taste_Salty\nt = 0.98163, df = 7, p-value = 0.359\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -0.5811623  1.4061623\nsample estimates:\nmean difference \n         0.4125 \n\n\nThe p-value is \\(p = 0.36\\) and the null-hypothosis can therefore not be rejeceted, meaning there is no significant difference in salty taste between the control and rosemary extract treated samples.\nComparison with PCA results\nWhen the results obtained from the T-tests are compared to a PCA on the week 4 data, it can be seen that they are in agreement. The very significant effect of the rosemary extract treatment on the boiled egg texture corresponds well with the loading being strongly associated with the control samples, in a direction which seems to be explanatory for the difference between the roseary extract- and control treatment groups. The non-significant effect on the salty taste variable is congruent with its loading being located in a direction which does not seem to be of importance in the separation of the control and treated groups.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>T-test</span>"
    ]
  },
  {
    "objectID": "chapters/week3/t_test.html#reading-material",
    "href": "chapters/week3/t_test.html#reading-material",
    "title": "9  T-test",
    "section": "9.3 Reading material",
    "text": "9.3 Reading material\n\nVideos on the elements of statistical testing focusing on hypothesis testing:\n\nHypothesis Testing and The Null Hypothesis, Clearly Explained\np-values: What they are and how to interpret them\n\nVideos on the test statistics and the T-test:\n\nTest Statistics\nT-Tests: A Matched Pair Made in Heaven\n\nChapter 3 of Introduction to Statistics by Brockhoff\n\n\n9.3.1 Hypothesis Testing and The Null Hypothesis, Clearly Explained - StatQuest\n\n\n\n9.3.2 p-values: What they are and how to interpret them - StatQuest\n\n\n\n9.3.3 Test Statistics - CrashCourse\n\n\n\n9.3.4 T-Tests: A Matched Pair Made in Heaven - CrashCourse",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>T-test</span>"
    ]
  },
  {
    "objectID": "chapters/week3/t_test.html#exercises",
    "href": "chapters/week3/t_test.html#exercises",
    "title": "9  T-test",
    "section": "9.4 Exercises",
    "text": "9.4 Exercises\n\n\n\n\n\n\nExercise 9.1 - T-test\n\n\n\n\n\n\nT-test comparison of a womens football team and a mens football team. The mean weight of the mens team is \\(\\bar{X}_{men} = 82kg\\) with a standard deviation of \\(s_{men} = 8kg\\), while the womens team has a mean weight of \\(\\bar{X}_{women} = 61kg\\) and a standarddeviation of \\(s_{women} = 6kg\\). There are 11 players on each team.\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nState the model and hypothesis\nCalculate the test statistic (\\(t_{obs}\\))\nCalculate the p-value\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 9.2 - Diet and fat metabolism - T-test by hand\n\n\n\n\n\n\nThe diet is a central factor involved in general health, and especially in relation to obesity, where a balance between intake of protein, fat and carbohydrates, as well as type of these nutrients seems important. Therefor various controlled studies are conducted to show the effect of different diets. A study examining the effect of protein from milk (casein or whey) and amount of fat on growth biomarkers of fat metabolism and type I diabetes was conducted in \\(40\\) mouse over an intervention period of \\(14\\) weeks.\nFor this exercise we are going to focus on cholesterol as a biomarker related to fat metabolism, and on a low fat diet (LF) and a high fat diet (HF). The cholesterol level at the end of the \\(14\\) week intervention is listed below (table 9.1).\n\n\n\nTable 9.1: Cholesterol samples from the end of week 14 for the low-fat (LF) and high-fat (HF) diet.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDiet\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\sum X\\)\n\\(\\sum X^2\\)\n\n\n\n\nLF\n3.97\n3.69\n2.61\n4.03\n2.98\n3.51\n3.62\n2.81\n3.62\n3.53\n\n\n\n\n\n34.37\n120.20\n\n\nHF\n4.68\n3.60\n4.84\n4.92\n3.70\n4.83\n3.38\n4.62\n4.60\n4.84\n4.84\n4.54\n5.27\n4.26\n4.3\n67.29\n305.92\n\n\n\n\n\n\n\nThis exercise is supposed to be done by hand where the computer only is used as a pocket calculator.\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nCalculate descriptive statistics for these data.\nSketch these results in a graph (by pen and paper - no computer)\nGive a frank guess on cholesterol differences between diets.\nState a hypothesis, and calculate the test statistics and the corresponding p-value.\n\nWhat assumptions did you make concerning the variances in the two distributions? What alternatives do you have?\n\nGive a confidence interval for the difference between diets.\nHow does the t-test result (p-value) correspond to the confidence interval?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 9.3 - Diet and fat metabolism - T-test in R\n\n\n\n\n\n\nThis exercise repeats some of the elements from Exercise 9.2 using R including an extension of the panel of relevant biomarkers and utilization of PCA to get a comprehensive overview.\nThe data can be found in the file Mouse_diet_intervention.xlsx. The code below imports the data, and subsets on two (of the three diets). The factor diet is called diet_fat, whereas cholesterol is called cholesterol.\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nRepeat the analysis of Exercise 9.2 using R, including plotting and testing.\n\nHint: For testing you might want to predefine the response and factor and then use the function t.test() to run the analysis, which actually returns most of the relevant results - see the code box below.\n\nHow robust are the results towards transformation of the response or extreme samples?\n\n\n\n\n\nIn addition to cholesterol, other biomarkers have been measured, such as insuline, triglycerides etc.\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nUse one of these additional biomarkers as measure of health status, and repeat the analysis comparing dietary fat (including plotting).\nMake a PCA on the biomarkers insulin, cholesterol, triglycerides, NEFA, glucose, and HbA1c, and plot the results.\n\nNEFA = nonesterified fatty acids.\n\nComment on the results. Do the T-test results fit with the PCA results? Are there other markers of dietary fat intake? Does the correlation structure make sense? (The later is hard to answer without physiological knowledge, but give it a shot)\n\n\n\n\n\n\n# Import data\nmouse &lt;- read_excel(\"Mouse_diet_intervention.xlsx\")\n\n# Include only rows where diet_protein = casein\nmouse &lt;- mouse[mouse$diet_protein == \"casein\", ]\n\n# Predefine response and factor\ndiet &lt;- mouse$diet_fat\ny &lt;- mouse$cholesterol\n\n\n\n\n\n\n\n\n\n\n\nExercise 9.4 - Fiber and cholesterol\n\n\n\n\n\n\nFiber is suspected to have benificial physiological activity if being a part of a regular diet. In this study \\(n = 13\\) healthy young men and women were enroled in a trial to unravel the effect of fiber supplement. At baseline (that is before dietary intervention), the persons were screned for a range of biomarkers in the blood including cholesterol fractions. Then, they were put on a diet with supplementation of dietary fibers for a period of \\(30\\) days. In the dataset FiberData.xlsx, the baseline levels (_t0), and end of trial levels (_t30) are listed for total-, hdl- and ldl cholesterol. The data is a part of a larger study conducted at Department of Nutrition Exercise and Sports.\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\nAnalyse if there is an effect of the fiber intervention on these biomarkers. That includes; inspection of raw data, possible transformations, visulization of effects, descriptive statistics, test of effect and presentation of relevant confidence intervals.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 9.5 - Stability of oil under different conditions\n\n\n\n\n\n\nOil are primary made up of triglycerides, where some of the fatty acids are unsaturated. This causes such a product to be susceptible to oxidation both from chemical oxidative agents such as metal ions, or from exposure to light. Oxidation of the unsaturated fatty acids changes the sensorical and physical properties of the oil.\nIn the southern part of Africa grows a robust bean - the Marama bean. This bean has a favorable dietary composition, including dietary fibers, fats and proteins, as most similar types of nuts, therefor this crop could be utilized for making healthy products by the locals for the locals. One such product is Marama bean oil. A study has been conducted to investigate the oxidative stability of the oil under various conditions. In the dataset MaramaBeanOilOx.xlsx are listed the results from such an experiment (including data from both normal and roasted beans). The experimental factors are:\n\nStorage time (Month)\n\nProduct type (Product)\nStorage temperature (Temperature)\nStorage condition (Light)\nPackaging air (Air)\n\nAnd the response variable reflecting oxidative stability is peroxide value and named PV.\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nRead in the data, and subset so that you only include data related to product type Oil.\nMake descriptive plots of the response variable PV imposing storage-temperature, condition and time.\n\nHint: factor(Temperature):Light specifies all combinations of these two factors. facet_grid(.~Month) splits the plot into several plots according to Month.\n\nWhat do you observe in terms of storage- time, temperature and condition from this plot?\n\n\n\n\n\nSome of the differences is so pronounced that testing seems irrelevant. However, there are small differences between storage temperature at storage condition dark.\n\nZack out the data for these two groups at storage time = 0.5 month, and make a comparison.\nDo the same at storage time = 1 month.\n\nWe would like to generate profiles for each condition over time, reflecting centrality and dispersion. In order to get there, we need to massage the data a little. The function summarySE() from the package Rmisc is a nice tool to construct a dataset with summary statistics. Try to run the code below - what have we learned about oil oxidation from this study?\n\n# Compute summary table for \"PV\"\nmaramaSummary &lt;- summarySE(maramaOil, measurevar = \"PV\",\n                           groupvars = c(\"Light\", \"Temperature\", \"Month\"))\n\n# Repeat time = 0 for all conditions\nM1 &lt;- maramaSummary[1, ]\nM1$Light &lt;- \"light\"\nM1$Temperature &lt;- 25\n\nM2 &lt;- maramaSummary[1, ]\nM2$Temperature &lt;- 35\n\nMtot &lt;- rbind(maramaSummary, M1, M2)\n\n# Plot a nice oxidation graph with errorbars\nggplot(Mtot, aes(x = Month, y = PV, color = factor(Temperature):Light)) +\n  geom_point() +\n  geom_line() +\n  geom_errorbar(aes(ymin = PV - se, ymax = PV + se), width = .03) +\n  theme_bw()\n\n\n\n\n\n\n\n\n\n\n\nExercise 9.6 - Aroma in milk and cheese\n\n\n\n\n\n\nIn order to increase the biodiversity of hayfields it is of interest growing different kinds of herbs together with the hay. Hay is used for feeding of cows. Type of feed may affect the amount and type of flavor components in the milk as well as the flavor components in cheese made from the milk. Hence, increasing the biodiversity of hayfields used for cow feed may impact the taste of milk and cheese. An experiment was conducted in order to investigate this. In the experiment cows were fed three different diets:\n\nFeed 1: Ryegrass and white clover\nFeed 2: Feed 1 + red clover, chicory and ribwort plantain\nFeed 3: Feed 2 + lucern, birdsfoot trefoil, melilot, caraway, yarrow and salad burnet.\n\nFlavor components were quantified in the raw milk as well as in the cheese after \\(12\\) months of ripening. Three farmers (G, H, and P) participated in the experiment. Furthermore, controls (K), consisting of bulk milk samples from the dairy, are included. The data are found in the file Milk_Cheese_Aroma.xlsx.\nData are originating from the EcoServe project and is kindly provided by Thomas Bæk Pedersen, Department of Food Science, University of Copenhagen.\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nImport the data set.\nOpen the data and get familiar with the data set (that could include something like; how many samples, how many samples from each combination of feed and farmer, how many variables, and some descriptive plots of some of them).\n\n\n\n\n\nIn this exercise we are interested in making a pairwise comparison to investigate whether differences exists between the feedings. Hence, for now we will exclude the controls.\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nMake two subsets of the data; one subset for milk samples and one subset for cheese samples. Exclude controls in the subsets.\n\n\nInvestigate a given aroma compound (e.g. Limonene) in the milk subset. This can be done by e.g. making a boxplot with feed on the x-axis and filling color according to farmer. Do you see any systematic effect of feed?\n\n\n\n\n\n\n# Investigate the code and create the two subsets (one for cheese and one for milk).\n# What does the \"&\", \"|\", and \"!\" do?\nMilk &lt;- subset(Data, Sample == \"Milk\" & Farmer == \"K\")\nMilk &lt;- subset(Data, Sample == \"Milk\" & !Farmer == \"K\")\nMilk &lt;- subset(Data, Sample == \"Milk\" | Farmer == \"K\")\nMilk &lt;- subset(Data, Sample == \"Milk\" | !Farmer == \"K\")\n\n# Make a boxplot\nggplot(yourDataHere, aes(xVariable, yVariable, fill = yourFactor)) +\n  geom_boxplot()\n\nIn a paired t-test we will investigate whether there is a difference between feed 1 and feed 3. In a paired t-test we are investigating if the mean of differences between pairs is significantly different from zero. The following steps will take you through the test.\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nCalculate the difference (for e.g. Limonene) between each pair. Note that these differences now represent your sample\n\n\n\n\n\n\nfeed1 &lt;- subset(Milk, Feed == \"I\") # Extract feed I\nfeed1 &lt;- feed1[with(feed1, order(Farmer)), ] # Sort according to farmer\n\nfeed3 &lt;- subset(Milk, Feed == \"III\") # Extract feed III\nfeed3 &lt;- feed3[with(feed3, order(Farmer)), ] # Sort according to farmer\n\ndiff &lt;- feed1 - feed3 # Compute the difference\n\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nMake a boxplot of the differences.\n\n\nAdd to the boxplot a zero line and the mean of the differences\nDiscuss the plot\nWhy do we add a line with intercept y=0? Why do we add the mean?\n\n\n\n\n\n\nfeed1 &lt;- subset(Milk, Feed == \"I\") # Extract feed I\nfeed1 &lt;- feed1[with(feed1, order(Farmer)), ] # Sort according to farmer\n\nfeed3 &lt;- subset(Milk, Feed == \"III\") # Extract feed III\nfeed3 &lt;- feed3[with(feed3, order(Farmer)), ] # Sort according to farmer\n\ndiff &lt;- data.frame(\"diff\" = feed1$Limonene - feed3$Limonene) # Compute the difference\nmeanDiff &lt;- mean(diff$diff) # Calculate mean of difference\n\nggplot(diff, aes(x = 1, y = diff)) +\n  geom_boxplot() +\n  geom_hline(aes(yintercept = 0), color = \"red\", linetype = \"dotted\") +\n  geom_point(x = 1, y = meanDiff, color = \"darkred\", size = 5)\n\nWe have now calculated the differences and we have calculated the mean of the differences. However, the mean is an estimated value and in order to find out whether there is a significant difference between feed 1 and feed 3, we need to calculate the confidence interval round the mean.\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nCalculate the mean and the standard deviation of the differences and use these calculations to find the confidence interval of the mean.\n\nFor a 95% confidence interval you can find the critical t-value by calling abs(qt(0.025,df)) for a two sided test. How would the R-call look if you were calculating the critical t-value for a one-sided test?\n\nIs zero included in the confidence interval? What does this mean?\nCalculate the t-statistic. Is there a significant difference between the two feedings?\nUnderstand the following code and call it in R.\n\nUnderstand the output and compare the confidence interval and the t-statistics with what you calculated in question 6 and question 7.\n\n\n\nt.test(feed1$Limonene, feed3$Limonene,\n       mu = 0, alt = 'two.sided', paired = T, conf.level = 0.95)\n\n\nNow investigate the cheese-subset. Pick the same aroma compound you were working on during the milk-subset. Do you think there is a difference in the aroma compound between feed I and feed III when looking at the cheese-samples? Write the hypothesis and test it using a paired t-test.\n\nUse the R-call t.test(……).\n\n\nIf you were a farmer, would you be worried that increasing the biodiversity of your hayfields with herbs would result in altered taste in the milk/cheese?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 9.7 - Power of paired tests\n\n\n\n\n\n\nIn Exercise 9.4 Fiber and Cholesterol you have probably used either a paired t-test or a two-sample t-test. Try to conduct both of these tests and see the difference. What needs to be full filled in order to make a paired test? How many participants would have been enrolled in the study, in the case where a two sample t-test were used for inferential analysis of the data?\n\n\n\n\n\n\n\n\n\n\nExercise 9.8 - Confidence intervals\n\n\n\n\n\n\nThe datafile Wine.xlsx lists aroma compounds from different wines from four countries. The compound Acetic acid has a skewed distribution, and therefore needs an appropriate transformation. Calculate the center of the distribution for each country and give a confidence interval for this parameter, where you take into account the need for transformation, but still want to report the results in original units.\nThe R function t.test() is capable of doing some of the calculations.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>T-test</span>"
    ]
  },
  {
    "objectID": "chapters/week3/hypothesis_testing.html",
    "href": "chapters/week3/hypothesis_testing.html",
    "title": "10  Hypothesis testing",
    "section": "",
    "text": "10.1 Reading material",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Hypothesis testing</span>"
    ]
  },
  {
    "objectID": "chapters/week3/hypothesis_testing.html#reading-material",
    "href": "chapters/week3/hypothesis_testing.html#reading-material",
    "title": "10  Hypothesis testing",
    "section": "",
    "text": "Video on the ideas behind hypothesis testing:\n\nThe Essential Guide To Hypothesis Testing\n\nChapter 3 of Introduction to Statistics by Brockhoff\n\nEspecially chapter 3.1 - 3.1.3 and 3.1.6.\n\n\n\n10.1.1 The Essential Guide To Hypothesis Testing - Very Normal\n\n\n\n10.1.2 Exercises\n\n\n\n\n\n\nExercise 10.1 - Hypothesis testing\n\n\n\n\n\n\nThis exercise is conceptual. The idea is, that you should think about a problem, and try to figure out what the null hypothesis is, and further what could be a relevant metric for measuring the distance to this null hypothesis.\n\nYou playing with a six-sided dice, and you are observing abnormally high number of fives. You set up an experiment to test whether the dice is skew, where you trow dice and register the outcome a number of time. What is the null hypothesis in this experiment?\nTwo response variables in a sample of size \\(n\\) seem to track, i.e. is positively correlated. What is the null hypothesis for testing this relation? and what measures the distance to this hypothesis?\nIn an experiment you have a treatment with three levels (placebo, treatment A and treatment B) and some relevant response variable. You are interested in whether there is a difference between the treatments.And in particular whether A is different from placebo and whether B is different from placebo. What is the null-hypothesis for the former question? and what would be a relevant metric for measuring the distance to that hypothesis? What is the null hypothesis for the pairwise comparisons and what further relevant metrics for measuring this distance.\nAccording to theory there is a proportional linear relation between \\(y\\) and \\(x\\). You fit a line between the two (\\(y = a + bx\\)). What is the null hypothesis concerning proportionality? and what measures the distance to this hypothesis?\n\n\n\n\n\n\n\n\n\n\n\nExercise 10.2 - Association or causality?\n\n\n\n\n\n\nThis exercise is intended to show, that you need to be careful with drawing conclussions solely based on statistical numbers (confidence intervals, p-values,…), and that you need to be critical and think about the study design, biology, life, etc.\nA study wants to investigate a certain biomarker in the discovery of cancer. From a population of cancer patients a sample of \\(n = 123\\) patients is taken, and their blood is investigated for a specific biomarker (BMa). The mean and standard deviation of this sample is estimated to \\(\\bar{x} = 3.4\\) mg/L and \\(s_x = 1.5\\) mg/L respectively.\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nCalculate the standard error for the mean of the distribution.\nMake a confidence interval for the mean of the distribution.\n\n\n\n\n\nThe average in this population seems rather high from a biological point of view. However, the researchers want to verify that this is indeed the case, and therefore go out and recruites a population of healthy individuals of size \\(n = 130\\). The discriptive statistics for this group is \\(\\bar{x} = 2.9\\) mg/L and \\(s_x = 1.3\\) mg/L.\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nMake a confidence interval for the mean in the healthy population.\nSketch the two population distributions. Are there an overlap?\nSketch the two confidence intervals and contemplate over similarity/differences between these two populations.\n\n\n\n\n\nThe researchers ask the question of whether the two distributions are similar.\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nFormulate the question as a null- and alternative hypothesis.\nTest the hypothesis, and comment on the question raised.\n\n\n\n\n\nThe answer to the question seems to indicate differences between the two populations. Now the researchers take this one step further, and claims, that this must be due to cancer status.\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nWhat is problematic in drawing the conclusion, that differences in BMa is caused by cancer status?\n\nHint: Think about study-design, and other differences between the two populations such as age, lifestyle etc.\n\nIn order to be certain about cancer leading to increased levels of BMa, which circumstances must be fullfilled? Is possible to make such studies on humans? Mice?",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Hypothesis testing</span>"
    ]
  },
  {
    "objectID": "chapters/week4/week_4.html",
    "href": "chapters/week4/week_4.html",
    "title": "Week 4",
    "section": "",
    "text": "Hand-in assignment\nIn this week we are going to discuss non-continuous data, and look at binary and count data. This task involves probability distributions such as binomial distribution and the Poisson distribution.\nExercise 11.4 Fig bars is to be handed in (through absalon or as hard-copy Wednesday night). You are welcome to put in R-code in the assignment, but it is your argumentation and interpretation that are the most important.",
    "crumbs": [
      "Week 4"
    ]
  },
  {
    "objectID": "chapters/week4/week_4.html#exercises",
    "href": "chapters/week4/week_4.html#exercises",
    "title": "Week 4",
    "section": "Exercises",
    "text": "Exercises\nFor Monday work through the following exercises:\n\nExercise 11.1\nExercise 11.2\nExercise 11.3\n\nIf you really think that this is easy stuff, then try to work through exercise Exercise 11.5\nFor Wednesday work through the following exercises:\n\nExercise 12.1\nExercise 12.2\nExercise 12.3",
    "crumbs": [
      "Week 4"
    ]
  },
  {
    "objectID": "chapters/week4/week_4.html#case-ii",
    "href": "chapters/week4/week_4.html#case-ii",
    "title": "Week 4",
    "section": "Case II",
    "text": "Case II\nThe second case should be handed in as a slide-show with voice no later than Thursday evening.",
    "crumbs": [
      "Week 4"
    ]
  },
  {
    "objectID": "chapters/week4/binomial_data.html",
    "href": "chapters/week4/binomial_data.html",
    "title": "11  Binomial data",
    "section": "",
    "text": "11.1 Reading material\nIn short, binomial data are of the type either / or, and is normally recorded as a list of \\(0\\)’s and \\(1\\)’s. The binomial distribution is characterized by how many trials there is conducted (\\(n\\)) and the probability of a positive (\\(\"1\"\\)) response (\\(p\\)). In notation that is:\n\\[\\begin{equation}\nX \\sim \\mathcal{B}(n,p)\n\\end{equation}\\]\nThe probability density function (pdf) evaluating the probability of \\(x\\) (out of \\(n\\)) positive responses is:\n\\[\\begin{equation}\n%\\binom{N}{K}\nP(X=x) = \\binom{n}{x} \\cdot p^x(1-p)^{n-x}\n\\end{equation}\\]\nWhere\n\\[\\begin{equation}\n%\\binom{N}{K}\n\\binom{n}{x} = \\frac{{n!}}{{x!\\left( {n - x} \\right)!}}\n\\end{equation}\\]\nis the binomial coefficient, which calculates how many combinations of \\(x\\) in \\(n\\) there exists.\nThe cumulative density function (cdf) simply sums op the individual point probabilities.\n\\[\\begin{equation}\n%\\binom{N}{K}\nP(X \\leq x) = P(X=0) + P(X=1) + ... + P(X=x)\n\\end{equation}\\]\nBased on data from \\(n\\) trials with \\(x\\) positive responses, the parameter \\(p\\) can be estimated as the frequency:\n\\[\\begin{equation}\n\\hat{p} = \\frac{x}{n}\n\\end{equation}\\]\nWith the following standard error:\n\\[\\begin{equation}\nS_{p} = \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\n\\end{equation}\\]\nFrom the Central Limit Theorem follows that the point estimate for \\(p\\) is approximately normally distributed, why the confidence interval is as follows:\n\\[\\begin{equation}\nCI_{p}: \\hat{p} \\pm z_{1-\\alpha/2}S_{p}\n\\end{equation}\\]\nBe aware that the extreme cases where \\(x = 0\\) or \\(x = n\\) does not comply with the above mentioned method for calculating the confidence interval, as \\(S_{p} = 0\\). A surpass in this situation is to find a value for \\(p\\) under which the observed data is still likely. For instance; find \\(p\\) such that:\n\\[\\begin{equation}\nP(X=0) = (1-p)^{n}&gt;\\alpha\n\\end{equation}\\]",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Binomial data</span>"
    ]
  },
  {
    "objectID": "chapters/week4/binomial_data.html#reading-material",
    "href": "chapters/week4/binomial_data.html#reading-material",
    "title": "11  Binomial data",
    "section": "",
    "text": "Introductory videos to the binomial distribution:\n\nThe Binomial Distribution\nAn Introduction to the Binomial Distribution\nThe Binomial Distribution and Test, Clearly Explained\n\nVideo on probability distribution:\n\nThe Main Ideas behind Probability Distributions\n\nChapter 2.1 and 2.2 of Introduction to Statistics by Brockhoff\n\n\nThe Binomial Distribution - CrashCourse\n\n\n\nAn Introduction to the Binomial Distribution - jbstatistics\n\n\n\nThe Binomial Distribution and Test, Clearly Explained - StatQuest\n\n\n\nThe Main Ideas behind Probability Distributions - StatQuest",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Binomial data</span>"
    ]
  },
  {
    "objectID": "chapters/week4/binomial_data.html#exercises",
    "href": "chapters/week4/binomial_data.html#exercises",
    "title": "11  Binomial data",
    "section": "11.2 Exercises",
    "text": "11.2 Exercises\n\n\n\n\n\n\nExercise 11.1 - Chocolate and binomial data\n\n\n\n\n\n\nA chocholate factory discards 68 chocolates out of 300 produced in a day.\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nState the model for these data\nCalculate the probability of discarding a chocolate (\\(\\hat{p}\\))\nCalculate the confidence interval for \\(\\hat{p}\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 11.2 - Triangle test\n\n\n\n\n\n\nIn sensorical science several types of experiments can be used for measuring (dis-)similarity between products. One of those is the Triangle Test.\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nCheck out on the internet how this test is conducted.\nAssume that you run a Triangle test with \\(n\\) judges. What is the nature- and statistical model for the data obtained from such a trial?\nState a null hypothesis, relating to the question of similarity of products in the study.\nAssume that you include \\(n = 20\\) judges in your experiment. How many correct answers do you need in order to statistically prove difference between products?\n\nYou need to define what statistically prove means.\nHint: The result is found by trial and error computation - see code below.\n\n\n\n\n\n\n\np &lt;- 1/3 # Parameter under the null hypothesis\nn &lt;- 20 # Number of trials \nx &lt;- 1:n # Define all possible outcomes\n\n# Outcome probability under the null distribution\nnullProb &lt;- 1 - pbinom(x - 1, n, p)\n\n# Put results into dataframe\ndf &lt;- data.frame( \n  nCorrect = x,\n  nullProb\n)\n\n# Plot data\nggplot(df, aes(nCorrect, nullProb)) +\n  geom_line() +\n  geom_hline(yintercept = 0.05) # Alpha value\n\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nGeneralize this to \\(n = 1,2,3,...,100\\) and plot the results (x-axis: number of trials, y-axis: frequency of correct answers).\n\nHint: You need to put this in a for loop over \\(n\\) and record the least number of samples needed for a significant results within each iteration. The code below can be used as inspiration.\n\n\n\n\n\n\n\np &lt;- 1/3 # Parameter under the null hypothesis\na &lt;- 0.05 # Set the significance threshold \nresults &lt;- c() # Predefine vector for storing results\n\nfor (n in 1:100) {\n  \n  # Define all possible outcomes for this round of the loop\n  x &lt;- 1:n\n  \n  # Outcome probability under the null distribution\n  nullProb &lt;- 1 - pbinom(x - 1, n, p) \n  \n  # Find all x-values where the null prob. is less than alpha\n  underAlpha &lt;- x[nullProb &lt; a]\n  \n  # Find the minimum x where the null prob is less than alpha and divide with n\n  results[n] &lt;- min(underAlpha) / n\n}\n\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nIn a population you think that \\(40\\%\\) are capable of answering correct for discrimination of two products. The rest (\\(60\\%\\)) will simply just give a blind guess. How large a proportion of correct answers would you expect in such a population?\n\nHint: Think of \\(100\\) persons.\n\nCompare the results for the later two questions, and give a frank idea of the number of participants needed for an experiment with the purpose of discriminating the products.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 11.3 - Uncertainty of the binomial distribution\n\n\n\n\n\n\nThis exercises examines the relation between the probability parameter \\(p\\) and the uncertainty on this \\(S_p\\) for the binomial distribution.\n\nLet \\(X\\) describe the number of successes out of \\(n\\) trial. Write op the model for \\(X\\)\nLet \\(x\\) be the observed number of successes from such a trial. Estimate the parameter of interest.\nUse the central limit theorem to approximate the standard error on this estimate, and write up the standard error for the parameter.\nDraw the relation between the parameter estimate and the standard error for the same estimate in a graph.\nAt which point is the uncertainty lowest/highest?\n\nHint: You can either solve this analytically by differentiation with respect to \\(\\hat{p}\\) or use the graph.\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 11.4 - Fig bars\n\n\n\n\n\n\nA company is producing fig and date bars and they are considering changing their date supplier. The company would like to produce fig and date bars with the same taste, smell and appearance as they have done for many years. It is therefore important that a new date supplier will not result in fig and date bars that differ from the original bars. The company asked the Department of Food Science at University of Copenhagen to perform a sensory test with five possible date suppliers. The Department of Food Science performed a triangle test with a trained sensory panel to detect if the new date suppliers would change the organoleptic (in this case smell, taste and sight) properties of the bars. The same \\(23\\) sensory judges performed the triangle test on smell, taste and appearance.\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nOpen the file dates.xlsx in Excel or in R. The results of the triangle test were either \\(1\\) for a correct assignment of the deviating sample or \\(0\\) for an incorrect assignment. Get a first impression of the data by looking at the raw data (the \\(1's\\) and \\(0's\\)).\n\nIs there a judge that is good at finding the deviating sample from the smell but bad a finding the deviating sample from the taste?\nIs there a judge that is good at finding the deviating sample from smell, taste and appearance?\n\nImport the data into R by making a data matrix like this (i.e. no need to read the excel file):\n\n\n\n\n\n\n# Import data into matrix\ndates &lt;- matrix(c(10, 11, 14, 19, 11, 9, 9,\n                  16, 9, 9, 18, 14, 22, 16, 11), \n                ncol = 5, byrow = TRUE) \n\n# Set column and row names\ncolnames(dates) &lt;- c(\"A - R\", \"B - R \", \"C - R \", \"D - R \", \"E - R\")\nrownames(dates) &lt;- c( \"Smell\", \"Taste\", \"Appearance\")\n\n# Compute the percent of correct\ndatesPercent &lt;- dates / 23\n\n# Show summary table\nsummary(dates)\n\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nMake a descriptive plot to visualize the data.\n\nYou can for example take inspiration from the plot in exercise 7.23 from Introduction to Statistics by Brockhoff.\nYou can also use ggplot2 by first making dates into a Tidy dataframe: reshape::melt(dates) and then use geom_bar(stat = 'identity').\n\nBy looking at the plotted data, does it seem likely that some of the new date producers could be used to produce fig and date bars that are similar to the reference/original bar?\nState a model for \\(X\\), where \\(X\\) is the number of correct answers in comparing a single product with the reference for a single sense, and state the chance probability. What is the probability of by random chance guessing the correct deviating sample?\nNow, state a null hypothesis and an alternative hypothesis.\n\nIs the alternative hypothesis directional (\\(&gt;\\) or \\(&lt;\\)) or non-directional (\\(\\neq\\))?\nWhat does the three alternatives correspond to in terms of probability of correct answer compared to unqualified guessing?\n\nTest this hypothesis for a triangle test result of \\(19\\) correct and \\(4\\) incorrect assignments (which is the result of the D - R smell triangle test).\n\n\n\n\n\nNote: A number of tests could be applied to test the hypothesis but since the sample size is fairly small then an exact binomial test is a good choice. This is found in the R functionbinom.test() (remember to specify the alternative hypothesis).\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nOn the basis of this exercise, which date suppliers would you recommend to the company?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 11.5 - Distribution of extreme values in the normal distribution\n\n\n\n\n\n\nThis exercise combines the (standard) normal distribution with the binomial distribution in order to calculate the distribution of the maximum (or minimum) value from a sample of size \\(n\\).\n\n\n\n\n\n\nImportant\n\n\n\nThis is a hard exercise, and not a part of the curriculum. However, the ideas used for solving the task is central in a number of applications.\n\n\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nConsider a single random draw \\(X\\) from the standard normal distribution. Calculate the probability of getting \\(x\\) or less. That is \\(P(X\\leq x)\\). If it helps, then set \\(x\\) to some specific number, say \\(1.2\\).\nNow consider a draw of size \\(n\\) from the same distribution (\\(X_1,..,X_n\\)). Write up the model for the number of data points less than \\(x\\) (from Q1).\nSet \\(n\\) to a specific number, and calculate the probability of non of the data points being larger than \\(x\\).\nGeneralize the above and write up the distribution for the maximum value in a finite sample from the standard normal distribution.\nSimulate some data, and check that your analytical solution match.",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Binomial data</span>"
    ]
  },
  {
    "objectID": "chapters/week4/poisson_data.html",
    "href": "chapters/week4/poisson_data.html",
    "title": "12  Poisson data",
    "section": "",
    "text": "12.1 Reading material\nIn short, the Poisson distribution are characterized by observations that are non-negative integers. That is \\(X \\in (0,1,2,...)\\), where each observation is a in a fixed volume, time, space etc.. I.e. the number of bacteria in \\(1gram\\) of sample or the number of events within one day. The Poisson distribution only have a single parameter \\(\\lambda\\), which is both the mean and the variance of the distribution. Formalized the distribution can be written as:\n\\[\\begin{equation}\nX \\sim Po(\\lambda)\n\\end{equation}\\]\nFor calculation of a point probability, the probability density function (pdf) is as follows:\n\\[\\begin{equation}\nP\\left(X = x \\right) = \\frac{\\lambda ^x {e^{ - \\lambda } }}{{x!}}\n\\end{equation}\\]\nThe cumulative probability function (cdf) is simply the sum over the individual point probabilities (just as in the binomial distribution).\nIn the case where we have data (\\(X_1,X_2,...,X_n\\)) following the Poisson distribution, we can use these to estimate the parameter \\(\\lambda\\). This is simply done by using the mean of \\(X\\).\n\\[\\begin{equation}\n\\hat{\\lambda} = \\bar{X}\n\\end{equation}\\]\nA confidence interval for this parameter is found by using the Central Limit Theorem, which states that the distribution of the mean approximately follow a normal distribution. That is:\n\\[\\begin{equation}\nCI_{\\lambda}: \\hat{\\lambda} \\pm z_{1-\\alpha/2}\\sqrt{\\hat{\\lambda}/n}\n\\end{equation}\\]\nWhere \\(z_{1-\\alpha/2}\\) is a fractile from the standard normal distribution. If \\(\\alpha = 0.05\\), then \\(z_{1-\\alpha/2} = 1.96\\).",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Poisson data</span>"
    ]
  },
  {
    "objectID": "chapters/week4/poisson_data.html#reading-material",
    "href": "chapters/week4/poisson_data.html#reading-material",
    "title": "12  Poisson data",
    "section": "",
    "text": "A video with an introduction to the Poisson distribution:\n\nAn Introduction to the Poisson Distribution\n\nChapter 2.2 of Introduction to Statistics by Brockhoff\n\nEspecially 2.2.4\n\n\n\nAn Introduction to the Poisson Distribution - jbstatistics",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Poisson data</span>"
    ]
  },
  {
    "objectID": "chapters/week4/poisson_data.html#exercises",
    "href": "chapters/week4/poisson_data.html#exercises",
    "title": "12  Poisson data",
    "section": "12.2 Exercises",
    "text": "12.2 Exercises\n\n\n\n\n\n\nNote\n\n\n\nSometimes, when solving exercises you end up by doing what you are asked, without understanding why. The next two exercises tries to surpass this, by presenting the task in a short form, from which you should specify the questions which will give answers to the task. It is up to you, but maybe try to use the short version, and see how far that will get you\n\n\n\n\n\n\n\n\nExercise 12.1 - Quality assurance\n\n\n\n\n\n\nShort version\nA production of a food material is checked batch wise for contamination of unwanted bacteria. The procedure is to take out \\(n\\) samples of a given size, innoculate each sample in a relevant media at a relevant temperature for a relevant number of hours, and then check each sample for growth.\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nDerive the underlying distribution of bacteria in the production batch.\nWhich assumptions do you impose? and how would you ensure the validity of these in the sampling procedure?.\n\n\n\n\n\nA batch were sampled following the procedure with \\(n = 5\\) and there were not observed any growth.\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nWhat is the upper confidence bound for the bacteria concentration in the sample under this observation?\n\n\n\n\n\nYou probably assume that in order to observe growth in a single sample there need to be at least \\(1\\) viral bacteria cell present. However, the way growth is measured is via checking the optical density a method which is not extremly sensitive, why there need to be at least \\(500\\) viral bacteria cells present in original sample in order to detect growth.\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nRecalculate the upper confidence bound for the bacteria concentration under this opservation.\n\n\n\n\n\nLong version\nA production of a food material is checked batch wise for contamination of unwanted bacteria. The procedure is to take out \\(n\\) samples of a given size, inoculate each sample in a relevant media at a relevant temperature for a relevant number of hours, and then check each sample for growth.\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nWhich distribution do the number of bacteria cells in each sample follow?\nCalculate the probability of observing growth. I.e. that a sample contain at least a single bacteria cell.\nWhich distribution does the observation of growth/no growth in the \\(n\\) samples follow? and what are the parameters for this distribution?\n\nWhat assumptions naturally follow? and how would you ensure the validity of these in the sampling procedure?\n\n\n\n\n\nA batch were sampled following the procedure with \\(n = 5\\) and there were not observed any growth in any of the \\(5\\) samples.\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nEstimate the binomial parameter and calculate a confidence interval for this.\n\nHint: For this extreme case (\\(0\\) positive) you can not use the approximation by the normal distribution. Instead look for a value of \\(p\\) that would produce the observed result with some certainty (you have to specify). You can do this analytically or trial and error using the dbinom() in R.\n\n\nWhat is the upper confidence bound for the bacteria concentration in the sample under this observation?\n\nYou probably assume that in order to observe growth in a single sample there need to be at least \\(1\\) viral bacteria cell present. However, the way growth is measured is via checking the optical density a method which is not extremely sensitive, why there need to be at least \\(500\\) viral bacteria cells present in original sample in order to detect growth.\n\nRecalculate the upper confidence bound for the bacteria concentration under this observation. Hint: This can not be done analytically, so you need to try with different values of \\(\\lambda\\) and see which one that matches\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 12.2 - Quality assurance - poisson and binomial distribution\n\n\n\n\n\n\nShort version\nA quality assurance program uses the test described in Exercise 12.1 with \\(n\\) samples. Derive the sensitivity of the test, that is; the probability of detecting contamination, given that is indeed there, as a function of the number of samples and the true concentration in the batch under investigation. Draw some curves visualizing this relation.\nLong version\nA quality assurance program uses the test described in Exercise 12.1 with \\(n\\) samples.\n\nSet \\(\\lambda = 1\\), calculate the binomial parameter for detecting growth in a single sample.\nSet \\(n=5\\), calculate the probability of observing no growth in any of the samples.\nWhat is the sensitivity of the test, that is; the probability of detecting contamination, given that it is indeed there.\nWhat happens if the sample size is changed for instance by a factor of \\(2\\) or \\(1/2\\)?\nGeneralize this for other values of \\(\\lambda\\) and \\(n\\), and draw curves for varying \\(n\\) with sensitivity on the y-axis and \\(\\lambda\\) on the x-axis.\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 12.3 - Quality assurance - chance of false rejections\n\n\n\n\n\n\nFor some types of microorganisms (bacteria or yeast) a product is damaged for very low concentrations, due to the possibility of growth during storage. However, a number of organisms are only damaging the product when present in high amounts - if present in low amounts, they do not affect the quality.\nFor a specific type of bacteria a concentration above a value \\(C\\) (measured in CFU/g) leads to a damaged product, that should not be put on market, whereas a concentration below this value results in a good product, at least within the labeled shelf life.\nYou have the responsibility for quality assurance at your company. In order to be able to export to Japan and USA (those countries are the most pernickety with respect to product safety) you need to document a thorough eigen-control system. So you set up a procedure.\nThis procedure is set up to measure the concentration in a product sample. That is, a predefined number of samples (\\(n\\)) is sampled from the batch, diluted, spread on plates, incubated and the number of colony forming units are counted. That leads to the observations: \\(X_1,X_2,\\ldots,X_n\\).\n\nBased on these observations give an estimate of the concentration in the entire batch.\nAdditionally, give a confidence interval for this concentration, where you approximate the distribution of the mean concentration with a normal distribution (using the central limit theorem).\nWhich of the three numbers (central estimate, lower and upper confidence bound) do you think is essential for determination of whether the batch is ok or not?\nWhich rule would you suggest for making this decision?\nWhat is the change of rejecting an ok product under this rule?\nSimulate the scenario for varying concentration parameters, varying sample size (\\(n\\)) and determine the rate of rejection of ok batches.\n\nFor construction of random Poisson data use the function rpois() of fictious samples.",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Poisson data</span>"
    ]
  },
  {
    "objectID": "chapters/week5/week_5.html",
    "href": "chapters/week5/week_5.html",
    "title": "Week 5",
    "section": "",
    "text": "Hand-in assignment\nIn this week we are going to discuss a last subject for non continuous data, that is, multinomial data and the multinomial distribution. The Chi squared test is a central test for discrete data types and can be used for inferential testing. Further, we are going to deal with the notion of power, that is; if there truly is a difference, how likely is it that the statistical tests will find it?\n(ex-power-triangle?) Power calculation - triangle test is to be handed in (through absalon or as hard-copy Wednesday night). You are welcome to put in R-code in the assignment, but it is your argumentation and interpretation that are the most important.",
    "crumbs": [
      "Week 5"
    ]
  },
  {
    "objectID": "chapters/week5/week_5.html#exercises",
    "href": "chapters/week5/week_5.html#exercises",
    "title": "Week 5",
    "section": "Exercises",
    "text": "Exercises\nFor Monday work through the following exercises:\n\nExercise 13.1\n(ex-power-calculation?)\n\nIf you really think that this is easy stuff, then try to work through exercise (ex-dist-of-extreme-values?)\nFor Wednesday work through the following exercises:\n\n(ex-triangle-or-duo-trio?)\n(ex-power-calculation-in-t-test?)",
    "crumbs": [
      "Week 5"
    ]
  },
  {
    "objectID": "chapters/week5/week_5.html#case-iii",
    "href": "chapters/week5/week_5.html#case-iii",
    "title": "Week 5",
    "section": "Case III",
    "text": "Case III\nThe third case should be handed in as a slide-show with voice no later than Thursday evening next week.",
    "crumbs": [
      "Week 5"
    ]
  },
  {
    "objectID": "chapters/week5/mutinomial_data.html",
    "href": "chapters/week5/mutinomial_data.html",
    "title": "13  Multinomial data",
    "section": "",
    "text": "13.1 Reading material\nMultinomial data are categorical data with more than two groups. If there is only two groups, the data follow the binomial distribution. These data are naturally organized in a so-called frequency- or contingency table (see for example Exercise 13.1). In general terms such a table with \\(n\\) rows and \\(k\\) columns can be shown as in table 13.1.\nSuch data can be collected in two ways following two slightly different models.\nOne distribution\n\\(N\\) samples, that are put into \\(nk\\) categories. For example, \\(256\\) people are selected and categorized according to gender and color of hair. The model for this case is:\n\\[\nN_{11},N_{12},...,N_{nk} \\sim Multinomial(N,p_{11},p_{12},...,p_{nk})\n\\]\nwhere\n\\[\nN = N_{11} + N_{12} + ... + N_{nk} \\quad \\text{and} \\quad p_{11} + p_{12} + ... + p_{nk} = 1$.\n\\]\ni.e. one distribution\nSeveral distributions\nSeveral \\(N_i\\) samples, that are put into \\(k\\) categories. In this case, we predefine the number of samples within each row and distribute those over the categories. For example, \\(100\\) men and \\(123\\) women, distributed on color of their hair. The model for this case is:\n\\[\nN_{i1},N_{i2},...,N_{ik} \\sim Multinomial(N_{i},p_{i1},p_{i2},...,p_{ik})\n\\]\nwhere\n\\[\nN_{i} = N_{i1} + N_{i2} + ... + N_{ik} \\quad \\text{and} \\quad p_{i1} + p_{i2} + ... + p_{ik} = 1$ for $i = 1,...,n.\n\\]\ni.e. several (\\(n\\)) distributions.\nThe natural question for both types of data is whether there is independence between the columns (or rows). For the example that is; Is the distribution similar regardless of gender. However, the null hypothesis is stated differently depending on the model.\nIn both cases the test is the same, and based on calculating the expected number of observations under the null hypothesis, and comparing those with the observed number of observations. If this number is large, then there is large differences between the observed and the expected, why the null hypothesis is rejected.\nThe expected value \\(E\\) for each cell in the table is calculated as:\n\\[\\begin{equation}\nE_{ij} = \\frac{N_{i \\cdot} N_{\\cdot j}}{N_{\\cdot \\cdot}}\n\\end{equation}\\]\nI.e. the row sum multiplied with the column sum and divided by the total sum (see table 13.2). The test statistic \\(X^2_{obs}\\) is calculated as:\n\\[\\begin{equation}\nX^2_{obs} = \\sum_{ij}{\\frac{(E_{ij} - N_{ij})^2}{E_{ij}}}\n\\end{equation}\\]\nI.e. the (squared) discrepancy between the expected (\\(E_{ij}\\)) and the observed (\\(N_{ij}\\)) divided by the expected value. Summed across all cells.\nUnder the null hypothesis \\(X^2_{obs}\\) follow a so-called chi-squared distribution (\\(\\chi^2\\)) with \\(df = (n-1)(k-1)\\) degrees of freedom. The P-value is one-sided:\n\\[\\begin{equation}\nP = P(\\chi^2_{df} &gt; X^2_{obs})\n\\end{equation}\\]\nOBS: Be aware that too low expected values (\\(E_{ij}\\)) makes this test unstable. A rule of thumb is that \\(80\\%\\) of the cells should be above \\(5\\) and ALL should be above \\(1\\). In the case where this is violated, cells can be merged by summing both the expected and observed values.",
    "crumbs": [
      "Week 5",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Multinomial data</span>"
    ]
  },
  {
    "objectID": "chapters/week5/mutinomial_data.html#reading-material",
    "href": "chapters/week5/mutinomial_data.html#reading-material",
    "title": "13  Multinomial data",
    "section": "",
    "text": "A video going through \\(\\chi^2\\)-test (Goodness of fit test) for frequency tables:\n\nChi-Square Tests\n\nChapter 7 of Introduction to Statistics by Brockhoff\n\n\nChi-Square Tests - CrashCourse",
    "crumbs": [
      "Week 5",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Multinomial data</span>"
    ]
  },
  {
    "objectID": "chapters/week5/mutinomial_data.html#exercises",
    "href": "chapters/week5/mutinomial_data.html#exercises",
    "title": "13  Multinomial data",
    "section": "13.2 Exercises",
    "text": "13.2 Exercises\n\n\n\n\n\n\nExercise 13.1 - Comparison of senses\n\n\n\n\n\n\nA study wants to compare two types of trout samples, being meat stored under different conditions. The instrument used is a sensorical panel of \\(23\\) judges using either their visual sense, smelling sense or tasting sense. At each trial, each judges is presented with three pieces of meat - two similar and one odd. The task for the judge is to identify the odd sample using one of the senses. Data from such an experiment is presented below (table 13.3).\n\n\n\n\nTable 13.3: Counts of outcomes for each sense.\n\n\n\n\n\n\nCorrect\nNot correct\n\n\n\n\nSmell\n14\n9\n\n\nTaste\n16\n7\n\n\nVisual appearance\n22\n1\n\n\n\n\n\n\n\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nFor now, stick to the sense Taste. State a statistical model for the outcome of each trial.\nFormulate a null hypothesis based on the model, and test how different the observed results are compared to this hypothesis.\n\n\n\n\n\nBased on the previous result, it seems like the different meat pieces is identifiable based on tasting. Now the questions is whether the two other senses performs similar in identification of the odd sample? (Hint: In this exercise you should use the method sketched in Method 7.19 and Method 7.21 in the eNotes).\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nGive a frank ranking of the senses based on the observed data.\nFormulate a model for each of the three senses.\nState a null hypothesis in relation to the question of similarity between senses.\nCompute by hand the expected values under this null hypothesis, \\(X^2_{obs}\\) and the degrees of freedom.\n\nUse the pchisq() to test the null hypothesis.\nTest the hypothesis using a function in R (try to figure out which one that does the job in a single line). Compare the results with your own calculation.\n\nReport the results in such a way, that differences between the three senses are communicated.\n\nHint: This can either be done by pairwise contrasts or confidence intervals for the central parameters.",
    "crumbs": [
      "Week 5",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Multinomial data</span>"
    ]
  },
  {
    "objectID": "chapters/week5/power_calculation.html",
    "href": "chapters/week5/power_calculation.html",
    "title": "14  Power calculation",
    "section": "",
    "text": "Learning objectives\n\n\n\nThe learning objectives for this theme is to understand the idea behind statistical power.\n\nKnow that a true difference might not be statistical detected due to size of data.\nUnderstand that statistical power depends on effect size, uncertainty and number of samples.\nBe able to make appropriate assumptions and calculate the power for a given study design.\nBe able to calculate power for studies evaluated by the t-test and by the binomial distribution.\n\n\n\n\n\n\n\n\n\nExample 14.1 - Quality control - power calculation\n\n\n\n\n\n\nThis example extends (exa-qa-estimation?).\nThe \\(3\\%\\) nonconforming products is unsatisfactory, and so you use a lot of money and (hopefully) make a lot of improvements. After a year you and your colleagues feel that the end quality has improved, and you wish to test that this improvement is indeed also statistically provable.\nYou believe that the number of nonconforming products is reduced by a factor of \\(2\\), that is down to \\(1.5\\%\\) nonconforming products.\nThe question is: In a trial, how many samples (\\(n\\)) should you select in order to statistically prove this change?.\nThe null hypothesis is \\(H0: p = p_0 = 0.03\\).\nWith the one-sided alternative: \\(HA: p &lt; p_0 = 0.03\\).\nFrom a study on say \\(n = 200\\) (\\(X_{HO} \\sim \\mathcal{B}(n,p = 0.03)\\)) with \\(x\\) nonconforming products the probability for accepting \\(H0\\) is:\n\\[\\begin{equation}\nP(X_{HO} \\leq x) \\geq \\alpha = 0.05\n\\end{equation}\\]\n\nn &lt;- 200\nx &lt;- 0:3\npbinom(x, n, 0.03)\n\n[1] 0.002261241 0.016248299 0.059290946 0.147151194\n\n\nSo, in order to reject \\(H0\\) (at level \\(\\alpha = 0.05\\)) you should among \\(200\\) samples find \\(0\\) or \\(1\\) nonconforming products (at \\(x = 2\\), the p-value is \\(p = 0.059\\)).\nIf you believe that the true probability is \\(p = 0.015\\) (\\(X_{HA} \\sim \\mathcal{B}(n,p = 0.015)\\)) then the probability for getting this number of positive samples (or what is even more extreme compared to \\(H0\\)) can be calculated as\n\\[\\begin{equation}\nP(X_{HA} \\leq 1) = P(X_{HA}=0) + P(X_{HA}=1).\n\\end{equation}\\]\n\nn &lt;- 200\nx &lt;- 1\npbinom(x, n, 0.015)\n\n[1] 0.1968966\n\n\nThis is the power of the trial under these assumptions. In detail that means, that only one out of five times you would be able to prove that the investment were worth the effort.\nIf you which to have a higher power for the study, then the number of samples should be increased e.g. to \\(n = 400\\):\n\nalpha &lt;- 0.05\nn &lt;- 400 # Number of trials\nx &lt;- 0:100 # Number of successes\n\n# The probability of outcome x under the null hypothesis that p = 0.03\nnullProb &lt;- pbinom(x, n, 0.03) \n\n\n# The maximum x-value that returns a probability smaller than alpha\nxMax &lt;- max(x[nullProb &lt; alpha])\n\npbinom(xMax, n, 0.015)\n\n[1] 0.6063058\n\n\nOr assessed for a sequence of \\(n\\) values (figure 14.1).\n\nalpha &lt;- 0.05\nN &lt;- 1:800 # Number of trials\npwr &lt;- c() # Initiate vector for storage\n\n\nfor (n in N) {\n  \n  x &lt;- 0:n\n  \n  nullProb &lt;- pbinom(x, n, 0.03)\n  \n  xMax &lt;- max(x[nullProb &lt; alpha])\n  pwr[n] &lt;- pbinom(xMax, n, 0.015)\n}\n\nplot(N, pwr)\n\n\n\n\n\n\n\nFigure 14.1: Power vs number of trials.\n\n\n\n\n\nIt is seen that indeed a very high number of samples needs to be collected in order to be fairly certain that the trial will statistically confirm that the number of nonconforming products has dropped.",
    "crumbs": [
      "Week 5",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Power calculation</span>"
    ]
  },
  {
    "objectID": "chapters/appendix/data_wrangling.html",
    "href": "chapters/appendix/data_wrangling.html",
    "title": "Appendix A — Wrangling data in R",
    "section": "",
    "text": "A.1 Logical indexing\nLogical indexing is one of the most powerful data wrangling techniques. It uses logical operators like == (equal to) and != (not equal to) to subset data based on multiply criteria. Below are some examples to explain indexing, logical operators, and finally subsetting data with logical indexing.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Wrangling data in R</span>"
    ]
  },
  {
    "objectID": "chapters/appendix/data_wrangling.html#logical-indexing",
    "href": "chapters/appendix/data_wrangling.html#logical-indexing",
    "title": "Appendix A — Wrangling data in R",
    "section": "",
    "text": "A.1.1 Indexing data\nIf x is a vector of the letters A through F in alphabetical order, then we can call the fourth letter (D) by the syntax x[4]:\n\nx &lt;- c(\"A\", \"B\", \"C\", \"D\", \"E\", \"F\")\n\nx[4]\n\n[1] \"D\"\n\n\nIf x is a matrix (or a dataframe) then we can index the data by the syntax x[row, column]. For example\n\nx[2, 3] - Extract the data in row 2, column 3.\nx[ , 3] - Extract all rows in column 3.\nx[2, ] - Extract all columns in row 2.\n\n\n# Create a matrix with 3 columns\nx &lt;- matrix(c(\"A\", \"B\", \"C\", \"D\", \"E\", \"F\"), ncol = 3)\n\n# Print full matrix\nx\n\n     [,1] [,2] [,3]\n[1,] \"A\"  \"C\"  \"E\" \n[2,] \"B\"  \"D\"  \"F\" \n\n# Row 2, column 3\nx[2, 3]\n\n[1] \"F\"\n\n# All rows, column 3\nx[ , 3]\n\n[1] \"E\" \"F\"\n\n# Row 2, all columns\nx[2, ]\n\n[1] \"B\" \"D\" \"F\"\n\n\n\n\nA.1.2 Logical operators in R\nLogical operators are basic statements that return TRUE or FALSE (table A.1).\n\n\n\nTable A.1: Some of the logical operators available in R\n\n\n\n\n\n\n\n\n\n\n\nName\nOperator\nReturns TRUE\nReturns FALSE\n\n\n\n\nEqual to\n==\n\"A\" == \"A\"\n\"A\" == \"B\"\n\n\nNot equal to\n!=\n\"A\" != \"B\"\n\"A\" != \"A\"\n\n\nLess than\n&lt;\n2 &lt; 3\"\n3 &lt; 2\n\n\nGreater than\n&gt;\n15 &gt; 10\n10 &gt; 15\n\n\nLess than or equal to\n&lt;=\n10 &lt;= 10 or 9 &lt;= 10\n10 &lt;= 9\n\n\nGreater than or equal to\n&gt;=\n22 &gt;= 22 or 25 &gt;= 22\n20 &gt;= 15\n\n\nIn\n%in%\n\"A\" %in% c(\"A\", \"B\" \"C\")\n\"F\" %in% c(\"A\", \"B\" \"C\")\n\n\n\n\n\n\nIf we have a vector x with multiple instances of A, B and C we can index it as:\n\n# Create the vector\nx &lt;- rep(c(\"A\", \"B\", \"C\"), each = 3)\n\n# Print the vector\nx\n\n[1] \"A\" \"A\" \"A\" \"B\" \"B\" \"B\" \"C\" \"C\" \"C\"\n\n# Check where the elements of x equals \"B\"\nx == \"B\"\n\n[1] FALSE FALSE FALSE  TRUE  TRUE  TRUE FALSE FALSE FALSE\n\n\nIf we want to check multiple statements at once we can use the AND & and the OR | operator.\n\n# Check where the elements of x equals \"B\" OR \"C\"\nx == \"A\" | x == \"C\"\n\n[1]  TRUE  TRUE  TRUE FALSE FALSE FALSE  TRUE  TRUE  TRUE\n\n# Check where the elements of x equals \"B\" AND \"C\"\nx == \"A\" & x == \"C\"\n\n[1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n\n\n\n\nA.1.3 Logical indexing of data\nIt is possible to use a logical vector (i.e. a vector that only contains TRUE and FALSE) to index data. When passing a logical vector to a data object it returns the data in all the places where the vector equals TRUE.\nAs an example, we load the palmerpenguins dataset, inspect it and do some simple logical indexing.\n\n# Inspect the data\nhead(penguins)\n\n# A tibble: 6 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           NA            NA                  NA          NA\n5 Adelie  Torgersen           36.7          19.3               193        3450\n6 Adelie  Torgersen           39.3          20.6               190        3650\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n# Extract all rows where species equals \"Adelie\"\nx &lt;- penguins[penguins$species == \"Adelie\", ]\n\nhead(x)\n\n# A tibble: 6 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           NA            NA                  NA          NA\n5 Adelie  Torgersen           36.7          19.3               193        3450\n6 Adelie  Torgersen           39.3          20.6               190        3650\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n# Extract all rows where species equals \"Adelie\"\n# AND body mass in greater than 4000\nx &lt;- penguins[penguins$species == \"Adelie\" & penguins$body_mass_g &gt; 4000, ]\n\nhead(x)\n\n# A tibble: 6 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 &lt;NA&gt;    &lt;NA&gt;                NA            NA                  NA          NA\n2 Adelie  Torgersen           39.2          19.6               195        4675\n3 Adelie  Torgersen           42            20.2               190        4250\n4 Adelie  Torgersen           34.6          21.1               198        4400\n5 Adelie  Torgersen           42.5          20.7               197        4500\n6 Adelie  Torgersen           46            21.5               194        4200\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Wrangling data in R</span>"
    ]
  },
  {
    "objectID": "chapters/appendix/data_wrangling.html#sec-pipe-operator",
    "href": "chapters/appendix/data_wrangling.html#sec-pipe-operator",
    "title": "Appendix A — Wrangling data in R",
    "section": "A.2 The pipe operator",
    "text": "A.2 The pipe operator\nIn R you have the opportunity to use a special syntax called the pipe operator |&gt;. It can be thought of as a pipe that “sends data downstream” to the next call in your script.\nIf we for example want to take the square root of some data df and afterwards show the first 6 values of that data with head() we can write\n\ndf |&gt;\n  sqrt() |&gt;\n  head()\n\nwhich is equivalent to\n\nhead(sqrt(df))\n\nThis might not make much sense for this example, but when things get a bit more complex the pipe operator can really help by making code easier to read.\nIf we for example want to format some data df and then plot it in ggplot2 we can write the following script\n\ndf |&gt;\n  pivot_longer(\n    cols = everything(),\n    names_to = \"group\",\n    names_prefix = \"X\",\n    values_to = \"response\"\n    ) |&gt;\n  mutate(group = str_c(\"Group \", group)) |&gt;\n  ggplot(aes(group, response)) +\n  geom_boxplot()\n\nWithout the pipe we would need to create several intermediate variables cluttering up our script and environment in the process.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Wrangling data in R</span>"
    ]
  },
  {
    "objectID": "chapters/appendix/data_wrangling.html#tidy-data",
    "href": "chapters/appendix/data_wrangling.html#tidy-data",
    "title": "Appendix A — Wrangling data in R",
    "section": "A.3 Tidy data",
    "text": "A.3 Tidy data\n\nA.3.1 What is tidy data?\n\n\nA.3.2 How to convert to tidy data",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Wrangling data in R</span>"
    ]
  },
  {
    "objectID": "chapters/appendix/data_wrangling.html#sec-summarise-by-group",
    "href": "chapters/appendix/data_wrangling.html#sec-summarise-by-group",
    "title": "Appendix A — Wrangling data in R",
    "section": "A.4 Summarise by group",
    "text": "A.4 Summarise by group\nSometimes we want to calculate a statistic per group. There are many different ways of doing this, and in these examples we are going to present a way possible ways.\nFor these examples we use the Palmer Penguins dataset. It consists of some data regarding the bill size, flipper length and body mass of three penguin species across three islands.\nLet us start by taking a look at our data. This can be done via the head() function or the glimpse() function from the Tidyverse.\n\nhead(penguins) # Show the first 6 rows of dataframe\n\n# A tibble: 6 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           NA            NA                  NA          NA\n5 Adelie  Torgersen           36.7          19.3               193        3450\n6 Adelie  Torgersen           39.3          20.6               190        3650\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\n\nglimpse(penguins)\n\nRows: 344\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male…\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\n\n\nA.4.1 One statistic per group\nIf we want to summarise one statistic, that could be the mean or the standard deviation, grouped by species or island (or both), we can do it in the following ways.\n\nUsing TidyverseUsing base R\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIf the pipe operator |&gt; is unkown to you see Section A.2.\n\n\nTo compute the mean of all numeric variables we can use the summarise() function from the dplyr package in Tidyverse. Remember to load the tidyverse package (library(tidyverse)) - this package includes dplyr as well as a whole lot of other nice packages.\nFor this example we want the mean for all numeric variables grouped by species.\n\npenguins |&gt;\n  drop_na() |&gt; # Remove rows with missing values\n  summarise(\n    across(\n      .cols = where(is.numeric), # Chose columns that are numeric \n      .fns = mean # Set the function we want to use\n           ),\n    .by = species # Group by species\n    )\n\n# A tibble: 3 × 6\n  species   bill_length_mm bill_depth_mm flipper_length_mm body_mass_g  year\n  &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt;\n1 Adelie              38.8          18.3              190.       3706. 2008.\n2 Gentoo              47.6          15.0              217.       5092. 2008.\n3 Chinstrap           48.8          18.4              196.       3733. 2008.\n\n\nBe aware that the function drops all non-numeric variables that are not part of the grouping. So the output of the above code is “missing” the island and sex variables. Also, the drop_na() function removes all rows with missing values - if this is not done all columns with missing values will return NA.\nIf we want to group by multiple variables, for example species and island, we just need to pass a vector of the variables to the .by = argument.\n\npenguins |&gt;\n  drop_na() |&gt; # Remove rows with missing values\n  summarise(\n    across(where(is.numeric), mean),\n    .by = c(island, species) # Group by island and species\n  )\n\n# A tibble: 5 × 7\n  island    species   bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;     &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;       &lt;dbl&gt;\n1 Torgersen Adelie              39.0          18.5              192.       3709.\n2 Biscoe    Adelie              39.0          18.4              189.       3710.\n3 Dream     Adelie              38.5          18.2              190.       3701.\n4 Biscoe    Gentoo              47.6          15.0              217.       5092.\n5 Dream     Chinstrap           48.8          18.4              196.       3733.\n# ℹ 1 more variable: year &lt;dbl&gt;\n\n\n\n\nTo compute the mean of all variables grouped by species we can use the aggregate() function. This is included in base R, so there is no need to load any packages.\n\n# Remove rows with missing values \npenguins_clean &lt;- na.omit(penguins)\n\naggregate(penguins_clean, list(species = penguins_clean$species), mean)\n\n    species species island bill_length_mm bill_depth_mm flipper_length_mm\n1    Adelie      NA     NA       38.82397      18.34726          190.1027\n2 Chinstrap      NA     NA       48.83382      18.42059          195.8235\n3    Gentoo      NA     NA       47.56807      14.99664          217.2353\n  body_mass_g sex     year\n1    3706.164  NA 2008.055\n2    3733.088  NA 2007.971\n3    5092.437  NA 2008.067\n\n\nBe aware that all non-numeric variables will return NA (and thus a lot of warnings). Also, the na.omit() function removes all rows with missing values - if this is not done all columns with missing values will return NA.\nIf we want to group by multiple variables, for example species and island, we just need to pass another grouping variable to the list.\n\naggregate(penguins_clean, \n          list(species = penguins_clean$species,\n               island = penguins_clean$island), \n          mean)\n\n    species    island species island bill_length_mm bill_depth_mm\n1    Adelie    Biscoe      NA     NA       38.97500      18.37045\n2    Gentoo    Biscoe      NA     NA       47.56807      14.99664\n3    Adelie     Dream      NA     NA       38.52000      18.24000\n4 Chinstrap     Dream      NA     NA       48.83382      18.42059\n5    Adelie Torgersen      NA     NA       39.03830      18.45106\n  flipper_length_mm body_mass_g sex     year\n1          188.7955    3709.659  NA 2008.136\n2          217.2353    5092.437  NA 2008.067\n3          189.9273    3701.364  NA 2008.018\n4          195.8235    3733.088  NA 2007.971\n5          191.5319    3708.511  NA 2008.021\n\n\n\n\n\n\n\nA.4.2 Create a summary table\nSometimes we want to create a summary table grouped by some variable. This can be done in the following ways.\n\nUsing TidyverseUsing base R\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIf the pipe operator |&gt; is unkown to you see Section A.2.\n\n\nTo compute a summary table of a numeric variable we can use the summarise() function from the dplyr package in Tidyverse. Remember to load the tidyverse package (library(tidyverse)) - this package includes dplyr as well as a whole lot of other nice packages.\nFor this example we want some summary statistics for the body_mass_g variable grouped by species.\n\npenguins |&gt;\n  drop_na() |&gt; # Remove rows with missing values\n  summarise(\n    N = length(body_mass_g),\n    Mean = mean(body_mass_g),\n    Median = median(body_mass_g),\n    Std = sd(body_mass_g),\n    IQR = IQR(body_mass_g),\n    .by = species # Group by species\n  )\n\n# A tibble: 3 × 6\n  species       N  Mean Median   Std   IQR\n  &lt;fct&gt;     &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Adelie      146 3706.   3700  459.  638.\n2 Gentoo      119 5092.   5050  501.  800 \n3 Chinstrap    68 3733.   3700  384.  462.\n\n\nBe aware that the drop_na() function removes all rows with missing values - if this is not done all columns with missing values will return NA.\nIf we want to group by multiple variables, for example species and island, we just need to pass another grouping variable to the list.\n\npenguins |&gt;\n  drop_na() |&gt; # Remove rows with missing values\n  summarise(\n    N = length(body_mass_g),\n    Mean = mean(body_mass_g),\n    Median = median(body_mass_g),\n    Std = sd(body_mass_g),\n    IQR = IQR(body_mass_g),\n    .by = c(island, species) # Group by island and species\n  )\n\n# A tibble: 5 × 7\n  island    species       N  Mean Median   Std   IQR\n  &lt;fct&gt;     &lt;fct&gt;     &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Torgersen Adelie       47 3709.   3700  452.  662.\n2 Biscoe    Adelie       44 3710.   3750  488.  588.\n3 Dream     Adelie       55 3701.   3600  449.  588.\n4 Biscoe    Gentoo      119 5092.   5050  501.  800 \n5 Dream     Chinstrap    68 3733.   3700  384.  462.\n\n\n\n\nTo compute a summary table of a numeric variable we can use the aggregate() function. This is included in base R, so there is no need to load any packages.\nFor this example we want some summary statistics for the body_mass_g variable grouped by species.\n\n# Remove rows with missing values \npenguins_clean &lt;- na.omit(penguins)\n\n# Define variables for grouping\nmy_groups &lt;- list(species = penguins_clean$species)\n\n# Compute summary stats\npenguins_n &lt;- aggregate(penguins_clean, my_groups, length)\npenguins_mean &lt;- aggregate(penguins_clean, my_groups, mean)\npenguins_median &lt;- aggregate(penguins_clean, my_groups, mean)\npenguins_std &lt;- aggregate(penguins_clean, my_groups, mean)\npenguins_iqr &lt;- aggregate(penguins_clean, my_groups, mean)\n\n# Collect everything in a dataframe\npenguins_summary &lt;- data.frame(\n  \"Species\" = penguins_n$species,\n  \"N\" = penguins_n$body_mass_g,\n  \"Mean\" = penguins_mean$body_mass_g,\n  \"Median\" = penguins_median$body_mass_g,\n  \"Std\" = penguins_std$body_mass_g,\n  \"IQR\" = penguins_iqr$body_mass_g\n  )\n\nprint(penguins_summary)\n\n    Species   N     Mean   Median      Std      IQR\n1    Adelie 146 3706.164 3706.164 3706.164 3706.164\n2 Chinstrap  68 3733.088 3733.088 3733.088 3733.088\n3    Gentoo 119 5092.437 5092.437 5092.437 5092.437\n\n\nIf we want to group by multiple variables, for example species and island, we just need to pass another grouping variable to the my_groups list.\n\n# Define variables for grouping\nmy_groups &lt;- list(species = penguins_clean$species,\n                  island = penguins_clean$island)\n\n# Compute summary stats\npenguins_n &lt;- aggregate(penguins_clean, my_groups, length)\npenguins_mean &lt;- aggregate(penguins_clean, my_groups, mean)\npenguins_median &lt;- aggregate(penguins_clean, my_groups, mean)\npenguins_std &lt;- aggregate(penguins_clean, my_groups, mean)\npenguins_iqr &lt;- aggregate(penguins_clean, my_groups, mean)\n\n# Collect everything in a dataframe\npenguins_summary &lt;- data.frame(\n  \"Species\" = penguins_n$species,\n  \"Island\" = penguins_n$island,\n  \"N\" = penguins_n$body_mass_g,\n  \"Mean\" = penguins_mean$body_mass_g,\n  \"Median\" = penguins_median$body_mass_g,\n  \"Std\" = penguins_std$body_mass_g,\n  \"IQR\" = penguins_iqr$body_mass_g\n)\n\nprint(penguins_summary)\n\n    Species    Island   N     Mean   Median      Std      IQR\n1    Adelie    Biscoe  44 3709.659 3709.659 3709.659 3709.659\n2    Gentoo    Biscoe 119 5092.437 5092.437 5092.437 5092.437\n3    Adelie     Dream  55 3701.364 3701.364 3701.364 3701.364\n4 Chinstrap     Dream  68 3733.088 3733.088 3733.088 3733.088\n5    Adelie Torgersen  47 3708.511 3708.511 3708.511 3708.511",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Wrangling data in R</span>"
    ]
  }
]